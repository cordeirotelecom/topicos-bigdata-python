"""
Aula 12: Databricks e Cloud Analytics - Plataforma Unificada de Big Data
Professor: Vagner Cordeiro
Disciplina: T√≥picos de Big Data em Python

Implementa√ß√£o completa de solu√ß√µes de Big Data na nuvem usando Databricks,
incluindo processamento distribu√≠do, machine learning, streaming analytics,
Delta Lake, MLflow e integra√ß√£o com servi√ßos cloud.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import time
import logging
import requests
import os
from typing import Dict, List, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# Simula√ß√£o do ambiente Databricks (para fins educacionais)
class DatabricksSimulator:
    """
    Simulador do ambiente Databricks para demonstra√ß√£o educacional
    """
    
    def __init__(self):
        self.workspace_url = "https://dbc-12345678-90ab.cloud.databricks.com"
        self.cluster_id = "0123-456789-abcdef"
        self.token = "dapi1234567890abcdef"
        self.current_cluster_state = "RUNNING"
        self.notebooks = {}
        self.jobs = {}
        self.ml_experiments = {}
        
    def get_cluster_info(self):
        return {
            "cluster_id": self.cluster_id,
            "state": self.current_cluster_state,
            "node_type": "i3.xlarge",
            "num_workers": 2,
            "spark_version": "11.3.x-scala2.12"
        }

class DatabricksAnalyticsPlatform:
    """
    Plataforma completa de analytics usando conceitos do Databricks
    
    Funcionalidades:
    - Processamento distribu√≠do com PySpark
    - Delta Lake para versionamento de dados
    - MLflow para MLOps
    - Streaming analytics em tempo real
    - Notebooks colaborativos
    - Pipeline de CI/CD para ML
    - Integra√ß√£o com servi√ßos cloud
    - Monitoramento e observabilidade
    """
    
    def __init__(self, environment="development"):
        """Inicializa a plataforma Databricks Analytics"""
        
        self.environment = environment
        self.databricks = DatabricksSimulator()
        self.logger = self._setup_logging()
        
        # Configura√ß√µes de ambiente
        self.storage_path = "/tmp/databricks_demo"
        self.delta_path = f"{self.storage_path}/delta"
        self.mlflow_path = f"{self.storage_path}/mlflow"
        
        # Inicializa estruturas de dados
        self.datasets = {}
        self.models = {}
        self.experiments = {}
        self.streaming_jobs = {}
        
        # Cria diret√≥rios necess√°rios
        self._initialize_storage()
        
        print("üöÄ Databricks Analytics Platform inicializada!")
        print(f"üåê Environment: {environment}")
        print(f"üìä Workspace URL: {self.databricks.workspace_url}")
        print(f"‚ö° Cluster ID: {self.databricks.cluster_id}")
        
    def _setup_logging(self):
        """Configura sistema de logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
    
    def _initialize_storage(self):
        """Inicializa estrutura de armazenamento"""
        import os
        
        paths = [self.storage_path, self.delta_path, self.mlflow_path]
        for path in paths:
            os.makedirs(path, exist_ok=True)
        
        print(f"üìÅ Storage inicializado em: {self.storage_path}")
    
    def create_delta_table(self, table_name: str, schema: Dict, partitions: List[str] = None):
        """
        Cria tabela Delta Lake com versionamento
        """
        print(f"üìä Criando tabela Delta: {table_name}")
        
        # Simula cria√ß√£o de tabela Delta
        table_info = {
            'name': table_name,
            'schema': schema,
            'partitions': partitions or [],
            'location': f"{self.delta_path}/{table_name}",
            'created_at': datetime.now(),
            'version': 0,
            'format': 'DELTA'
        }
        
        # Gera dados de exemplo baseados no schema
        sample_data = self._generate_sample_data(schema, 10000)
        
        # Salva metadados
        self.datasets[table_name] = {
            'info': table_info,
            'data': sample_data,
            'versions': [{'version': 0, 'timestamp': datetime.now(), 'records': len(sample_data)}]
        }
        
        print(f"‚úÖ Tabela Delta {table_name} criada com {len(sample_data):,} registros")
        print(f"üìç Location: {table_info['location']}")
        
        if partitions:
            print(f"üîÑ Particionada por: {', '.join(partitions)}")
        
        return table_info
    
    def _generate_sample_data(self, schema: Dict, num_records: int) -> pd.DataFrame:
        """Gera dados de exemplo baseados no schema"""
        
        data = {}
        
        for column, data_type in schema.items():
            if data_type == 'string':
                if 'id' in column.lower():
                    data[column] = [f"ID_{i:06d}" for i in range(num_records)]
                elif 'name' in column.lower():
                    names = ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry']
                    data[column] = np.random.choice(names, num_records)
                elif 'category' in column.lower():
                    categories = ['A', 'B', 'C', 'D', 'E']
                    data[column] = np.random.choice(categories, num_records)
                else:
                    data[column] = [f"value_{i}" for i in range(num_records)]
                    
            elif data_type == 'integer':
                if 'age' in column.lower():
                    data[column] = np.random.randint(18, 80, num_records)
                elif 'count' in column.lower():
                    data[column] = np.random.poisson(50, num_records)
                else:
                    data[column] = np.random.randint(1, 1000, num_records)
                    
            elif data_type == 'double':
                if 'price' in column.lower():
                    data[column] = np.random.lognormal(3, 1, num_records)
                elif 'score' in column.lower():
                    data[column] = np.random.beta(2, 5, num_records) * 100
                else:
                    data[column] = np.random.normal(0, 1, num_records)
                    
            elif data_type == 'timestamp':
                start_date = datetime.now() - timedelta(days=365)
                data[column] = [start_date + timedelta(days=np.random.randint(0, 365)) 
                              for _ in range(num_records)]
                              
            elif data_type == 'boolean':
                data[column] = np.random.choice([True, False], num_records)
        
        return pd.DataFrame(data)
    
    def upsert_delta_table(self, table_name: str, new_data: pd.DataFrame, 
                          merge_condition: str):
        """
        Executa UPSERT (MERGE) em tabela Delta
        """
        print(f"üîÑ Executando UPSERT na tabela {table_name}")
        
        if table_name not in self.datasets:
            raise ValueError(f"Tabela {table_name} n√£o encontrada")
        
        # Simula opera√ß√£o MERGE
        current_data = self.datasets[table_name]['data']
        
        # Para simplicidade, simula merge por √≠ndice
        merged_data = pd.concat([current_data, new_data]).drop_duplicates(
            subset=merge_condition.split('=')[0].strip(), keep='last'
        )
        
        # Atualiza vers√£o
        current_version = self.datasets[table_name]['versions'][-1]['version']
        new_version = current_version + 1
        
        self.datasets[table_name]['data'] = merged_data
        self.datasets[table_name]['versions'].append({
            'version': new_version,
            'timestamp': datetime.now(),
            'records': len(merged_data),
            'operation': 'MERGE'
        })
        
        print(f"‚úÖ UPSERT conclu√≠do - Vers√£o {new_version}")
        print(f"üìä Registros atuais: {len(merged_data):,}")
        print(f"üìà Novos registros: {len(new_data):,}")
        
        return merged_data
    
    def time_travel_query(self, table_name: str, version: Optional[int] = None, 
                         timestamp: Optional[datetime] = None):
        """
        Executa consulta com time travel no Delta Lake
        """
        print(f"‚è∞ Executando time travel na tabela {table_name}")
        
        if table_name not in self.datasets:
            raise ValueError(f"Tabela {table_name} n√£o encontrada")
        
        versions = self.datasets[table_name]['versions']
        
        if version is not None:
            # Busca por vers√£o espec√≠fica
            target_version = None
            for v in versions:
                if v['version'] == version:
                    target_version = v
                    break
            
            if target_version is None:
                print(f"‚ùå Vers√£o {version} n√£o encontrada")
                return None
                
            print(f"üìÖ Consultando vers√£o {version} ({target_version['timestamp']})")
            
        elif timestamp is not None:
            # Busca vers√£o mais pr√≥xima do timestamp
            target_version = None
            for v in sorted(versions, key=lambda x: x['timestamp']):
                if v['timestamp'] <= timestamp:
                    target_version = v
                else:
                    break
            
            if target_version is None:
                print(f"‚ùå Nenhuma vers√£o encontrada antes de {timestamp}")
                return None
                
            print(f"üìÖ Consultando vers√£o {target_version['version']} ({target_version['timestamp']})")
        
        else:
            # Vers√£o mais recente
            target_version = versions[-1]
            print(f"üìÖ Consultando vers√£o atual {target_version['version']}")
        
        # Simula retorno dos dados da vers√£o espec√≠fica
        # Em uma implementa√ß√£o real, isso consultaria o Delta Log
        data = self.datasets[table_name]['data']
        
        print(f"üìä Registros encontrados: {len(data):,}")
        
        return data
    
    def create_streaming_job(self, job_name: str, source_config: Dict, 
                           sink_config: Dict, processing_logic: str):
        """
        Cria job de streaming analytics
        """
        print(f"üåä Criando job de streaming: {job_name}")
        
        job_config = {
            'name': job_name,
            'source': source_config,
            'sink': sink_config,
            'processing': processing_logic,
            'created_at': datetime.now(),
            'status': 'RUNNING',
            'processed_records': 0,
            'checkpoints': []
        }
        
        # Simula fonte de dados em streaming
        if source_config['type'] == 'kafka':
            print(f"üì° Conectando ao Kafka: {source_config['brokers']}")
            print(f"üìã T√≥pico: {source_config['topic']}")
        elif source_config['type'] == 'kinesis':
            print(f"üöÄ Conectando ao Kinesis: {source_config['stream_name']}")
        elif source_config['type'] == 'eventhub':
            print(f"üì® Conectando ao Event Hub: {source_config['namespace']}")
        
        # Simula processamento
        self._simulate_stream_processing(job_config)
        
        self.streaming_jobs[job_name] = job_config
        
        print(f"‚úÖ Job de streaming {job_name} iniciado")
        
        return job_config
    
    def _simulate_stream_processing(self, job_config: Dict):
        """Simula processamento de streaming"""
        
        # Gera dados de exemplo para streaming
        batch_size = 1000
        processing_time = 2.0  # segundos
        
        for batch in range(5):  # Simula 5 batches
            print(f"  üì¶ Processando batch {batch + 1}...")
            
            # Simula dados de entrada
            if 'aggregation' in job_config['processing']:
                print(f"    üî¢ Executando agrega√ß√µes...")
            if 'windowing' in job_config['processing']:
                print(f"    ü™ü Aplicando windowing...")
            if 'filtering' in job_config['processing']:
                print(f"    üîç Aplicando filtros...")
            
            job_config['processed_records'] += batch_size
            job_config['checkpoints'].append({
                'batch': batch + 1,
                'timestamp': datetime.now(),
                'records': batch_size
            })
            
            time.sleep(0.1)  # Simula processamento
        
        print(f"  ‚úÖ Processados {job_config['processed_records']:,} registros")
    
    def create_ml_experiment(self, experiment_name: str, model_type: str, 
                           dataset_name: str, parameters: Dict):
        """
        Cria experimento de ML com MLflow
        """
        print(f"üß™ Criando experimento ML: {experiment_name}")
        
        if dataset_name not in self.datasets:
            raise ValueError(f"Dataset {dataset_name} n√£o encontrado")
        
        # Configura√ß√£o do experimento
        experiment_config = {
            'name': experiment_name,
            'model_type': model_type,
            'dataset': dataset_name,
            'parameters': parameters,
            'created_at': datetime.now(),
            'status': 'RUNNING',
            'runs': []
        }
        
        # Simula execu√ß√£o do experimento
        self._simulate_ml_training(experiment_config)
        
        self.experiments[experiment_name] = experiment_config
        
        print(f"‚úÖ Experimento {experiment_name} criado")
        
        return experiment_config
    
    def _simulate_ml_training(self, experiment_config: Dict):
        """Simula treinamento de modelo ML"""
        
        model_type = experiment_config['model_type']
        parameters = experiment_config['parameters']
        
        print(f"ü§ñ Treinando modelo {model_type}...")
        
        # Simula diferentes tipos de modelo
        if model_type == 'random_forest':
            print(f"  üå≥ Random Forest - n_estimators: {parameters.get('n_estimators', 100)}")
            accuracy = np.random.uniform(0.85, 0.95)
            
        elif model_type == 'xgboost':
            print(f"  üöÄ XGBoost - max_depth: {parameters.get('max_depth', 6)}")
            accuracy = np.random.uniform(0.88, 0.96)
            
        elif model_type == 'neural_network':
            print(f"  üß† Neural Network - hidden_layers: {parameters.get('hidden_layers', 3)}")
            accuracy = np.random.uniform(0.82, 0.94)
            
        else:
            print(f"  üìä {model_type} - par√¢metros: {parameters}")
            accuracy = np.random.uniform(0.80, 0.92)
        
        # Simula m√©tricas
        metrics = {
            'accuracy': accuracy,
            'precision': accuracy + np.random.normal(0, 0.02),
            'recall': accuracy + np.random.normal(0, 0.02),
            'f1_score': accuracy + np.random.normal(0, 0.01),
            'training_time': np.random.uniform(30, 300)  # segundos
        }
        
        # Registra run
        run_info = {
            'run_id': f"run_{len(experiment_config['runs']) + 1}",
            'timestamp': datetime.now(),
            'parameters': parameters,
            'metrics': metrics,
            'status': 'FINISHED'
        }
        
        experiment_config['runs'].append(run_info)
        experiment_config['status'] = 'FINISHED'
        
        print(f"  üìà Accuracy: {accuracy:.4f}")
        print(f"  ‚è±Ô∏è Training time: {metrics['training_time']:.1f}s")
        
        return run_info
    
    def deploy_model(self, experiment_name: str, run_id: str, 
                    deployment_target: str = "staging"):
        """
        Deploy do modelo treinado
        """
        print(f"üöÄ Fazendo deploy do modelo...")
        
        if experiment_name not in self.experiments:
            raise ValueError(f"Experimento {experiment_name} n√£o encontrado")
        
        experiment = self.experiments[experiment_name]
        
        # Encontra o run espec√≠fico
        target_run = None
        for run in experiment['runs']:
            if run['run_id'] == run_id:
                target_run = run
                break
        
        if target_run is None:
            raise ValueError(f"Run {run_id} n√£o encontrado")
        
        # Configura√ß√£o do deployment
        deployment_config = {
            'model_name': f"{experiment_name}_model",
            'version': len(self.models.get(experiment_name, [])) + 1,
            'run_id': run_id,
            'target': deployment_target,
            'deployed_at': datetime.now(),
            'status': 'ACTIVE',
            'endpoint_url': f"https://model-serving.databricks.com/{experiment_name}",
            'metrics': target_run['metrics']
        }
        
        # Registra modelo
        if experiment_name not in self.models:
            self.models[experiment_name] = []
        
        self.models[experiment_name].append(deployment_config)
        
        print(f"‚úÖ Modelo deployado com sucesso!")
        print(f"üìä Vers√£o: {deployment_config['version']}")
        print(f"üåê Endpoint: {deployment_config['endpoint_url']}")
        print(f"üéØ Target: {deployment_target}")
        print(f"üìà Accuracy: {deployment_config['metrics']['accuracy']:.4f}")
        
        return deployment_config
    
    def create_notebook(self, notebook_name: str, language: str = "python"):
        """
        Cria notebook colaborativo
        """
        print(f"üìì Criando notebook: {notebook_name}")
        
        notebook_config = {
            'name': notebook_name,
            'language': language,
            'created_at': datetime.now(),
            'last_modified': datetime.now(),
            'cells': [],
            'collaborators': ['user@company.com'],
            'cluster_id': self.databricks.cluster_id
        }
        
        # Adiciona c√©lulas de exemplo
        if language == "python":
            example_cells = [
                {
                    'type': 'markdown',
                    'content': f"# {notebook_name}\n\nNotebook para an√°lise de Big Data"
                },
                {
                    'type': 'code',
                    'content': "# Importa√ß√µes\nimport pandas as pd\nimport numpy as np\nfrom pyspark.sql import SparkSession"
                },
                {
                    'type': 'code',
                    'content': "# Configura√ß√£o Spark\nspark = SparkSession.builder.appName('DataAnalysis').getOrCreate()"
                }
            ]
        elif language == "sql":
            example_cells = [
                {
                    'type': 'markdown',
                    'content': f"# {notebook_name}\n\nAn√°lise SQL no Databricks"
                },
                {
                    'type': 'sql',
                    'content': "-- Consulta de exemplo\nSELECT COUNT(*) FROM my_table"
                }
            ]
        
        notebook_config['cells'] = example_cells
        self.databricks.notebooks[notebook_name] = notebook_config
        
        print(f"‚úÖ Notebook {notebook_name} criado")
        print(f"üîó Cluster attached: {self.databricks.cluster_id}")
        print(f"üë• Colaboradores: {len(notebook_config['collaborators'])}")
        
        return notebook_config
    
    def schedule_job(self, job_name: str, notebook_path: str, 
                    schedule: str, cluster_config: Dict):
        """
        Agenda execu√ß√£o de job
        """
        print(f"‚è∞ Agendando job: {job_name}")
        
        job_config = {
            'name': job_name,
            'notebook_path': notebook_path,
            'schedule': schedule,
            'cluster_config': cluster_config,
            'created_at': datetime.now(),
            'status': 'ACTIVE',
            'last_run': None,
            'next_run': self._calculate_next_run(schedule),
            'runs_history': []
        }
        
        self.databricks.jobs[job_name] = job_config
        
        print(f"‚úÖ Job {job_name} agendado")
        print(f"üìÖ Schedule: {schedule}")
        print(f"‚è≠Ô∏è Pr√≥xima execu√ß√£o: {job_config['next_run']}")
        print(f"üìì Notebook: {notebook_path}")
        
        return job_config
    
    def _calculate_next_run(self, schedule: str) -> datetime:
        """Calcula pr√≥xima execu√ß√£o baseada no schedule"""
        
        now = datetime.now()
        
        if schedule == "daily":
            return now + timedelta(days=1)
        elif schedule == "hourly":
            return now + timedelta(hours=1)
        elif schedule.startswith("cron"):
            # Simplifica√ß√£o - em produ√ß√£o usaria croniter
            return now + timedelta(hours=24)
        else:
            return now + timedelta(hours=1)
    
    def monitor_cluster_performance(self, duration_minutes: int = 5):
        """
        Monitora performance do cluster
        """
        print(f"üìä Monitorando cluster por {duration_minutes} minutos...")
        
        metrics_history = []
        
        for minute in range(duration_minutes):
            # Simula m√©tricas do cluster
            metrics = {
                'timestamp': datetime.now(),
                'cpu_utilization': np.random.uniform(30, 90),
                'memory_utilization': np.random.uniform(40, 85),
                'disk_io': np.random.uniform(10, 60),
                'network_io': np.random.uniform(5, 40),
                'active_jobs': np.random.randint(0, 5),
                'queued_jobs': np.random.randint(0, 3)
            }
            
            metrics_history.append(metrics)
            
            print(f"  ‚è±Ô∏è Minuto {minute + 1}:")
            print(f"    CPU: {metrics['cpu_utilization']:.1f}%")
            print(f"    Memory: {metrics['memory_utilization']:.1f}%")
            print(f"    Active Jobs: {metrics['active_jobs']}")
            
            time.sleep(0.1)  # Simula tempo
        
        # An√°lise das m√©tricas
        avg_cpu = np.mean([m['cpu_utilization'] for m in metrics_history])
        avg_memory = np.mean([m['memory_utilization'] for m in metrics_history])
        max_jobs = max([m['active_jobs'] for m in metrics_history])
        
        print(f"\nüìà Resumo do Monitoramento:")
        print(f"  CPU m√©dio: {avg_cpu:.1f}%")
        print(f"  Memory m√©dio: {avg_memory:.1f}%")
        print(f"  Max jobs simult√¢neos: {max_jobs}")
        
        # Recomenda√ß√µes
        if avg_cpu > 80:
            print(f"‚ö†Ô∏è Alto uso de CPU - considere scaling up")
        if avg_memory > 80:
            print(f"‚ö†Ô∏è Alto uso de mem√≥ria - considere mais RAM")
        
        return metrics_history
    
    def optimize_cluster_cost(self, workload_pattern: str):
        """
        Otimiza custos do cluster baseado no padr√£o de workload
        """
        print(f"üí∞ Otimizando custos para workload: {workload_pattern}")
        
        recommendations = []
        
        if workload_pattern == "batch_processing":
            recommendations.extend([
                "Usar Spot instances para reduzir custos",
                "Implementar auto-scaling baseado em carga",
                "Agendar jobs em hor√°rios de menor custo",
                "Usar cluster pools para inicializa√ß√£o r√°pida"
            ])
            
        elif workload_pattern == "interactive_analytics":
            recommendations.extend([
                "Usar clusters persistentes durante hor√°rio comercial",
                "Implementar auto-termination para per√≠odos ociosos",
                "Considerar instance types otimizadas para mem√≥ria",
                "Usar Delta caching para melhor performance"
            ])
            
        elif workload_pattern == "ml_training":
            recommendations.extend([
                "Usar GPU instances para deep learning",
                "Implementar distributed training",
                "Usar MLflow para tracking de experimentos",
                "Considerar preemptible instances para experimentos"
            ])
            
        elif workload_pattern == "streaming":
            recommendations.extend([
                "Usar clusters dedicados para streaming",
                "Implementar checkpointing adequado",
                "Otimizar batch intervals",
                "Monitorar lag e throughput"
            ])
        
        # Simula c√°lculo de economia
        current_cost = np.random.uniform(1000, 5000)  # USD/m√™s
        potential_savings = np.random.uniform(0.15, 0.40)  # 15-40%
        estimated_savings = current_cost * potential_savings
        
        print(f"\nüíµ An√°lise de Custos:")
        print(f"  Custo atual estimado: ${current_cost:.2f}/m√™s")
        print(f"  Economia potencial: {potential_savings*100:.1f}%")
        print(f"  Economia estimada: ${estimated_savings:.2f}/m√™s")
        
        print(f"\nüéØ Recomenda√ß√µes:")
        for i, rec in enumerate(recommendations, 1):
            print(f"  {i}. {rec}")
        
        return {
            'current_cost': current_cost,
            'potential_savings_percent': potential_savings * 100,
            'estimated_monthly_savings': estimated_savings,
            'recommendations': recommendations
        }
    
    def run_complete_databricks_demo(self):
        """
        Executa demonstra√ß√£o completa da plataforma Databricks
        """
        print("üöÄ DEMONSTRA√á√ÉO COMPLETA - DATABRICKS ANALYTICS PLATFORM")
        print("="*70)
        
        results = {}
        
        # 1. Cria√ß√£o de tabelas Delta
        print("\n1Ô∏è‚É£ CRIA√á√ÉO DE TABELAS DELTA LAKE")
        print("-" * 50)
        
        # Tabela de vendas
        sales_schema = {
            'transaction_id': 'string',
            'customer_id': 'string',
            'product_id': 'string',
            'quantity': 'integer',
            'price': 'double',
            'total_amount': 'double',
            'transaction_date': 'timestamp',
            'category': 'string'
        }
        
        sales_table = self.create_delta_table(
            'sales_transactions', 
            sales_schema, 
            ['transaction_date', 'category']
        )
        
        # Tabela de clientes
        customer_schema = {
            'customer_id': 'string',
            'name': 'string',
            'age': 'integer',
            'location': 'string',
            'signup_date': 'timestamp',
            'premium_member': 'boolean'
        }
        
        customer_table = self.create_delta_table(
            'customers',
            customer_schema,
            ['location']
        )
        
        # 2. Opera√ß√µes UPSERT
        print("\n2Ô∏è‚É£ OPERA√á√ïES UPSERT E TIME TRAVEL")
        print("-" * 50)
        
        # Simula novos dados de vendas
        new_sales_data = self._generate_sample_data(sales_schema, 500)
        
        updated_sales = self.upsert_delta_table(
            'sales_transactions',
            new_sales_data,
            'transaction_id = new.transaction_id'
        )
        
        # Time travel query
        historical_data = self.time_travel_query('sales_transactions', version=0)
        
        # 3. Streaming Analytics
        print("\n3Ô∏è‚É£ STREAMING ANALYTICS")
        print("-" * 50)
        
        kafka_config = {
            'type': 'kafka',
            'brokers': 'localhost:9092',
            'topic': 'sales_events'
        }
        
        delta_sink = {
            'type': 'delta',
            'table': 'sales_stream',
            'mode': 'append'
        }
        
        streaming_job = self.create_streaming_job(
            'sales_realtime_analytics',
            kafka_config,
            delta_sink,
            'aggregation,windowing,filtering'
        )
        
        # 4. ML Experiments
        print("\n4Ô∏è‚É£ MACHINE LEARNING EXPERIMENTS")
        print("-" * 50)
        
        # Experimento de classifica√ß√£o
        rf_params = {
            'n_estimators': 100,
            'max_depth': 10,
            'min_samples_split': 5
        }
        
        rf_experiment = self.create_ml_experiment(
            'customer_churn_prediction',
            'random_forest',
            'customers',
            rf_params
        )
        
        # Experimento XGBoost
        xgb_params = {
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 200
        }
        
        xgb_experiment = self.create_ml_experiment(
            'sales_forecasting',
            'xgboost',
            'sales_transactions',
            xgb_params
        )
        
        # 5. Model Deployment
        print("\n5Ô∏è‚É£ MODEL DEPLOYMENT")
        print("-" * 50)
        
        # Deploy do melhor modelo
        churn_deployment = self.deploy_model(
            'customer_churn_prediction',
            'run_1',
            'production'
        )
        
        forecast_deployment = self.deploy_model(
            'sales_forecasting',
            'run_1',
            'staging'
        )
        
        # 6. Notebooks Colaborativos
        print("\n6Ô∏è‚É£ NOTEBOOKS COLABORATIVOS")
        print("-" * 50)
        
        analysis_notebook = self.create_notebook(
            'Sales_Analysis_Dashboard',
            'python'
        )
        
        sql_notebook = self.create_notebook(
            'Customer_Insights_SQL',
            'sql'
        )
        
        # 7. Job Scheduling
        print("\n7Ô∏è‚É£ AGENDAMENTO DE JOBS")
        print("-" * 50)
        
        cluster_config = {
            'node_type': 'i3.large',
            'num_workers': 2,
            'spark_version': '11.3.x-scala2.12'
        }
        
        daily_job = self.schedule_job(
            'daily_sales_report',
            '/notebooks/Sales_Analysis_Dashboard',
            'daily',
            cluster_config
        )
        
        # 8. Monitoramento
        print("\n8Ô∏è‚É£ MONITORAMENTO E OBSERVABILIDADE")
        print("-" * 50)
        
        performance_metrics = self.monitor_cluster_performance(3)
        
        # 9. Otimiza√ß√£o de Custos
        print("\n9Ô∏è‚É£ OTIMIZA√á√ÉO DE CUSTOS")
        print("-" * 50)
        
        cost_optimization = self.optimize_cluster_cost('interactive_analytics')
        
        # Compilar resultados
        results = {
            'delta_tables': {
                'sales_transactions': sales_table,
                'customers': customer_table
            },
            'streaming_jobs': [streaming_job],
            'ml_experiments': [rf_experiment, xgb_experiment],
            'model_deployments': [churn_deployment, forecast_deployment],
            'notebooks': [analysis_notebook, sql_notebook],
            'scheduled_jobs': [daily_job],
            'performance_metrics': performance_metrics,
            'cost_optimization': cost_optimization
        }
        
        print("\nüéâ DEMONSTRA√á√ÉO COMPLETA FINALIZADA!")
        print("="*70)
        print("üìä Resumo dos Resultados:")
        print(f"üóÉÔ∏è Tabelas Delta criadas: {len(results['delta_tables'])}")
        print(f"üåä Jobs de streaming: {len(results['streaming_jobs'])}")
        print(f"üß™ Experimentos ML: {len(results['ml_experiments'])}")
        print(f"üöÄ Modelos deployados: {len(results['model_deployments'])}")
        print(f"üìì Notebooks criados: {len(results['notebooks'])}")
        print(f"‚è∞ Jobs agendados: {len(results['scheduled_jobs'])}")
        print(f"üí∞ Economia estimada: ${cost_optimization['estimated_monthly_savings']:.2f}/m√™s")
        
        return results
    
    def cleanup(self):
        """Limpa recursos"""
        print("\nüßπ Limpando recursos...")
        print("‚úÖ Databricks Analytics Platform finalizada!")

# Demonstra√ß√£o principal
if __name__ == "__main__":
    
    print("üöÄ Iniciando Databricks Analytics Platform Demo")
    print("Este demo simula funcionalidades do Databricks para fins educacionais")
    print("-" * 70)
    
    # Inicializa plataforma
    platform = DatabricksAnalyticsPlatform("production")
    
    try:
        # Executa demo completo
        results = platform.run_complete_databricks_demo()
        
        print(f"\nüìà Demo executado com sucesso!")
        print(f"Funcionalidades demonstradas:")
        print(f"‚Ä¢ Delta Lake com versionamento e time travel")
        print(f"‚Ä¢ Streaming analytics em tempo real")
        print(f"‚Ä¢ MLflow para MLOps e experimenta√ß√£o")
        print(f"‚Ä¢ Notebooks colaborativos")
        print(f"‚Ä¢ Agendamento e orquestra√ß√£o de jobs")
        print(f"‚Ä¢ Monitoramento e observabilidade")
        print(f"‚Ä¢ Otimiza√ß√£o de custos")
        
    except Exception as e:
        print(f"‚ùå Erro durante execu√ß√£o: {e}")
        
    finally:
        # Cleanup
        platform.cleanup()

"""
CONCEITOS AVAN√áADOS DEMONSTRADOS:

1. üèóÔ∏è DATABRICKS ARCHITECTURE
   - Workspace colaborativo
   - Cluster management
   - Runtime environments
   - Integration com cloud providers

2. üìä DELTA LAKE
   - ACID transactions
   - Time travel e versionamento
   - Schema evolution
   - Upsert operations
   - Data lineage

3. üåä STREAMING ANALYTICS
   - Real-time processing
   - Structured streaming
   - Checkpointing
   - Exactly-once processing
   - Late data handling

4. ü§ñ MLFLOW E MLOPS
   - Experiment tracking
   - Model registry
   - Model deployment
   - A/B testing
   - Model monitoring

5. üìì COLLABORATIVE NOTEBOOKS
   - Multi-language support
   - Version control
   - Real-time collaboration
   - Visualization widgets
   - Dashboard creation

6. ‚öôÔ∏è JOB ORCHESTRATION
   - Workflow scheduling
   - Dependency management
   - Error handling
   - Resource optimization
   - Multi-task workflows

7. üìà MONITORING & OBSERVABILITY
   - Cluster metrics
   - Job performance
   - Cost tracking
   - Resource utilization
   - Alert management

8. üí∞ COST OPTIMIZATION
   - Spot instances
   - Auto-scaling
   - Resource right-sizing
   - Usage patterns analysis
   - Cost allocation

APLICA√á√ïES REAIS:
- Data lakehouse architecture
- Real-time analytics
- ML platform as a service
- Data engineering pipelines
- Business intelligence
- Customer 360 analytics
- Fraud detection
- Recommendation systems
"""
