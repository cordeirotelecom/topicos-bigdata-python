#!/usr/bin/env python3
"""
Aula 05: Pipeline Completo de An√°lise de Dados
Professor: Vagner Cordeiro
Curso: T√≥picos de Big Data em Python

Este script demonstra um pipeline completo de an√°lise de dados,
desde a coleta at√© insights finais, com boas pr√°ticas e ferramentas modernas.
"""

# Importa√ß√µes com tratamento de erros
try:
    import pandas
    import numpy as np
    PANDAS_AVAILABLE = True
    print("‚úÖ Pandas e NumPy dispon√≠veis")
except ImportError:
    print("‚ö†Ô∏è Pandas/NumPy n√£o est√° instalado. Usando simula√ß√£o de conceitos.")
    PANDAS_AVAILABLE = False
    # Criando stubs para tipos
    pandas = None
    class pd:
        class DataFrame:
            pass

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    MATPLOTLIB_AVAILABLE = True
    print("‚úÖ Matplotlib e Seaborn dispon√≠veis")
except ImportError:
    print("‚ö†Ô∏è Matplotlib/Seaborn n√£o est√° instalado. Visualiza√ß√µes ser√£o simuladas.")
    MATPLOTLIB_AVAILABLE = False

try:
    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
    print("‚úÖ Plotly dispon√≠vel")
except ImportError:
    print("‚ö†Ô∏è Plotly n√£o est√° instalado. Gr√°ficos interativos ser√£o simulados.")
    PLOTLY_AVAILABLE = False

try:
    from scipy import stats
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression, LogisticRegression
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import classification_report, confusion_matrix, r2_score
    SKLEARN_AVAILABLE = True
    print("‚úÖ SciPy e Scikit-learn dispon√≠veis")
except ImportError:
    print("‚ö†Ô∏è SciPy/Scikit-learn n√£o est√° instalado. Machine learning ser√° simulado.")
    SKLEARN_AVAILABLE = False

import warnings
warnings.filterwarnings('ignore')

from datetime import datetime, timedelta
import random
import sqlite3
import json
from typing import Optional, Dict, List, Any, Union

# Configurar estilo dos gr√°ficos se dispon√≠vel
if MATPLOTLIB_AVAILABLE:
    try:
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
    except:
        pass

class DataAnalysisPipeline:
    """Pipeline completo para an√°lise de dados"""
    
    def __init__(self, name: str = "Data Analysis Pipeline"):
        self.name = name
        self.data: Any = None  # Pode ser DataFrame ou dict para simula√ß√£o
        self.cleaned_data: Any = None  # Pode ser DataFrame ou dict para simula√ß√£o
        self.analysis_results: Dict[str, Any] = {}
        self.insights: List[str] = []
        
    def load_data(self, data_source: Any, source_type: str = "dataframe") -> None:
        """Carrega dados de diferentes fontes"""
        print(f"üì• Carregando dados de: {source_type}")
        
        if not PANDAS_AVAILABLE:
            print("üìä Simulando carregamento de dados...")
            self.data = self._simulate_dataframe()
            print(f"‚úÖ Dados simulados carregados: 1000 linhas, 10 colunas")
            return
        
        try:
            if source_type == "dataframe":
                self.data = data_source
            elif source_type == "csv":
                self.data = pandas.read_csv(data_source) if pandas else None
            elif source_type == "json":
                self.data = pandas.read_json(data_source) if pandas else None
            elif source_type == "sql":
                # data_source seria uma tupla (connection, query)
                conn, query = data_source
                self.data = pandas.read_sql_query(query, conn) if pandas else None
            
            if self.data is not None and hasattr(self.data, 'shape'):
                print(f"‚úÖ Dados carregados: {self.data.shape[0]} linhas, {self.data.shape[1]} colunas")
            else:
                print("‚úÖ Dados carregados com sucesso")
                
        except Exception as e:
            print(f"‚ùå Erro ao carregar dados: {e}")
            self.data = self._simulate_dataframe()
            print("üìä Usando dados simulados para demonstra√ß√£o")
    
    def _simulate_dataframe(self) -> Dict[str, Any]:
        """Simula um DataFrame quando pandas n√£o est√° dispon√≠vel"""
        return {
            'vendas': [random.randint(100, 1000) for _ in range(100)],
            'categoria': [random.choice(['A', 'B', 'C']) for _ in range(100)],
            'regiao': [random.choice(['Norte', 'Sul', 'Leste', 'Oeste']) for _ in range(100)],
            'data': [datetime.now() - timedelta(days=random.randint(1, 365)) for _ in range(100)],
            'preco': [random.uniform(10, 100) for _ in range(100)]
        }
    
    def explore_data(self) -> None:
        """Explora os dados iniciais"""
        print("\nüîç EXPLORA√á√ÉO INICIAL DOS DADOS")
        print("=" * 50)
        
        if not PANDAS_AVAILABLE:
            print("üìä Simulando explora√ß√£o de dados...")
            print("Shape simulado: (1000, 10)")
            print("Colunas simuladas: vendas, categoria, regi√£o, data, pre√ßo...")
            print("Tipos de dados: num√©ricos e categ√≥ricos")
            print("Valores nulos: 5% dos dados simulados")
            return
        
        if self.data is None:
            print("‚ùå Nenhum dado carregado")
            return
            
        try:
            print(f"Shape dos dados: {self.data.shape}")
            print(f"\nColunas: {list(self.data.columns)}")
            print(f"\nTipos de dados:")
            print(self.data.dtypes)
            print(f"\nInforma√ß√µes gerais:")
            print(self.data.info())
            print(f"\nPrimeiras 5 linhas:")
            print(self.data.head())
            print(f"\nEstat√≠sticas descritivas:")
            print(self.data.describe())
            print(f"\nValores nulos por coluna:")
            print(self.data.isnull().sum())
            
        except Exception as e:
            print(f"‚ùå Erro na explora√ß√£o: {e}")
            print("üìä Dados carregados, mas explora√ß√£o limitada")
    
    def clean_data(self) -> None:
        """Limpa e prepara os dados"""
        print("\nüßπ LIMPEZA DOS DADOS")
        print("=" * 50)
        
        if not PANDAS_AVAILABLE:
            print("üìä Simulando limpeza de dados...")
            self.cleaned_data = self._simulate_dataframe()
            print("‚úÖ Dados simulados limpos")
            print("- Valores nulos tratados")
            print("- Outliers removidos")
            print("- Texto padronizado")
            return
        
        if self.data is None:
            print("‚ùå Nenhum dado para limpar")
            return
            
        try:
            # Copiar dados originais
            self.cleaned_data = self.data.copy()
            
            # 1. Tratar valores nulos
            self._handle_missing_values()
            
            # 2. Padronizar texto
            self._standardize_text()
            
            # 3. Remover outliers
            self._remove_outliers()
            
            print("‚úÖ Limpeza conclu√≠da")
            
        except Exception as e:
            print(f"‚ùå Erro na limpeza: {e}")
            self.cleaned_data = self.data
    
    def _handle_missing_values(self) -> None:
        """Trata valores nulos"""
        print("üîß Tratando valores nulos...")
        
        if self.cleaned_data is None or not PANDAS_AVAILABLE:
            return
            
        try:
            # Para colunas num√©ricas
            numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns
            categorical_cols = self.cleaned_data.select_dtypes(include=['object', 'category']).columns
            
            for col in numeric_cols:
                if self.cleaned_data[col].isnull().sum() > 0:
                    missing_count = self.cleaned_data[col].isnull().sum()
                    if self.cleaned_data[col].dtype in ['int64', 'int32']:
                        fill_value = self.cleaned_data[col].median()
                    else:
                        fill_value = self.cleaned_data[col].mean()
                    
                    self.cleaned_data[col].fillna(fill_value, inplace=True)
                    print(f"   - {col}: {missing_count} valores preenchidos com {fill_value:.2f}")
            
            # Para colunas categ√≥ricas
            for col in categorical_cols:
                if self.cleaned_data[col].isnull().sum() > 0:
                    missing_count = self.cleaned_data[col].isnull().sum()
                    fill_value = self.cleaned_data[col].mode().iloc[0] if not self.cleaned_data[col].mode().empty else 'Unknown'
                    self.cleaned_data[col].fillna(fill_value, inplace=True)
                    print(f"   - {col}: {missing_count} valores preenchidos com '{fill_value}'")
                    
        except Exception as e:
            print(f"   ‚ùå Erro no tratamento de nulos: {e}")
    
    def _standardize_text(self) -> None:
        """Padroniza campos de texto"""
        print("üîß Padronizando texto...")
        
        if self.cleaned_data is None or not PANDAS_AVAILABLE:
            return
            
        try:
            text_cols = self.cleaned_data.select_dtypes(include=['object']).columns
            
            for col in text_cols:
                # Remove espa√ßos extras
                self.cleaned_data[col] = self.cleaned_data[col].astype(str).str.strip()
                # Capitaliza se for categoria pequena
                if self.cleaned_data[col].nunique() / len(self.cleaned_data) < 0.1:
                    self.cleaned_data[col] = self.cleaned_data[col].str.title()
                    
        except Exception as e:
            print(f"   ‚ùå Erro na padroniza√ß√£o: {e}")
    
    def _remove_outliers(self) -> None:
        """Remove outliers usando IQR"""
        print("üîß Removendo outliers...")
        
        if self.cleaned_data is None or not PANDAS_AVAILABLE:
            return
            
        try:
            if self.cleaned_data is None:
                print("   ‚ö†Ô∏è Dados n√£o dispon√≠veis para remo√ß√£o de outliers")
                return
                
            if not hasattr(self.cleaned_data, 'select_dtypes'):
                print("   ‚ö†Ô∏è Utilizando simula√ß√£o para remo√ß√£o de outliers")
                return
                
            numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns
            initial_size = len(self.cleaned_data)
            
            for col in numeric_cols:
                Q1 = self.cleaned_data[col].quantile(0.25)
                Q3 = self.cleaned_data[col].quantile(0.75)
                IQR = Q3 - Q1
                
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = (self.cleaned_data[col] < lower_bound) | (self.cleaned_data[col] > upper_bound)
                outlier_count = outliers.sum()
                
                if outlier_count > 0:
                    print(f"   - {col}: {outlier_count} outliers identificados")
                    # Remove outliers
                    self.cleaned_data = self.cleaned_data[~outliers]
            
            removed = initial_size - len(self.cleaned_data)
            if removed > 0:
                print(f"   üìä Total de linhas removidas: {removed}")
                
        except Exception as e:
            print(f"   ‚ùå Erro na remo√ß√£o de outliers: {e}")
            print("   üîÑ Utilizando simula√ß√£o de remo√ß√£o de outliers")
    
    def analyze_correlations(self, target_column: Optional[str] = None) -> None:
        """Analisa correla√ß√µes entre vari√°veis"""
        print("\nüìä AN√ÅLISE DE CORRELA√á√ïES")
        print("=" * 50)
        
        if not PANDAS_AVAILABLE:
            print("üìä Simulando an√°lise de correla√ß√µes...")
            print("Correla√ß√µes encontradas:")
            print("- vendas x pre√ßo: 0.85 (alta correla√ß√£o positiva)")
            print("- vendas x categoria: 0.45 (correla√ß√£o moderada)")
            print("- pre√ßo x regi√£o: -0.12 (correla√ß√£o fraca negativa)")
            return
        
        if self.cleaned_data is None:
            print("‚ùå Dados n√£o est√£o limpos")
            return
            
        try:
            # Selecionar apenas colunas num√©ricas
            numeric_data = self.cleaned_data.select_dtypes(include=[np.number])
            
            if numeric_data.empty:
                print("‚ùå Nenhuma coluna num√©rica encontrada")
                return
            
            # Calcular matriz de correla√ß√£o
            correlation_matrix = numeric_data.corr()
            
            print("üìà Matriz de correla√ß√£o:")
            print(correlation_matrix)
            
            # Encontrar correla√ß√µes mais fortes
            if target_column and target_column in correlation_matrix.columns:
                correlations = correlation_matrix[target_column].drop(target_column)
                correlations = correlations.reindex(correlations.abs().sort_values(ascending=False).index)
                
                print(f"\nüéØ Correla√ß√µes com {target_column}:")
                for var, corr in correlations.head(5).items():
                    strength = self._interpret_correlation(abs(corr))
                    direction = "positiva" if corr > 0 else "negativa"
                    print(f"   - {var}: {corr:.3f} ({strength} {direction})")
            
            # Criar visualiza√ß√£o se poss√≠vel
            if MATPLOTLIB_AVAILABLE:
                self._plot_correlation_heatmap(correlation_matrix)
                
        except Exception as e:
            print(f"‚ùå Erro na an√°lise de correla√ß√µes: {e}")
    
    def _interpret_correlation(self, corr_value: float) -> str:
        """Interpreta o valor de correla√ß√£o"""
        if corr_value >= 0.7:
            return "forte"
        elif corr_value >= 0.3:
            return "moderada"
        else:
            return "fraca"
    
    def _plot_correlation_heatmap(self, correlation_matrix: Any) -> None:
        """Cria heatmap de correla√ß√£o"""
        try:
            plt.figure(figsize=(10, 8))
            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
                       square=True, linewidths=0.5)
            plt.title('Matriz de Correla√ß√£o')
            plt.tight_layout()
            plt.show()
            print("üìä Heatmap de correla√ß√£o criado")
        except Exception as e:
            print(f"‚ùå Erro ao criar heatmap: {e}")
    
    def create_visualizations(self) -> None:
        """Cria visualiza√ß√µes dos dados"""
        print("\nüìä CRIA√á√ÉO DE VISUALIZA√á√ïES")
        print("=" * 50)
        
        if not PANDAS_AVAILABLE:
            print("üìä Simulando cria√ß√£o de visualiza√ß√µes...")
            print("Gr√°ficos criados:")
            print("- Histograma de vendas")
            print("- Boxplot por categoria")
            print("- Gr√°fico de dispers√£o vendas x pre√ßo")
            print("- Gr√°fico de pizza por regi√£o")
            return
        
        if self.cleaned_data is None:
            print("‚ùå Dados n√£o est√£o limpos")
            return
            
        try:
            # An√°lise univariada
            self._create_univariate_plots()
            
            # An√°lise bivariada
            self._create_bivariate_plots()
            
            # An√°lise temporal se houver dados de data
            self._create_temporal_plots()
            
        except Exception as e:
            print(f"‚ùå Erro na cria√ß√£o de visualiza√ß√µes: {e}")
    
    def _create_univariate_plots(self) -> None:
        """Cria gr√°ficos univariados"""
        if not MATPLOTLIB_AVAILABLE:
            print("üìä Visualiza√ß√µes univariadas simuladas")
            return
            
        try:
            numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns
            categorical_cols = self.cleaned_data.select_dtypes(include=['object']).columns
            
            # Histogramas para vari√°veis num√©ricas
            for col in numeric_cols[:3]:  # Limitar a 3 para n√£o sobrecarregar
                plt.figure(figsize=(8, 6))
                plt.hist(self.cleaned_data[col].dropna(), bins=30, alpha=0.7, edgecolor='black')
                plt.title(f'Distribui√ß√£o de {col}')
                plt.xlabel(col)
                plt.ylabel('Frequ√™ncia')
                plt.show()
                print(f"üìä Histograma criado para {col}")
            
            # Gr√°ficos de pizza para vari√°veis categ√≥ricas
            for col in categorical_cols[:2]:  # Limitar a 2
                plt.figure(figsize=(8, 8))
                value_counts = self.cleaned_data[col].value_counts()
                if len(value_counts) <= 10:  # Apenas se n√£o houver muitas categorias
                    plt.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%')
                    plt.title(f'Distribui√ß√£o de {col}')
                    plt.show()
                    print(f"üìä Gr√°fico de pizza criado para {col}")
                    
        except Exception as e:
            print(f"‚ùå Erro nos gr√°ficos univariados: {e}")
    
    def _create_bivariate_plots(self) -> None:
        """Cria gr√°ficos bivariados"""
        if not MATPLOTLIB_AVAILABLE:
            print("üìä Visualiza√ß√µes bivariadas simuladas")
            return
            
        try:
            numeric_cols = list(self.cleaned_data.select_dtypes(include=[np.number]).columns)
            
            if len(numeric_cols) >= 2:
                # Scatter plot entre duas primeiras vari√°veis num√©ricas
                plt.figure(figsize=(8, 6))
                plt.scatter(self.cleaned_data[numeric_cols[0]], 
                          self.cleaned_data[numeric_cols[1]], alpha=0.6)
                plt.xlabel(numeric_cols[0])
                plt.ylabel(numeric_cols[1])
                plt.title(f'{numeric_cols[0]} vs {numeric_cols[1]}')
                plt.show()
                print(f"üìä Scatter plot criado: {numeric_cols[0]} vs {numeric_cols[1]}")
                
        except Exception as e:
            print(f"‚ùå Erro nos gr√°ficos bivariados: {e}")
    
    def _create_temporal_plots(self) -> None:
        """Cria gr√°ficos temporais se houver dados de data"""
        if not MATPLOTLIB_AVAILABLE:
            print("üìä Visualiza√ß√µes temporais simuladas")
            return
            
        try:
            # Procurar colunas de data
            date_cols = []
            for col in self.cleaned_data.columns:
                if 'data' in col.lower() or 'date' in col.lower() or 'time' in col.lower():
                    date_cols.append(col)
            
            if date_cols:
                print(f"üìä An√°lise temporal criada para: {date_cols[0]}")
            else:
                print("üìä Nenhuma coluna temporal identificada")
                
        except Exception as e:
            print(f"‚ùå Erro nos gr√°ficos temporais: {e}")
    
    def build_models(self, target_column: Optional[str] = None) -> None:
        """Constr√≥i modelos de machine learning"""
        print("\nü§ñ CONSTRU√á√ÉO DE MODELOS")
        print("=" * 50)
        
        if not SKLEARN_AVAILABLE:
            print("üìä Simulando constru√ß√£o de modelos...")
            print("Modelos criados:")
            print("- Regress√£o Linear: R¬≤ = 0.85")
            print("- Random Forest: Acur√°cia = 0.92")
            print("- Regress√£o Log√≠stica: F1-Score = 0.88")
            return
        
        if self.cleaned_data is None:
            print("‚ùå Dados n√£o est√£o limpos")
            return
            
        if target_column is None:
            numeric_cols = list(self.cleaned_data.select_dtypes(include=[np.number]).columns)
            if numeric_cols:
                target_column = numeric_cols[0]
                print(f"üéØ Usando {target_column} como vari√°vel alvo")
            else:
                print("‚ùå Nenhuma vari√°vel alvo especificada")
                return
        
        try:
            # Preparar dados
            features = list(self.cleaned_data.select_dtypes(include=[np.number]).columns)
            if target_column in features:
                features.remove(target_column)
            
            if not features:
                print("‚ùå Nenhuma feature num√©rica dispon√≠vel")
                return
            
            X = self.cleaned_data[features]
            y = self.cleaned_data[target_column]
            
            # Dividir dados
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # Modelo de regress√£o
            if self.cleaned_data[target_column].dtype in ['float64', 'int64']:
                self._build_regression_model(X_train, X_test, y_train, y_test, target_column)
            
            # Modelo de classifica√ß√£o (se a vari√°vel alvo tem poucas categorias √∫nicas)
            if self.cleaned_data[target_column].nunique() <= 10:
                self._build_classification_model(X_train, X_test, y_train, y_test, target_column)
                
        except Exception as e:
            print(f"‚ùå Erro na constru√ß√£o de modelos: {e}")
    
    def _build_regression_model(self, X_train: Any, X_test: Any, y_train: Any, y_test: Any, target: str) -> None:
        """Constr√≥i modelo de regress√£o"""
        try:
            # Regress√£o Linear
            lr_model = LinearRegression()
            lr_model.fit(X_train, y_train)
            
            y_pred = lr_model.predict(X_test)
            r2 = r2_score(y_test, y_pred)
            
            print(f"üìà Regress√£o Linear para {target}:")
            print(f"   - R¬≤ Score: {r2:.3f}")
            
            # Salvar modelo
            self.analysis_results[f'regression_{target}'] = {
                'model': lr_model,
                'r2_score': r2,
                'features': list(X_train.columns)
            }
            
        except Exception as e:
            print(f"‚ùå Erro na regress√£o: {e}")
    
    def _build_classification_model(self, X_train: Any, X_test: Any, y_train: Any, y_test: Any, target: str) -> None:
        """Constr√≥i modelo de classifica√ß√£o"""
        try:
            # Random Forest
            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
            rf_model.fit(X_train, y_train)
            
            y_pred = rf_model.predict(X_test)
            accuracy = (y_pred == y_test).mean()
            
            print(f"üå≥ Random Forest para {target}:")
            print(f"   - Acur√°cia: {accuracy:.3f}")
            
            # Import√¢ncia das features
            feature_importance = rf_model.feature_importances_
            for feature, importance in zip(X_train.columns, feature_importance):
                print(f"   - {feature}: {importance:.3f}")
            
            # Salvar modelo
            self.analysis_results[f'classification_{target}'] = {
                'model': rf_model,
                'accuracy': accuracy,
                'features': list(X_train.columns),
                'feature_importance': dict(zip(X_train.columns, feature_importance))
            }
            
        except Exception as e:
            print(f"‚ùå Erro na classifica√ß√£o: {e}")
    
    def generate_insights(self) -> None:
        """Gera insights e conclus√µes"""
        print("\nüí° INSIGHTS E CONCLUS√ïES")
        print("=" * 50)
        
        self.insights = []
        
        if self.cleaned_data is None:
            self.insights.append("‚ö†Ô∏è An√°lise limitada - dados n√£o carregados adequadamente")
        
        # Insights sobre a qualidade dos dados
        if PANDAS_AVAILABLE and self.cleaned_data is not None:
            try:
                total_rows = len(self.cleaned_data)
                total_cols = len(self.cleaned_data.columns)
                
                self.insights.append(f"üìä Dataset cont√©m {total_rows:,} registros e {total_cols} vari√°veis")
                
                # Verificar balanceamento se houver vari√°veis categ√≥ricas
                categorical_cols = self.cleaned_data.select_dtypes(include=['object']).columns
                for col in categorical_cols[:2]:  # Limitar an√°lise
                    value_counts = self.cleaned_data[col].value_counts()
                    if len(value_counts) <= 10:
                        most_common = value_counts.index[0]
                        percentage = (value_counts.iloc[0] / total_rows) * 100
                        self.insights.append(f"üìà {col}: categoria '{most_common}' representa {percentage:.1f}% dos dados")
                
                # Insights sobre vari√°veis num√©ricas
                numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns
                for col in numeric_cols[:3]:  # Limitar an√°lise
                    mean_val = self.cleaned_data[col].mean()
                    std_val = self.cleaned_data[col].std()
                    cv = (std_val / mean_val) * 100 if mean_val != 0 else 0
                    
                    if cv > 50:
                        self.insights.append(f"üìä {col}: alta variabilidade (CV = {cv:.1f}%)")
                    elif cv < 10:
                        self.insights.append(f"üìä {col}: baixa variabilidade (CV = {cv:.1f}%)")
                        
            except Exception as e:
                self.insights.append(f"‚ö†Ô∏è Erro na an√°lise de insights: {e}")
        
        # Insights sobre modelos
        for model_name, results in self.analysis_results.items():
            if 'r2_score' in results:
                r2 = results['r2_score']
                if r2 > 0.8:
                    self.insights.append(f"üéØ Modelo {model_name}: excelente poder preditivo (R¬≤ = {r2:.3f})")
                elif r2 > 0.6:
                    self.insights.append(f"üéØ Modelo {model_name}: bom poder preditivo (R¬≤ = {r2:.3f})")
                else:
                    self.insights.append(f"üéØ Modelo {model_name}: poder preditivo limitado (R¬≤ = {r2:.3f})")
            
            if 'accuracy' in results:
                acc = results['accuracy']
                if acc > 0.9:
                    self.insights.append(f"üéØ Modelo {model_name}: excelente acur√°cia ({acc:.3f})")
                elif acc > 0.7:
                    self.insights.append(f"üéØ Modelo {model_name}: boa acur√°cia ({acc:.3f})")
        
        # Adicionar insights gerais
        if not self.insights:
            self.insights.extend([
                "üìä Pipeline de an√°lise executado com sucesso",
                "üîç Dados explorados e processados adequadamente",
                "üìà Visualiza√ß√µes criadas para entendimento dos padr√µes",
                "ü§ñ Modelos constru√≠dos para predi√ß√£o e classifica√ß√£o"
            ])
        
        # Exibir insights
        for i, insight in enumerate(self.insights, 1):
            print(f"{i:2d}. {insight}")
    
    def export_results(self, filename: str = "analysis_results.json") -> None:
        """Exporta resultados da an√°lise"""
        print(f"\nüíæ EXPORTANDO RESULTADOS PARA {filename}")
        print("=" * 50)
        
        try:
            results = {
                'pipeline_name': self.name,
                'execution_date': datetime.now().isoformat(),
                'insights': self.insights,
                'data_info': {
                    'rows': len(self.cleaned_data) if self.cleaned_data is not None and PANDAS_AVAILABLE else 'N/A',
                    'columns': len(self.cleaned_data.columns) if self.cleaned_data is not None and PANDAS_AVAILABLE else 'N/A'
                },
                'models_summary': {}
            }
            
            # Adicionar resumo dos modelos (sem os objetos modelo)
            for model_name, model_data in self.analysis_results.items():
                results['models_summary'][model_name] = {
                    key: value for key, value in model_data.items() 
                    if key not in ['model']  # Excluir objeto modelo
                }
            
            # Salvar arquivo
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"‚úÖ Resultados exportados para {filename}")
            
        except Exception as e:
            print(f"‚ùå Erro ao exportar: {e}")
    
    def run_complete_pipeline(self, data_source: Any = None, target_column: Optional[str] = None) -> None:
        """Executa pipeline completo de an√°lise"""
        print("üöÄ INICIANDO PIPELINE COMPLETO DE AN√ÅLISE DE DADOS")
        print("=" * 60)
        
        try:
            # 1. Carregar dados
            if data_source is not None:
                self.load_data(data_source)
            else:
                # Gerar dados de exemplo
                self.load_data(self._generate_sample_data(), "dataframe")
            
            # 2. Explorar dados
            self.explore_data()
            
            # 3. Limpar dados
            self.clean_data()
            
            # 4. Analisar correla√ß√µes
            self.analyze_correlations(target_column)
            
            # 5. Criar visualiza√ß√µes
            self.create_visualizations()
            
            # 6. Construir modelos
            self.build_models(target_column)
            
            # 7. Gerar insights
            self.generate_insights()
            
            # 8. Exportar resultados
            self.export_results()
            
            print("\nüéâ PIPELINE COMPLETO EXECUTADO COM SUCESSO!")
            print("=" * 60)
            
        except Exception as e:
            print(f"\n‚ùå ERRO NO PIPELINE: {e}")
            print("Pipeline interrompido, mas resultados parciais podem estar dispon√≠veis.")
    
    def _generate_sample_data(self) -> Any:
        """Gera dados de exemplo para demonstra√ß√£o"""
        if not PANDAS_AVAILABLE:
            return self._simulate_dataframe()
        
        # Gerar dados sint√©ticos mais realistas
        np.random.seed(42)
        n_samples = 1000
        
        data = {
            'vendas': np.random.normal(500, 150, n_samples),
            'preco': np.random.uniform(10, 100, n_samples),
            'categoria': np.random.choice(['Eletr√¥nicos', 'Roupas', 'Casa', 'Livros'], n_samples),
            'regiao': np.random.choice(['Norte', 'Sul', 'Leste', 'Oeste'], n_samples),
            'desconto': np.random.uniform(0, 0.3, n_samples),
            'avaliacao': np.random.uniform(1, 5, n_samples),
            'idade_cliente': np.random.normal(35, 12, n_samples),
            'data_compra': [datetime.now() - timedelta(days=np.random.randint(1, 365)) for _ in range(n_samples)]
        }
        
        # Criar correla√ß√µes realistas
        # Adicionar alguns valores nulos
        missing_indices = np.random.choice(n_samples, size=int(0.05 * n_samples), replace=False)
        for idx in missing_indices:
            col = np.random.choice(['preco', 'avaliacao', 'idade_cliente'])
            data[col][idx] = np.nan
        
        # Calcular vendas DEPOIS de adicionar NaN para evitar erro de convers√£o
        data['vendas'] = (data['preco'] * np.random.uniform(8, 12, n_samples) * 
                         (1 - data['desconto']) * 
                         np.random.normal(1, 0.2, n_samples))
        
        # Converter para int apenas onde n√£o h√° NaN
        mask = ~np.isnan(data['vendas'])
        data['vendas'][mask] = data['vendas'][mask].astype(int)
        
        return pandas.DataFrame(data) if pandas else {"message": "Dados simulados gerados", "rows": n_samples}


def demonstrate_pipeline():
    """Demonstra o uso do pipeline"""
    print("üéì DEMONSTRA√á√ÉO: Pipeline de An√°lise de Dados")
    print("=" * 60)
    
    # Criar inst√¢ncia do pipeline
    pipeline = DataAnalysisPipeline("Demo - An√°lise de Vendas")
    
    # Executar pipeline completo
    pipeline.run_complete_pipeline(target_column='vendas')


def demonstrate_concepts():
    """Demonstra conceitos fundamentais independente das bibliotecas"""
    print("\nüìö CONCEITOS FUNDAMENTAIS DE AN√ÅLISE DE DADOS")
    print("=" * 60)
    
    print("""
    1. üìä EXPLORA√á√ÉO DE DADOS (EDA)
       - Entender a estrutura dos dados
       - Identificar padr√µes e anomalias
       - Calcular estat√≠sticas descritivas
    
    2. üßπ LIMPEZA DE DADOS
       - Tratar valores nulos
       - Remover outliers
       - Padronizar formatos
    
    3. üìà AN√ÅLISE ESTAT√çSTICA
       - Correla√ß√µes entre vari√°veis
       - Distribui√ß√µes
       - Testes de hip√≥teses
    
    4. üìä VISUALIZA√á√ÉO
       - Histogramas para distribui√ß√µes
       - Scatter plots para correla√ß√µes
       - Box plots para outliers
    
    5. ü§ñ MACHINE LEARNING
       - Regress√£o para valores cont√≠nuos
       - Classifica√ß√£o para categorias
       - Valida√ß√£o de modelos
    
    6. üí° GERA√á√ÉO DE INSIGHTS
       - Interpretar resultados
       - Identificar padr√µes de neg√≥cio
       - Propor a√ß√µes baseadas em dados
    """)


if __name__ == "__main__":
    print("üéØ AULA 05: Pipeline Completo de An√°lise de Dados")
    print("Autor: Professor Vagner Cordeiro")
    print("=" * 60)
    
    # Demonstrar conceitos
    demonstrate_concepts()
    
    # Executar demonstra√ß√£o
    demonstrate_pipeline()
    
    print("\n‚úÖ Aula conclu√≠da! Pipeline de an√°lise de dados demonstrado com sucesso.")
