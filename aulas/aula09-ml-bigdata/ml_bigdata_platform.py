#!/usr/bin/env python3
"""
Aula 09: Machine Learning com Big Data
Professor: Vagner Cordeiro
Curso: T√≥picos de Big Data em Python

Plataforma completa de Machine Learning para Big Data usando PySpark
e t√©cnicas avan√ßadas de processamento distribu√≠do.
"""

import os
import sys
import random
import json
import time
import math
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

# Importa√ß√µes com tratamento de erros
try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, when, isnan, isnull, count, mean, stddev, max as spark_max, min as spark_min
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType
    from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder
    from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier
    from pyspark.ml.regression import LinearRegression, RandomForestRegressor
    from pyspark.ml.clustering import KMeans
    from pyspark.ml.recommendation import ALS
    from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator, ClusteringEvaluator
    from pyspark.ml import Pipeline
    PYSPARK_AVAILABLE = True
    print("‚úÖ PySpark dispon√≠vel")
except ImportError:
    print("‚ö†Ô∏è PySpark n√£o est√° instalado. Usando simula√ß√£o de conceitos.")
    PYSPARK_AVAILABLE = False

try:
    import pandas as pd
    import numpy as np
    PANDAS_AVAILABLE = True
    print("‚úÖ Pandas e NumPy dispon√≠veis")
except ImportError:
    print("‚ö†Ô∏è Pandas/NumPy n√£o dispon√≠vel. Funcionalidades limitadas.")
    PANDAS_AVAILABLE = False

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    MATPLOTLIB_AVAILABLE = True
    print("‚úÖ Matplotlib e Seaborn dispon√≠veis")
except ImportError:
    print("‚ö†Ô∏è Matplotlib/Seaborn n√£o dispon√≠vel. Visualiza√ß√µes ser√£o simuladas.")
    MATPLOTLIB_AVAILABLE = False

class BigDataMLPlatform:
    """Plataforma de Machine Learning para Big Data"""
    
    def __init__(self, app_name: str = "BigData ML Platform"):
        """Inicializa a plataforma"""
        self.app_name = app_name
        self.spark = None
        self.models = {}
        self.datasets = {}
        self.results = {}
        
        if PYSPARK_AVAILABLE:
            self._initialize_spark()
        else:
            print("üé≠ Modo simula√ß√£o: PySpark n√£o dispon√≠vel")
    
    def _initialize_spark(self) -> None:
        """Inicializa sess√£o Spark"""
        try:
            self.spark = SparkSession.builder \
                .appName(self.app_name) \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .getOrCreate()
            
            self.spark.sparkContext.setLogLevel("WARN")
            print(f"‚úÖ Spark inicializado: {self.spark.version}")
            
        except Exception as e:
            print(f"‚ùå Erro ao inicializar Spark: {e}")
            self.spark = None
    
    def generate_fraud_detection_data(self, num_transactions: int = 100000) -> Any:
        """Gera dataset sint√©tico para detec√ß√£o de fraude"""
        print(f"üìä Gerando {num_transactions:,} transa√ß√µes para detec√ß√£o de fraude...")
        
        if not PYSPARK_AVAILABLE:
            return self._simulate_fraud_data(num_transactions)
        
        # Schema dos dados
        schema = StructType([
            StructField("transaction_id", StringType(), True),
            StructField("user_id", StringType(), True),
            StructField("amount", DoubleType(), True),
            StructField("merchant", StringType(), True),
            StructField("category", StringType(), True),
            StructField("hour", IntegerType(), True),
            StructField("day_of_week", IntegerType(), True),
            StructField("is_weekend", BooleanType(), True),
            StructField("user_age", IntegerType(), True),
            StructField("account_age_days", IntegerType(), True),
            StructField("avg_amount_last_30_days", DoubleType(), True),
            StructField("transaction_count_today", IntegerType(), True),
            StructField("time_since_last_transaction_hours", DoubleType(), True),
            StructField("is_fraud", IntegerType(), True)  # Target variable
        ])
        
        # Gerar dados sint√©ticos
        data = []
        merchants = ['Amazon', 'Walmart', 'Target', 'Costco', 'Gas Station', 'Restaurant', 'ATM']
        categories = ['Shopping', 'Groceries', 'Fuel', 'Dining', 'Cash', 'Entertainment']
        
        for i in range(num_transactions):
            # Probabilidade de fraude: 2%
            is_fraud = 1 if random.random() < 0.02 else 0
            
            # Transa√ß√µes fraudulentas t√™m padr√µes diferentes
            if is_fraud:
                amount = random.uniform(1000, 10000)  # Valores altos
                hour = random.choice([2, 3, 4, 23])  # Hor√°rios suspeitos
                user_age = random.randint(18, 80)
                account_age = random.randint(1, 30)  # Contas novas
                avg_amount = random.uniform(50, 200)
                tx_count_today = random.randint(5, 20)  # Muitas transa√ß√µes
                time_since_last = random.uniform(0.1, 2)  # Transa√ß√µes r√°pidas
            else:
                amount = math.exp(random.gauss(4, 1.5))  # Distribui√ß√£o lognormal simulada
                hour = random.randint(6, 22)  # Hor√°rios normais
                user_age = random.randint(18, 80)
                account_age = random.randint(30, 2000)  # Contas estabelecidas
                avg_amount = math.exp(random.gauss(4, 1))  # Lognormal simulada
                tx_count_today = random.randint(1, 5)
                time_since_last = random.uniform(2, 48)
            
            data.append((
                f"tx_{i:08d}",
                f"user_{random.randint(1, 10000):06d}",
                round(amount, 2),
                random.choice(merchants),
                random.choice(categories),
                hour,
                random.randint(0, 6),  # 0=Sunday
                hour in [0, 6],  # Weekend
                user_age,
                account_age,
                round(avg_amount, 2),
                tx_count_today,
                round(time_since_last, 2),
                is_fraud
            ))
        
        if self.spark:
            df = self.spark.createDataFrame(data, schema)
            print(f"‚úÖ Dataset criado: {df.count():,} linhas, {len(df.columns)} colunas")
            return df
        else:
            print("‚úÖ Schema definido para dados simulados")
            return data
    
    def _simulate_fraud_data(self, num_transactions: int) -> Dict[str, List]:
        """Simula dados de fraude quando PySpark n√£o est√° dispon√≠vel"""
        print("üé≠ Simulando dados de detec√ß√£o de fraude...")
        
        data = {
            'transaction_id': [f"tx_{i:08d}" for i in range(num_transactions)],
            'amount': [math.exp(random.gauss(4, 1.5)) for _ in range(num_transactions)],
            'is_fraud': [1 if random.random() < 0.02 else 0 for _ in range(num_transactions)],
            'hour': [random.randint(6, 22) for _ in range(num_transactions)],
            'merchant': [random.choice(['Amazon', 'Walmart', 'Target']) for _ in range(num_transactions)]
        }
        
        fraud_count = sum(data['is_fraud'])
        print(f"‚úÖ Dados simulados: {num_transactions:,} transa√ß√µes, {fraud_count} fraudes ({fraud_count/num_transactions*100:.2f}%)")
        
        return data
    
    def generate_recommendation_data(self, num_users: int = 10000, num_items: int = 1000, num_ratings: int = 100000) -> Any:
        """Gera dataset sint√©tico para sistema de recomenda√ß√£o"""
        print(f"üìä Gerando dados de recomenda√ß√£o: {num_users} usu√°rios, {num_items} itens, {num_ratings} avalia√ß√µes...")
        
        if not PYSPARK_AVAILABLE:
            return self._simulate_recommendation_data(num_users, num_items, num_ratings)
        
        # Gerar ratings com base em prefer√™ncias dos usu√°rios
        data = []
        
        for _ in range(num_ratings):
            user_id = random.randint(1, num_users)
            item_id = random.randint(1, num_items)
            
            # Simular prefer√™ncias: alguns usu√°rios gostam de categorias espec√≠ficas
            user_preference = user_id % 5  # 5 tipos de usu√°rios
            item_category = item_id % 5    # 5 categorias de itens
            
            if user_preference == item_category:
                # Usu√°rio gosta desta categoria
                base_rating = random.uniform(3.5, 5.0)
            else:
                # Rating neutro/baixo
                base_rating = random.uniform(1.0, 3.5)
            
            rating = max(1, min(5, round(base_rating)))
            data.append((user_id, item_id, float(rating)))
        
        # Schema
        schema = StructType([
            StructField("user_id", IntegerType(), True),
            StructField("item_id", IntegerType(), True),
            StructField("rating", DoubleType(), True),
        ])
        
        if self.spark:
            df = self.spark.createDataFrame(data, schema)
            print(f"‚úÖ Dataset de recomenda√ß√£o criado: {df.count():,} avalia√ß√µes")
            return df
        else:
            print("‚úÖ Dados de recomenda√ß√£o simulados")
            return data
    
    def _simulate_recommendation_data(self, num_users: int, num_items: int, num_ratings: int) -> Dict[str, List]:
        """Simula dados de recomenda√ß√£o"""
        print("üé≠ Simulando dados de recomenda√ß√£o...")
        
        data = {
            'user_id': [random.randint(1, num_users) for _ in range(num_ratings)],
            'item_id': [random.randint(1, num_items) for _ in range(num_ratings)],
            'rating': [random.uniform(1, 5) for _ in range(num_ratings)]
        }
        
        print(f"‚úÖ Dados simulados: {num_ratings:,} avalia√ß√µes")
        return data
    
    def fraud_detection_pipeline(self, data: Any) -> Dict[str, Any]:
        """Pipeline de detec√ß√£o de fraude"""
        print("\nüîç PIPELINE DE DETEC√á√ÉO DE FRAUDE")
        print("=" * 50)
        
        if not PYSPARK_AVAILABLE:
            return self._simulate_fraud_detection()
        
        try:
            # 1. An√°lise explorat√≥ria
            print("üìä An√°lise explorat√≥ria dos dados...")
            self._analyze_fraud_data(data)
            
            # 2. Prepara√ß√£o dos dados
            print("üîß Preparando features...")
            prepared_data = self._prepare_fraud_features(data)
            
            # 3. Divis√£o treino/teste
            train_data, test_data = prepared_data.randomSplit([0.8, 0.2], seed=42)
            
            print(f"üìä Dados de treino: {train_data.count():,} registros")
            print(f"üìä Dados de teste: {test_data.count():,} registros")
            
            # 4. Treinar modelos
            models_results = {}
            
            # Logistic Regression
            lr_result = self._train_fraud_logistic_regression(train_data, test_data)
            models_results['logistic_regression'] = lr_result
            
            # Random Forest
            rf_result = self._train_fraud_random_forest(train_data, test_data)
            models_results['random_forest'] = rf_result
            
            # GBT
            gbt_result = self._train_fraud_gbt(train_data, test_data)
            models_results['gradient_boosting'] = gbt_result
            
            # 5. Comparar modelos
            best_model = self._compare_fraud_models(models_results)
            
            return {
                'models': models_results,
                'best_model': best_model,
                'train_count': train_data.count(),
                'test_count': test_data.count()
            }
            
        except Exception as e:
            print(f"‚ùå Erro no pipeline de fraude: {e}")
            return self._simulate_fraud_detection()
    
    def _simulate_fraud_detection(self) -> Dict[str, Any]:
        """Simula resultados de detec√ß√£o de fraude"""
        print("üé≠ Simulando detec√ß√£o de fraude...")
        
        results = {
            'logistic_regression': {'auc': 0.92, 'precision': 0.85, 'recall': 0.78},
            'random_forest': {'auc': 0.95, 'precision': 0.89, 'recall': 0.83},
            'gradient_boosting': {'auc': 0.96, 'precision': 0.91, 'recall': 0.85}
        }
        
        print("üìà Resultados simulados:")
        for model, metrics in results.items():
            print(f"   {model}: AUC={metrics['auc']:.3f}, Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}")
        
        return {'models': results, 'best_model': 'gradient_boosting'}
    
    def _analyze_fraud_data(self, data: Any) -> None:
        """Analisa dados de fraude"""
        print("üîç Analisando distribui√ß√£o de fraudes...")
        
        # Contagem de fraudes
        fraud_stats = data.groupBy("is_fraud").count().collect()
        for row in fraud_stats:
            fraud_type = "Fraude" if row['is_fraud'] == 1 else "Normal"
            print(f"   {fraud_type}: {row['count']:,} transa√ß√µes")
        
        # Estat√≠sticas por valor
        print("üí∞ An√°lise por valor de transa√ß√£o...")
        data.groupBy("is_fraud").agg(
            mean("amount").alias("avg_amount"),
            stddev("amount").alias("std_amount")
        ).show()
    
    def _prepare_fraud_features(self, data: Any) -> Any:
        """Prepara features para detec√ß√£o de fraude"""
        
        # Selecionar features num√©ricas
        feature_cols = [
            "amount", "hour", "day_of_week", "user_age", "account_age_days",
            "avg_amount_last_30_days", "transaction_count_today", 
            "time_since_last_transaction_hours"
        ]
        
        # Assembler
        assembler = VectorAssembler(
            inputCols=feature_cols,
            outputCol="features"
        )
        
        # Scaler
        scaler = StandardScaler(
            inputCol="features",
            outputCol="scaled_features",
            withStd=True,
            withMean=True
        )
        
        # Pipeline de prepara√ß√£o
        prep_pipeline = Pipeline(stages=[assembler, scaler])
        prep_model = prep_pipeline.fit(data)
        
        return prep_model.transform(data)
    
    def _train_fraud_logistic_regression(self, train_data: Any, test_data: Any) -> Dict[str, float]:
        """Treina modelo de regress√£o log√≠stica"""
        print("ü§ñ Treinando Regress√£o Log√≠stica...")
        
        lr = LogisticRegression(
            featuresCol="scaled_features",
            labelCol="is_fraud",
            maxIter=100
        )
        
        lr_model = lr.fit(train_data)
        predictions = lr_model.transform(test_data)
        
        # Avalia√ß√£o
        evaluator = BinaryClassificationEvaluator(
            labelCol="is_fraud",
            rawPredictionCol="rawPrediction",
            metricName="areaUnderROC"
        )
        
        auc = evaluator.evaluate(predictions)
        print(f"   AUC: {auc:.4f}")
        
        return {'auc': auc, 'model': lr_model}
    
    def _train_fraud_random_forest(self, train_data: Any, test_data: Any) -> Dict[str, float]:
        """Treina modelo Random Forest"""
        print("üå≥ Treinando Random Forest...")
        
        rf = RandomForestClassifier(
            featuresCol="scaled_features",
            labelCol="is_fraud",
            numTrees=100
        )
        
        rf_model = rf.fit(train_data)
        predictions = rf_model.transform(test_data)
        
        evaluator = BinaryClassificationEvaluator(
            labelCol="is_fraud",
            rawPredictionCol="rawPrediction",
            metricName="areaUnderROC"
        )
        
        auc = evaluator.evaluate(predictions)
        print(f"   AUC: {auc:.4f}")
        
        return {'auc': auc, 'model': rf_model}
    
    def _train_fraud_gbt(self, train_data: Any, test_data: Any) -> Dict[str, float]:
        """Treina modelo Gradient Boosting"""
        print("‚ö° Treinando Gradient Boosting...")
        
        gbt = GBTClassifier(
            featuresCol="scaled_features",
            labelCol="is_fraud",
            maxIter=100
        )
        
        gbt_model = gbt.fit(train_data)
        predictions = gbt_model.transform(test_data)
        
        evaluator = BinaryClassificationEvaluator(
            labelCol="is_fraud",
            rawPredictionCol="rawPrediction",
            metricName="areaUnderROC"
        )
        
        auc = evaluator.evaluate(predictions)
        print(f"   AUC: {auc:.4f}")
        
        return {'auc': auc, 'model': gbt_model}
    
    def _compare_fraud_models(self, models_results: Dict) -> str:
        """Compara modelos de fraude"""
        print("\nüìä COMPARA√á√ÉO DOS MODELOS")
        print("-" * 30)
        
        best_auc = 0
        best_model = ""
        
        for model_name, result in models_results.items():
            auc = result['auc']
            print(f"{model_name:20}: AUC = {auc:.4f}")
            
            if auc > best_auc:
                best_auc = auc
                best_model = model_name
        
        print(f"\nüèÜ Melhor modelo: {best_model} (AUC = {best_auc:.4f})")
        return best_model
    
    def recommendation_pipeline(self, data: Any) -> Dict[str, Any]:
        """Pipeline de sistema de recomenda√ß√£o"""
        print("\nüéØ PIPELINE DE SISTEMA DE RECOMENDA√á√ÉO")
        print("=" * 50)
        
        if not PYSPARK_AVAILABLE:
            return self._simulate_recommendation_system()
        
        try:
            # 1. An√°lise dos dados
            print("üìä An√°lise dos dados de rating...")
            self._analyze_recommendation_data(data)
            
            # 2. Dividir dados
            train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)
            
            # 3. Treinar modelo ALS
            print("ü§ñ Treinando modelo de Collaborative Filtering (ALS)...")
            
            als = ALS(
                maxIter=10,
                regParam=0.1,
                userCol="user_id",
                itemCol="item_id",
                ratingCol="rating",
                coldStartStrategy="drop"
            )
            
            als_model = als.fit(train_data)
            
            # 4. Fazer predi√ß√µes
            predictions = als_model.transform(test_data)
            
            # 5. Avaliar modelo
            evaluator = RegressionEvaluator(
                metricName="rmse",
                labelCol="rating",
                predictionCol="prediction"
            )
            
            rmse = evaluator.evaluate(predictions)
            print(f"üìà RMSE: {rmse:.4f}")
            
            # 6. Gerar recomenda√ß√µes
            print("üéØ Gerando recomenda√ß√µes...")
            user_recs = als_model.recommendForAllUsers(10)
            item_recs = als_model.recommendForAllItems(10)
            
            print(f"‚úÖ Recomenda√ß√µes geradas para {user_recs.count()} usu√°rios")
            
            return {
                'model': als_model,
                'rmse': rmse,
                'user_recommendations': user_recs,
                'item_recommendations': item_recs
            }
            
        except Exception as e:
            print(f"‚ùå Erro no pipeline de recomenda√ß√£o: {e}")
            return self._simulate_recommendation_system()
    
    def _simulate_recommendation_system(self) -> Dict[str, Any]:
        """Simula sistema de recomenda√ß√£o"""
        print("üé≠ Simulando sistema de recomenda√ß√£o...")
        
        rmse = 0.87
        print(f"üìà RMSE simulado: {rmse:.4f}")
        print("‚úÖ Recomenda√ß√µes simuladas para 8000 usu√°rios")
        
        return {'rmse': rmse, 'users_with_recs': 8000}
    
    def _analyze_recommendation_data(self, data: Any) -> None:
        """Analisa dados de recomenda√ß√£o"""
        print("üìä Estat√≠sticas dos ratings:")
        
        data.describe("rating").show()
        
        print("üë• Usu√°rios mais ativos:")
        data.groupBy("user_id").count().orderBy(col("count").desc()).limit(5).show()
        
        print("üì± Itens mais avaliados:")
        data.groupBy("item_id").count().orderBy(col("count").desc()).limit(5).show()
    
    def clustering_pipeline(self, data: Any, k: int = 5) -> Dict[str, Any]:
        """Pipeline de clustering de usu√°rios"""
        print(f"\nüéØ PIPELINE DE CLUSTERING (K={k})")
        print("=" * 50)
        
        if not PYSPARK_AVAILABLE:
            return self._simulate_clustering(k)
        
        try:
            # Preparar features para clustering
            # Vamos usar estat√≠sticas dos usu√°rios como features
            print("üîß Preparando features de usu√°rio...")
            
            user_features = data.groupBy("user_id").agg(
                count("rating").alias("num_ratings"),
                mean("rating").alias("avg_rating"),
                stddev("rating").alias("std_rating")
            ).fillna(0)
            
            # Preparar features para clustering
            feature_cols = ["num_ratings", "avg_rating", "std_rating"]
            assembler = VectorAssembler(
                inputCols=feature_cols,
                outputCol="features"
            )
            
            user_features_assembled = assembler.transform(user_features)
            
            # Aplicar K-Means
            print(f"ü§ñ Aplicando K-Means com {k} clusters...")
            kmeans = KMeans(k=k, seed=42)
            kmeans_model = kmeans.fit(user_features_assembled)
            
            # Fazer predi√ß√µes
            clustered_users = kmeans_model.transform(user_features_assembled)
            
            # Analisar clusters
            print("üìä An√°lise dos clusters:")
            cluster_analysis = clustered_users.groupBy("prediction").agg(
                count("user_id").alias("num_users"),
                mean("num_ratings").alias("avg_num_ratings"),
                mean("avg_rating").alias("avg_avg_rating")
            ).orderBy("prediction")
            
            cluster_analysis.show()
            
            # Calcular m√©tricas
            evaluator = ClusteringEvaluator()
            silhouette = evaluator.evaluate(clustered_users)
            print(f"üìà Silhouette Score: {silhouette:.4f}")
            
            return {
                'model': kmeans_model,
                'clustered_data': clustered_users,
                'silhouette_score': silhouette,
                'num_clusters': k
            }
            
        except Exception as e:
            print(f"‚ùå Erro no clustering: {e}")
            return self._simulate_clustering(k)
    
    def _simulate_clustering(self, k: int) -> Dict[str, Any]:
        """Simula clustering"""
        print("üé≠ Simulando clustering de usu√°rios...")
        
        silhouette = 0.65
        print(f"üìà Silhouette Score simulado: {silhouette:.4f}")
        print(f"‚úÖ {k} clusters criados")
        
        # Simular distribui√ß√£o dos clusters
        for i in range(k):
            users_in_cluster = random.randint(1500, 2500)
            avg_ratings = random.uniform(2.5, 4.5)
            print(f"   Cluster {i}: {users_in_cluster} usu√°rios, rating m√©dio: {avg_ratings:.2f}")
        
        return {'silhouette_score': silhouette, 'num_clusters': k}
    
    def run_complete_demo(self):
        """Executa demonstra√ß√£o completa da plataforma"""
        print("üöÄ DEMONSTRA√á√ÉO COMPLETA: Big Data ML Platform")
        print("=" * 60)
        
        try:
            # 1. Detec√ß√£o de Fraude
            print("\n1Ô∏è‚É£ DETEC√á√ÉO DE FRAUDE")
            fraud_data = self.generate_fraud_detection_data(50000)
            fraud_results = self.fraud_detection_pipeline(fraud_data)
            self.results['fraud_detection'] = fraud_results
            
            # 2. Sistema de Recomenda√ß√£o
            print("\n2Ô∏è‚É£ SISTEMA DE RECOMENDA√á√ÉO")
            rec_data = self.generate_recommendation_data(5000, 500, 50000)
            rec_results = self.recommendation_pipeline(rec_data)
            self.results['recommendation'] = rec_results
            
            # 3. Clustering de Usu√°rios
            print("\n3Ô∏è‚É£ CLUSTERING DE USU√ÅRIOS")
            clustering_results = self.clustering_pipeline(rec_data, k=5)
            self.results['clustering'] = clustering_results
            
            # 4. Relat√≥rio Final
            self._generate_final_report()
            
        except Exception as e:
            print(f"‚ùå Erro na demonstra√ß√£o: {e}")
        
        finally:
            self.stop()
    
    def _generate_final_report(self):
        """Gera relat√≥rio final"""
        print("\nüìã RELAT√ìRIO FINAL")
        print("=" * 50)
        
        if 'fraud_detection' in self.results:
            fraud_res = self.results['fraud_detection']
            if 'best_model' in fraud_res:
                print(f"üîç Detec√ß√£o de Fraude: Melhor modelo = {fraud_res['best_model']}")
        
        if 'recommendation' in self.results:
            rec_res = self.results['recommendation']
            if 'rmse' in rec_res:
                print(f"üéØ Recomenda√ß√£o: RMSE = {rec_res['rmse']:.4f}")
        
        if 'clustering' in self.results:
            cluster_res = self.results['clustering']
            if 'silhouette_score' in cluster_res:
                print(f"üéØ Clustering: Silhouette = {cluster_res['silhouette_score']:.4f}")
        
        print(f"\n‚úÖ Demonstra√ß√£o conclu√≠da com sucesso!")
        
        # Salvar resultados
        self._save_results()
    
    def _save_results(self):
        """Salva resultados em arquivo JSON"""
        try:
            results_summary = {
                'timestamp': datetime.now().isoformat(),
                'platform': self.app_name,
                'results': {}
            }
            
            for task, result in self.results.items():
                # Extrair apenas m√©tricas (n√£o objetos Spark)
                if isinstance(result, dict):
                    summary = {}
                    for key, value in result.items():
                        if isinstance(value, (int, float, str, bool)):
                            summary[key] = value
                    results_summary['results'][task] = summary
            
            filename = f"ml_bigdata_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results_summary, f, indent=2, ensure_ascii=False)
            
            print(f"üíæ Resultados salvos em: {filename}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao salvar resultados: {e}")
    
    def stop(self):
        """Para a sess√£o Spark"""
        if self.spark:
            self.spark.stop()
            print("üîí Sess√£o Spark finalizada")

def demonstrate_ml_concepts():
    """Demonstra conceitos fundamentais de ML com Big Data"""
    print("\nüìö CONCEITOS FUNDAMENTAIS: ML + BIG DATA")
    print("=" * 60)
    
    concepts = {
        "ü§ñ Machine Learning Distribu√≠do": [
            "Paraleliza√ß√£o: Divis√£o de dados e processamento em clusters",
            "Algoritmos escal√°veis: Gradient Descent distribu√≠do",
            "Feature Engineering: Transforma√ß√µes em larga escala",
            "Model Training: Treinamento paralelo e iterativo",
            "Hyperparameter Tuning: Busca distribu√≠da de par√¢metros"
        ],
        
        "üìä Tipos de Problemas": [
            "Classifica√ß√£o: Detec√ß√£o de fraude, spam, categoriza√ß√£o",
            "Regress√£o: Predi√ß√£o de pre√ßos, demanda, valores",
            "Clustering: Segmenta√ß√£o de clientes, anomalias",
            "Recomenda√ß√£o: Sistemas de recomenda√ß√£o colaborativos",
            "Processamento de Linguagem: An√°lise de sentimentos"
        ],
        
        "‚ö° Desafios de Escala": [
            "Volume: Terabytes/Petabytes de dados de treinamento",
            "Velocidade: Treinamento e infer√™ncia em tempo real",
            "Variedade: Dados estruturados, texto, imagens, streams",
            "Mem√≥ria: Modelos que n√£o cabem em uma m√°quina",
            "Distribui√ß√£o: Coordena√ß√£o entre m√∫ltiplos n√≥s"
        ],
        
        "üõ†Ô∏è Ferramentas e Frameworks": [
            "Apache Spark MLlib: ML distribu√≠do integrado",
            "TensorFlow: Deep learning distribu√≠do",
            "Horovod: Treinamento distribu√≠do para DL",
            "Ray: ML distribu√≠do moderno",
            "Dask: Computa√ß√£o paralela em Python"
        ]
    }
    
    for category, items in concepts.items():
        print(f"\n{category}")
        print("-" * 50)
        for item in items:
            print(f"  ‚Ä¢ {item}")

def main():
    """Fun√ß√£o principal"""
    print("üéØ AULA 09: Machine Learning com Big Data")
    print("Autor: Professor Vagner Cordeiro")
    print("=" * 60)
    
    # Demonstrar conceitos
    demonstrate_ml_concepts()
    
    # Executar demonstra√ß√£o pr√°tica
    platform = BigDataMLPlatform("Aula09_ML_BigData")
    
    try:
        platform.run_complete_demo()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Demonstra√ß√£o interrompida pelo usu√°rio")
    except Exception as e:
        print(f"‚ùå Erro na demonstra√ß√£o: {e}")
    finally:
        platform.stop()
    
    print("\n‚úÖ Aula conclu√≠da! Conceitos de ML com Big Data demonstrados.")

if __name__ == "__main__":
    main()
