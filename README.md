# ðŸš€ TÃ³picos de Big Data em Python# ðŸš€ TÃ³picos de Big Data em Python



**Plataforma completa para aprendizado de Big Data com Apache Hadoop, anÃ¡lise de dados cientÃ­ficos e laboratÃ³rios prÃ¡ticos.****Plataforma completa para aprendizado de Big Data com Apache Hadoop, anÃ¡lise de dados cientÃ­ficos e laboratÃ³rios prÃ¡ticos.**



[![Status](https://img.shields.io/badge/Status-Online-brightgreen)](https://datascience-pro.netlify.app)[![Status](https://img.shields.io/badge/Status-Online-brightgreen)](https://datascience-pro.netlify.app)

[![Hadoop](https://img.shields.io/badge/Hadoop-3.3.4-orange)](https://hadoop.apache.org/)[![Hadoop](https://img.shields.io/badge/Hadoop-3.3.4-orange)](https://hadoop.apache.org/)

[![Python](https://img.shields.io/badge/Python-3.8+-blue)](https://python.org/)[![Python](https://img.shields.io/badge/Python-3.8+-blue)](https://python.org/)



## ðŸŒŸ Recursos Principais## ðŸŒŸ Recursos Principais



### ðŸ˜ Apache Hadoop - LaboratÃ³rio Completo### ï¿½ Apache Hadoop - LaboratÃ³rio Completo

- **ðŸ‡§ðŸ‡· 100% em PortuguÃªs** - ExplicaÃ§Ãµes didÃ¡ticas e claras- **ðŸ‡§ðŸ‡· 100% em PortuguÃªs** - ExplicaÃ§Ãµes didÃ¡ticas e claras

- **ðŸ—ºï¸ MapReduce PrÃ¡tico** - Exemplos funcionais com Python- **ðŸ—ºï¸ MapReduce PrÃ¡tico** - Exemplos funcionais com Python

- **ðŸ’¾ Tutorial HDFS** - Sistema de arquivos distribuÃ­do- **ðŸ’¾ Tutorial HDFS** - Sistema de arquivos distribuÃ­do

- **ðŸ’» Guia de InstalaÃ§Ã£o** - Passo a passo no Ubuntu- **ðŸ’» Guia de InstalaÃ§Ã£o** - Passo a passo no Ubuntu

- **ðŸ‹ï¸ ExercÃ­cios Hands-on** - Datasets reais para praticar- **ðŸ‹ï¸ ExercÃ­cios Hands-on** - Datasets reais para praticar



### ðŸ“Š AnÃ¡lise de Dados CientÃ­ficos  ### ï¿½ AnÃ¡lise de Dados CientÃ­ficos  

- **Upload Inteligente** - CSV, JSON, Excel- **Upload Inteligente** - CSV, JSON, Excel

- **AnÃ¡lise AutomÃ¡tica** - EstatÃ­sticas e correlaÃ§Ãµes- **AnÃ¡lise AutomÃ¡tica** - EstatÃ­sticas e correlaÃ§Ãµes

- **VisualizaÃ§Ãµes AvanÃ§adas** - GrÃ¡ficos interativos- **VisualizaÃ§Ãµes AvanÃ§adas** - GrÃ¡ficos interativos

- **RelatÃ³rios Completos** - ExportaÃ§Ã£o de resultados- **RelatÃ³rios Completos** - ExportaÃ§Ã£o de resultados



## ðŸŒ Acesso Online## ðŸŒ Acesso Online



**ðŸš€ Site Principal:** https://datascience-pro.netlify.app**ðŸš€ Site Principal:** https://datascience-pro.netlify.app



**ðŸ˜ LaboratÃ³rio Hadoop:** https://datascience-pro.netlify.app/aulas/aula06-hadoop-intro/laboratorio-hadoop-pratico.html**ðŸ˜ LaboratÃ³rio Hadoop:** https://datascience-pro.netlify.app/aulas/aula06-hadoop-intro/laboratorio-hadoop-pratico.html



## ðŸš€ Status Atual - TUDO FUNCIONANDO! âœ…## ï¿½ ConteÃºdo PedagÃ³gico



- âœ… **Site Online e Responsivo**```

- âœ… **LaboratÃ³rio Hadoop 100% PortuguÃªs** site_analise_dados/

- âœ… **MapReduce com Python Funcionando**â”œâ”€â”€ index.html              # PÃ¡gina principal (SPA)

- âœ… **HDFS Tutorial DidÃ¡tico Completo**â”œâ”€â”€ css/

- âœ… **Guia de InstalaÃ§Ã£o Ubuntu**â”‚   â””â”€â”€ style.css           # Estilos completos e responsivos

- âœ… **ExercÃ­cios com Datasets Reais**â”œâ”€â”€ js/

- âœ… **Design Profissional (Ã­cones pequenos, fundo branco)**â”‚   â”œâ”€â”€ main.js            # Funcionalidades principais e utilitÃ¡rios

- âœ… **Deploy Netlify AutomÃ¡tico**â”‚   â”œâ”€â”€ analysis-tool.js   # Ferramenta de anÃ¡lise de dados

â”‚   â””â”€â”€ app.js             # InicializaÃ§Ã£o e referÃªncias

**Ãšltima atualizaÃ§Ã£o: 25/09/2025 - Tudo funcional!** ðŸŽ‰â”œâ”€â”€ static/                # Arquivos estÃ¡ticos (imagens, etc.)

â”œâ”€â”€ templates/             # Templates auxiliares (se necessÃ¡rio)

---â””â”€â”€ uploads/               # DiretÃ³rio para uploads (local)

```

**ðŸ˜ O laboratÃ³rio Hadoop mais completo em portuguÃªs estÃ¡ online!**
## ðŸŒ Deploy no Netlify

### PrÃ©-requisitos
1. Conta no [Netlify](https://www.netlify.com/)
2. RepositÃ³rio Git (GitHub, GitLab, ou Bitbucket)

### OpÃ§Ã£o 1: Deploy via Git (Recomendado)

1. **Prepare o repositÃ³rio:**
```bash
git init
git add .
git commit -m "Initial commit - DataScience Pro site"
git branch -M main
git remote add origin https://github.com/seu-usuario/site-analise-dados.git
git push -u origin main
```

2. **Configure o Netlify:**
   - Acesse o [Netlify Dashboard](https://app.netlify.com/)
   - Clique em "New site from Git"
   - Conecte seu repositÃ³rio
   - Configure as opÃ§Ãµes de build:
     - **Branch to deploy**: `main`
     - **Build command**: (deixe vazio)
     - **Publish directory**: `site_analise_dados`

3. **Deploy automÃ¡tico:**
   - O site serÃ¡ deployado automaticamente
   - AtualizaÃ§Ãµes no repositÃ³rio triggeram novos deploys

### OpÃ§Ã£o 2: Deploy Manual

1. **Prepare os arquivos:**
```bash
# Comprima apenas o conteÃºdo da pasta site_analise_dados
cd "site_analise_dados"
# Selecione todos os arquivos (index.html, css/, js/, etc.)
```

2. **Upload no Netlify:**
   - Acesse [Netlify Drop](https://app.netlify.com/drop)
   - Arraste e solte a pasta ou arquivo ZIP
   - O site serÃ¡ deployado instantaneamente

### ConfiguraÃ§Ãµes AvanÃ§adas

Crie um arquivo `netlify.toml` na raiz do projeto:

```toml
[build]
  publish = "site_analise_dados"

[[headers]]
  for = "/*"
  [headers.values]
    X-Frame-Options = "DENY"
    X-XSS-Protection = "1; mode=block"
    X-Content-Type-Options = "nosniff"
    Referrer-Policy = "strict-origin-when-cross-origin"

[[headers]]
  for = "*.js"
  [headers.values]
    Cache-Control = "public, max-age=31536000"

[[headers]]
  for = "*.css"
  [headers.values]
    Cache-Control = "public, max-age=31536000"

[build.environment]
  NODE_VERSION = "18"

# Redirects para SPA
[[redirects]]
  from = "/*"
  to = "/index.html"
  status = 200
```

## ðŸ’¡ Funcionalidades Detalhadas

### ðŸ“Š AnÃ¡lise de Dados
- **Upload Seguro**: ValidaÃ§Ã£o de tipos e tamanhos de arquivo
- **Processamento Inteligente**: DetecÃ§Ã£o automÃ¡tica de tipos de dados
- **AnÃ¡lise EstatÃ­stica**: MÃ©tricas completas para variÃ¡veis numÃ©ricas
- **CorrelaÃ§Ãµes**: Matriz de correlaÃ§Ã£o visual
- **Qualidade**: AvaliaÃ§Ã£o de completude e duplicatas
- **Outliers**: DetecÃ§Ã£o usando mÃ©todo IQR

### ðŸ“ˆ VisualizaÃ§Ãµes
- **DistribuiÃ§Ãµes**: Histogramas para dados numÃ©ricos
- **Categorias**: GrÃ¡ficos de barras para dados categÃ³ricos
- **CorrelaÃ§Ãµes**: Heatmaps interativos
- **Responsive**: AdaptaÃ§Ã£o automÃ¡tica a diferentes telas

### ðŸ“š ConteÃºdo Educacional
- **Metodologia CientÃ­fica**: Do planejamento Ã  publicaÃ§Ã£o
- **Ferramentas**: Excel, Python, R, SPSS
- **TÃ©cnicas AvanÃ§adas**: Machine Learning, Deep Learning
- **Boas PrÃ¡ticas**: ValidaÃ§Ã£o, reprodutibilidade, Ã©tica

## ðŸŽ¯ Casos de Uso

### Para Estudantes
- Aprender metodologia de anÃ¡lise de dados
- Praticar com dados reais
- Acessar referÃªncias acadÃªmicas
- Desenvolver projetos de pesquisa

### Para Pesquisadores
- AnÃ¡lise exploratÃ³ria rÃ¡pida
- ValidaÃ§Ã£o de qualidade dos dados
- GeraÃ§Ã£o de relatÃ³rios preliminares
- ReferÃªncias para metodologia

### Para Profissionais
- Prototipagem de anÃ¡lises
- ApresentaÃ§Ãµes para stakeholders
- EducaÃ§Ã£o de equipes
- Benchmarking de dados

## ðŸ”§ Desenvolvimento Local

1. **Clone o projeto:**
```bash
git clone https://github.com/seu-usuario/site-analise-dados.git
cd site-analise-dados
```

2. **Execute localmente:**
```bash
# OpÃ§Ã£o 1: Servidor Python
cd site_analise_dados
python -m http.server 8000

# OpÃ§Ã£o 2: Live Server (VS Code)
# Instale a extensÃ£o Live Server e abra index.html

# OpÃ§Ã£o 3: Node.js serve
npx serve site_analise_dados
```

3. **Acesse:**
```
http://localhost:8000
```

## ðŸ“± Responsividade

O site Ã© totalmente responsivo e funciona perfeitamente em:
- **Desktop**: Layout completo com trÃªs colunas
- **Tablet**: Layout adaptado com duas colunas
- **Mobile**: Layout em coluna Ãºnica com navegaÃ§Ã£o otimizada

## â™¿ Acessibilidade

- **WCAG 2.1 AA**: Conformidade com padrÃµes de acessibilidade
- **NavegaÃ§Ã£o por Teclado**: Suporte completo
- **Screen Readers**: MarcaÃ§Ã£o semÃ¢ntica adequada
- **Alto Contraste**: Esquema de cores acessÃ­vel
- **Reduced Motion**: Respeita preferÃªncias de animaÃ§Ã£o

## ðŸ”’ SeguranÃ§a

- **CSP Headers**: PolÃ­tica de seguranÃ§a de conteÃºdo
- **HTTPS Only**: ForÃ§ar conexÃµes seguras
- **Input Validation**: ValidaÃ§Ã£o de uploads
- **XSS Protection**: ProteÃ§Ã£o contra scripts maliciosos

## ðŸ“ˆ Performance

- **Otimizado**: CSS e JS minificados
- **Lazy Loading**: Carregamento sob demanda
- **Caching**: Headers de cache apropriados
- **CDN Ready**: CompatÃ­vel com redes de distribuiÃ§Ã£o

## ðŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ðŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ðŸ‘¨â€ðŸ’» Autor

Desenvolvido com â¤ï¸ para a comunidade de ciÃªncia de dados.

## ðŸ”— Links Ãšteis

- **Demo**: [https://seu-site.netlify.app](https://seu-site.netlify.app)
- **DocumentaÃ§Ã£o**: [Wiki do Projeto](https://github.com/seu-usuario/site-analise-dados/wiki)
- **Issues**: [Reportar Problemas](https://github.com/seu-usuario/site-analise-dados/issues)
- **Discussions**: [FÃ³rum da Comunidade](https://github.com/seu-usuario/site-analise-dados/discussions)

---

**DataScience Pro** - Transformando dados em conhecimento cientÃ­fico! ðŸš€ðŸ“Š
=======
# Big Data em Python: Casos PrÃ¡ticos de Santa Catarina

*Um guia educacional de Big Data atravÃ©s de storytelling com casos reais de FlorianÃ³polis e regiÃ£o*

---

## ðŸ“– Sobre o Projeto

Este repositÃ³rio apresenta conceitos de **Big Data** e **Python** atravÃ©s de **narrativas educacionais** baseadas em casos reais de Santa Catarina, seguindo o protagonista **Patrick** em suas aventuras com dados.

### ðŸŽ¯ **Diferenciais**
- **Storytelling educativo**: Aprendizado atravÃ©s de narrativas envolventes
- **Casos reais de SC**: Ponte HercÃ­lio Luz, DETRAN-SC, turismo de Floripa
- **Menos cÃ³digo, mais explicaÃ§Ã£o**: Foco na compreensÃ£o conceitual
- **Contexto local**: Exemplos prÃ¡ticos da Grande FlorianÃ³polis
- **Protagonista Ãºnico**: Patrick como guia consistente em todos os capÃ­tulos

### ðŸ“š **Estrutura do Livro (5 CapÃ­tulos Narrativos)**

#### **Parte I: Fundamentos atravÃ©s de HistÃ³rias**
1. **O Despertar dos Dados** - Patrick descobre Big Data em FlorianÃ³polis âœ…
2. **IoT e Cidades Inteligentes** - Patrick explora sensores em SÃ£o JosÃ© âœ…
3. **AnÃ¡lise de Dados TurÃ­sticos** - Patrick desvenda padrÃµes do turismo âœ…

#### **Parte II: Tecnologias AvanÃ§adas**  
4. **Apache Spark em AÃ§Ã£o** - Patrick processa dados do DETRAN-SC âœ…
5. **Machine Learning Aplicado** - Patrick prevÃª preÃ§os imobiliÃ¡rios âœ…

### ðŸŽ­ **Metodologia Narrativa**
- **Patrick**: Protagonista Ãºnico e consistente em todos os capÃ­tulos
- **Contexto SC**: Todos os casos baseados em Santa Catarina
- **EducaÃ§Ã£o**: Foco em explicaÃ§Ãµes didÃ¡ticas, nÃ£o em cÃ³digo complexo
- **Aplicabilidade**: Conceitos aplicÃ¡veis a qualquer regiÃ£o do Brasil

---

## ðŸŽ“ **PÃºblico-Alvo**

- **Estudantes de tecnologia** e ciÃªncia de dados
- **Analistas de dados** iniciantes/intermediÃ¡rios
- **Gestores pÃºblicos** interessados em transformaÃ§Ã£o digital
- **Profissionais** que buscam aprender Big Data de forma didÃ¡tica
- **Qualquer pessoa** interessada em dados e storytelling educativo

---

## ðŸ› ï¸ **Tecnologias e Conceitos Abordados**

### **Linguagens e Frameworks**
- **Python**: Pandas, NumPy, Matplotlib, Seaborn
- **Big Data**: Apache Spark, PySpark, processamento distribuÃ­do
- **Machine Learning**: Scikit-learn, regressÃ£o, classificaÃ§Ã£o
- **Dados**: APIs, CSV, JSON, anÃ¡lise exploratÃ³ria
- **VisualizaÃ§Ã£o**: GrÃ¡ficos interpretativos e storytelling com dados

### **Conceitos Fundamentais**
- Volume, Velocidade, Variedade, Veracidade, Valor (5 V's)
- ETL/ELT e pipelines de dados
- IoT e sensores urbanos
- Smart cities e transformaÃ§Ã£o digital
- Feature engineering e modelagem preditiva

---

## ï¿½ **Palavras-Chave para Profissionais de Big Data e AnÃ¡lise de Dados**

*Skills essenciais que aparecem em vagas de emprego e sÃ£o abordadas neste livro*

### **Linguagens e Frameworks**
Python, PySpark, Apache Spark, SQL, Scala, R, Java, Hadoop, Hive, Apache Kafka, Apache Airflow, Apache Beam, Apache Flink, Databricks, Snowflake, dbt, Great Expectations, MLflow, Kubeflow, TensorFlow, PyTorch, Keras, XGBoost, LightGBM, Scikit-learn, Pandas, NumPy, Matplotlib, Seaborn, Plotly, Streamlit, Dash, FastAPI, Flask, Django

### **Cloud e Infraestrutura**
AWS, Azure, Google Cloud Platform (GCP), Amazon S3, Azure Data Lake, Google BigQuery, Amazon Redshift, Azure Synapse Analytics, Google Cloud Storage, EC2, Azure VM, Google Compute Engine, Lambda, Azure Functions, Google Cloud Functions, Docker, Kubernetes, Terraform, Apache Mesos, Yarn, Spark Cluster, Elasticsearch, MongoDB, Cassandra, Redis, PostgreSQL, MySQL, Oracle, SQL Server

### **Ferramentas de Dados e ETL**
Apache Nifi, Talend, Informatica, Pentaho, SSIS, Azure Data Factory, AWS Glue, Google Cloud Dataflow, Apache Sqoop, Apache Flume, Logstash, Fivetran, Stitch, Airbyte, Singer, Apache Superset, Tableau, Power BI, Looker, Grafana, Metabase, Apache Zeppelin, Jupyter Notebooks, Google Colab, Apache Parquet, Apache Avro, JSON, XML, CSV

### **Machine Learning e IA**
Deep Learning, Neural Networks, CNN, RNN, LSTM, GAN, Transformer, BERT, GPT, Computer Vision, Natural Language Processing (NLP), Time Series Analysis, Recommender Systems, Classification, Regression, Clustering, Dimensionality Reduction, Feature Engineering, Model Selection, Cross-validation, Hyperparameter Tuning, A/B Testing, MLOps, Model Deployment, Model Monitoring, AutoML, Ensemble Methods

### **Metodologias e Conceitos**
Data Engineering, Data Science, Data Analytics, Business Intelligence, ETL/ELT, Data Pipeline, Data Warehouse, Data Lake, Lakehouse, Data Mesh, Real-time Processing, Batch Processing, Stream Processing, Data Governance, Data Quality, Data Lineage, Data Catalog, Metadata Management, GDPR, Data Privacy, Agile, Scrum, DevOps, CI/CD, Git, Version Control

### **EspecializaÃ§Ã£o e Soft Skills**
Statistics, Mathematics, Linear Algebra, Probability, Hypothesis Testing, Statistical Modeling, Experimentation Design, Problem Solving, Critical Thinking, Communication, Data Storytelling, Data Visualization, Business Acumen, Domain Knowledge, Project Management, Teamwork, Leadership, Continuous Learning, Adaptability, Innovation

---

## ï¿½ðŸ“Š **Casos Reais Desenvolvidos**

### ðŸŒ‰ **CapÃ­tulo 1: Despertar dos Dados - Ponte HercÃ­lio Luz**
- Patrick descobre padrÃµes no trÃ¡fego da ponte
- AnÃ¡lise de 2,8 milhÃµes de veÃ­culos/ano
- IntroduÃ§Ã£o aos conceitos de Big Data
- Insights sobre mobilidade urbana em FlorianÃ³polis

### ðŸ™ï¸ **CapÃ­tulo 2: SÃ£o JosÃ© Conectado**  
- Patrick explora sistemas IoT de monitoramento urbano
- Sensores de qualidade do ar e trÃ¡fego
- Conceitos de smart cities aplicados
- TransformaÃ§Ã£o digital em cidades mÃ©dias

### ðŸ–ï¸ **CapÃ­tulo 3: Turismo de FlorianÃ³polis**
- Patrick analisa sazonalidade da ocupaÃ§Ã£o hoteleira
- PadrÃµes de demanda turÃ­stica na Ilha da Magia
- Dados reais de turismo de SC
- PrevisÃ£o e otimizaÃ§Ã£o para o setor

### ðŸš— **CapÃ­tulo 4: DETRAN Santa Catarina**
- Patrick processa 4,2 milhÃµes de veÃ­culos registrados
- IntroduÃ§Ã£o ao Apache Spark e processamento distribuÃ­do
- AnÃ¡lise de frota por municÃ­pio catarinense
- Escalabilidade para grandes volumes de dados

### ðŸ  **CapÃ­tulo 5: Mercado ImobiliÃ¡rio de Floripa**
- Patrick desenvolve Machine Learning para previsÃ£o de preÃ§os
- Fatores de valorizaÃ§Ã£o na Ilha da Magia
- Feature engineering com dados locais
- AplicaÃ§Ã£o prÃ¡tica de algoritmos de ML
- 80+ skills essenciais para o mercado de trabalho

---

## ðŸš€ **Guia PrÃ¡tico: InstalaÃ§Ã£o e Uso**

### **ðŸ“– Guias Completos DisponÃ­veis**

#### **ðŸ› ï¸ [INSTALACAO_COMPLETA.md](./INSTALACAO_COMPLETA.md)**
Tutorial detalhado passo-a-passo para configurar todo ambiente:
- InstalaÃ§Ã£o Python, Java, bibliotecas
- ConfiguraÃ§Ã£o ambiente virtual
- Teste automatizado de validaÃ§Ã£o
- SoluÃ§Ã£o de problemas comuns
- Scripts de verificaÃ§Ã£o completa

#### **ðŸ¤– [GUIA_INTELIGENCIA_ARTIFICIAL.md](./GUIA_INTELIGENCIA_ARTIFICIAL.md)**
Guia completo de IA aplicada a Big Data com casos prÃ¡ticos:
- Machine Learning com dados de SC
- Deep Learning para sÃ©ries temporais
- Processamento de Linguagem Natural (NLP)
- Computer Vision e Algoritmos GenÃ©ticos
- 5 casos prÃ¡ticos prontos para executar

#### **ðŸŽ¯ [CASOS_PRATICOS.md](./CASOS_PRATICOS.md)**
Projetos completos prontos para usar e adaptar:
- Dashboard interativo com Streamlit
- Sistema de prediÃ§Ã£o de demanda turÃ­stica
- API REST para dados urbanos em tempo real
- Exemplos prÃ¡ticos com cÃ³digo completo
- Algoritmos GenÃ©ticos para otimizaÃ§Ã£o
- Projetos avanÃ§ados e prÃ³ximos passos

### **ðŸ“‹ PrÃ©-requisitos**

**Sistema Operacional:**
- Windows 10/11, macOS 10.15+, ou Linux Ubuntu 18.04+

**Software Essencial:**
- **Python 3.8+** - [Download aqui](https://python.org/downloads)
- **Java 8 ou 11** (para PySpark) - [Download OpenJDK](https://adoptium.net/)
- **Git** - [Download aqui](https://git-scm.com/downloads)

**Verificar InstalaÃ§Ãµes:**
```bash
# Verificar Python
python --version
# Deve mostrar: Python 3.8.x ou superior

# Verificar Java  
java -version
# Deve mostrar: openjdk version "8" ou "11"

# Verificar Git
git --version
```

---

### **âš¡ InstalaÃ§Ã£o RÃ¡pida (5 minutos)**

#### **Passo 1: Clone o RepositÃ³rio**
```bash
# Abra o terminal/prompt de comando
git clone https://github.com/cordeirotelecom/topicos-bigdata-python.git
cd topicos-bigdata-python
```

#### **Passo 2: Crie um Ambiente Virtual (Recomendado)**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux  
python3 -m venv venv
source venv/bin/activate
```

#### **Passo 3: Instale as DependÃªncias**
```bash
# Instalar todas as bibliotecas necessÃ¡rias
pip install -r requirements.txt

# Verificar se PySpark foi instalado corretamente
python -c "import pyspark; print('PySpark OK!')"
```

#### **Passo 4: Teste a InstalaÃ§Ã£o**
```bash
# Iniciar Jupyter Lab
jupyter lab

# Ou Jupyter Notebook clÃ¡ssico
jupyter notebook
```

---

### **ðŸ“– Como Usar: Guia Passo a Passo**

#### **Para Iniciantes Completos**

**1. Comece pelo CapÃ­tulo 1**
```bash
# Navegue atÃ© a pasta do livro
cd livro/

# Abra o primeiro capÃ­tulo
# Windows: notepad capitulo01-despertar-dos-dados.md
# macOS: open capitulo01-despertar-dos-dados.md  
# Linux: gedit capitulo01-despertar-dos-dados.md
```

**2. Siga a Ordem dos CapÃ­tulos**
- ðŸ“– **CapÃ­tulo 1**: Conceitos bÃ¡sicos de Big Data
- ðŸ™ï¸ **CapÃ­tulo 2**: IoT e sensores urbanos
- ðŸ–ï¸ **CapÃ­tulo 3**: AnÃ¡lise de dados turÃ­sticos
- âš¡ **CapÃ­tulo 4**: Processamento distribuÃ­do com Spark
- ðŸ¤– **CapÃ­tulo 5**: Machine Learning aplicado

**3. Experimente os Conceitos**
```python
# Exemplo prÃ¡tico do CapÃ­tulo 1
import pandas as pd
import matplotlib.pyplot as plt

# Simular dados de trÃ¡fego da Ponte HercÃ­lio Luz
dados_ponte = {
    'hora': range(0, 24),
    'veiculos': [50, 30, 20, 25, 45, 120, 350, 500, 
                 400, 300, 250, 280, 320, 300, 350, 
                 400, 500, 600, 450, 300, 200, 150, 100, 70]
}

df = pd.DataFrame(dados_ponte)
plt.plot(df['hora'], df['veiculos'])
plt.title('TrÃ¡fego na Ponte HercÃ­lio Luz - 24h')
plt.xlabel('Hora do Dia')
plt.ylabel('NÃºmero de VeÃ­culos')
plt.show()
```

### **ðŸ’¡ Exemplos PrÃ¡ticos RÃ¡pidos**

#### **ðŸ™ï¸ AnÃ¡lise de IoT - Sensores em SÃ£o JosÃ©**
```python
import pandas as pd
import numpy as np

# Simular dados de sensores de qualidade do ar
np.random.seed(42)
horas = pd.date_range('2025-01-01', periods=168, freq='H')  # 1 semana

dados_iot = pd.DataFrame({
    'timestamp': horas,
    'pm25': np.random.normal(25, 8, 168),  # PM2.5 (Î¼g/mÂ³)
    'temperatura': 20 + 10 * np.sin(np.arange(168) * 2 * np.pi / 24) + np.random.normal(0, 2, 168),
    'umidade': 60 + 20 * np.sin(np.arange(168) * 2 * np.pi / 24 + np.pi/4) + np.random.normal(0, 5, 168)
})

# AnÃ¡lise rÃ¡pida
print("ðŸ“Š Qualidade do Ar - SÃ£o JosÃ© SC")
print(f"PM2.5 mÃ©dio: {dados_iot['pm25'].mean():.1f} Î¼g/mÂ³")
print(f"Temperatura mÃ©dia: {dados_iot['temperatura'].mean():.1f}Â°C")
print(f"Dias com qualidade ruim (PM2.5 > 35): {(dados_iot['pm25'] > 35).sum()}")

# VisualizaÃ§Ã£o
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.plot(dados_iot['pm25'])
plt.title('PM2.5')
plt.subplot(1, 3, 2)
plt.plot(dados_iot['temperatura'])
plt.title('Temperatura')
plt.subplot(1, 3, 3)
plt.plot(dados_iot['umidade'])
plt.title('Umidade')
plt.tight_layout()
plt.show()
```

#### **ðŸ–ï¸ Machine Learning - Turismo Floripa**
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Dados sintÃ©ticos de ocupaÃ§Ã£o hoteleira
np.random.seed(42)
dados_turismo = pd.DataFrame({
    'mes': np.random.randint(1, 13, 1000),
    'dia_semana': np.random.randint(1, 8, 1000),
    'temperatura': np.random.normal(25, 5, 1000),
    'chuva': np.random.choice([0, 1], 1000, p=[0.7, 0.3]),
    'feriado': np.random.choice([0, 1], 1000, p=[0.9, 0.1])
})

# Simular ocupaÃ§Ã£o baseada nas features
ocupacao = (
    50 +  # base
    dados_turismo['mes'].apply(lambda x: 30 if x in [12, 1, 2] else 0) +  # verÃ£o
    dados_turismo['dia_semana'].apply(lambda x: 20 if x in [6, 7] else 0) +  # fim de semana
    dados_turismo['temperatura'] * 0.5 +  # temperatura
    dados_turismo['feriado'] * 25 -  # feriados
    dados_turismo['chuva'] * 15 +  # chuva reduz ocupaÃ§Ã£o
    np.random.normal(0, 10, 1000)  # ruÃ­do
)
dados_turismo['ocupacao'] = np.clip(ocupacao, 0, 100)

# Treinar modelo
X = dados_turismo[['mes', 'dia_semana', 'temperatura', 'chuva', 'feriado']]
y = dados_turismo['ocupacao']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

modelo = RandomForestRegressor(n_estimators=100, random_state=42)
modelo.fit(X_train, y_train)

# PrediÃ§Ã£o para um fim de semana de verÃ£o
predicao = modelo.predict([[1, 7, 28, 0, 0]])  # Janeiro, domingo, 28Â°C, sem chuva, sem feriado
print(f"ðŸ¨ OcupaÃ§Ã£o prevista: {predicao[0]:.1f}%")

# ImportÃ¢ncia das variÃ¡veis
importancias = pd.DataFrame({
    'variavel': X.columns,
    'importancia': modelo.feature_importances_
}).sort_values('importancia', ascending=False)
print("\nðŸ“ˆ Fatores mais importantes:")
for _, row in importancias.iterrows():
    print(f"{row['variavel']}: {row['importancia']:.3f}")
```

#### **âš¡ Big Data com PySpark - DETRAN SC**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg

# Inicializar Spark
spark = SparkSession.builder \
    .appName("DETRAN-SC Analysis") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# Simular dados de veÃ­culos de SC
dados_veiculos = [
    ("FlorianÃ³polis", "Carro", 2023, 45000, "Gasoline"),
    ("SÃ£o JosÃ©", "Moto", 2023, 12000, "Gasoline"),
    ("PalhoÃ§a", "Carro", 2022, 18000, "Flex"),
    ("BiguaÃ§u", "Carro", 2023, 8000, "Flex"),
    ("FlorianÃ³polis", "Moto", 2022, 25000, "Gasoline"),
    ("SÃ£o JosÃ©", "Carro", 2023, 22000, "Electric"),
    ("Laguna", "Carro", 2021, 5000, "Gasoline"),
    ("Joinville", "Carro", 2023, 35000, "Flex"),
    ("Blumenau", "Moto", 2023, 15000, "Gasoline"),
    ("ItajaÃ­", "Carro", 2022, 12000, "Flex")
]

colunas = ["cidade", "tipo", "ano", "quantidade", "combustivel"]
df_veiculos = spark.createDataFrame(dados_veiculos, colunas)

print("ðŸš— AnÃ¡lise de Frota - DETRAN SC")
print("=" * 40)

# AnÃ¡lise por cidade
print("\nðŸ“ VeÃ­culos por cidade:")
df_veiculos.groupBy("cidade") \
    .agg(count("*").alias("registros"), 
         sum("quantidade").alias("total_veiculos")) \
    .orderBy(col("total_veiculos").desc()) \
    .show()

# AnÃ¡lise por tipo de combustÃ­vel
print("â›½ DistribuiÃ§Ã£o por combustÃ­vel:")
df_veiculos.groupBy("combustivel") \
    .agg(sum("quantidade").alias("total")) \
    .orderBy(col("total").desc()) \
    .show()

# TendÃªncia de eletrificaÃ§Ã£o
print("ðŸ”‹ VeÃ­culos elÃ©tricos:")
df_veiculos.filter(col("combustivel") == "Electric").show()

spark.stop()
```

---

#### **Para UsuÃ¡rios IntermediÃ¡rios**

**1. Explore os Dados PrÃ¡ticos**
```python
# Exemplo do CapÃ­tulo 4: Spark com dados do DETRAN-SC
from pyspark.sql import SparkSession

# Inicializar Spark
spark = SparkSession.builder \
    .appName("DETRAN-SC Analysis") \
    .getOrCreate()

# Simular dados de veÃ­culos de SC
dados_veiculos = [
    ("FlorianÃ³polis", "Carro", 2020, 15000),
    ("SÃ£o JosÃ©", "Moto", 2021, 8000),
    ("PalhoÃ§a", "Carro", 2022, 12000),
    ("BiguaÃ§u", "Moto", 2020, 3000)
]

colunas = ["cidade", "tipo", "ano", "quantidade"]
df_spark = spark.createDataFrame(dados_veiculos, colunas)

# AnÃ¡lise por cidade
df_spark.groupBy("cidade").sum("quantidade").show()
```

**2. Aplique Machine Learning**
```python
# Exemplo do CapÃ­tulo 5: ML para preÃ§os imobiliÃ¡rios
from sklearn.linear_model import LinearRegression
import numpy as np

# Dados simulados de imÃ³veis em Floripa
area = np.array([60, 80, 120, 150, 200]).reshape(-1, 1)
preco = np.array([400000, 550000, 750000, 900000, 1200000])

# Treinar modelo
modelo = LinearRegression()
modelo.fit(area, preco)

# Predizer preÃ§o para apartamento de 100mÂ²
preco_100m2 = modelo.predict([[100]])
print(f"PreÃ§o estimado para 100mÂ²: R$ {preco_100m2[0]:,.0f}")
```

---

### **ðŸ”§ SoluÃ§Ã£o de Problemas Comuns**

#### **Erro: Java nÃ£o encontrado**
```bash
# Verificar JAVA_HOME
echo $JAVA_HOME  # Linux/macOS
echo %JAVA_HOME%  # Windows

# Se vazio, definir manualmente:
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64  # Linux
export JAVA_HOME=/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home  # macOS
```

#### **Erro: PySpark nÃ£o inicializa**
```python
# ConfiguraÃ§Ã£o manual do PySpark
import os
os.environ['JAVA_HOME'] = '/caminho/para/java'
os.environ['SPARK_HOME'] = '/caminho/para/spark'

import findspark
findspark.init()

import pyspark
```

#### **Erro: MÃ³dulo nÃ£o encontrado**
```bash
# Reinstalar dependÃªncias
pip install --upgrade -r requirements.txt

# Verificar se estÃ¡ no ambiente virtual correto
which python  # Linux/macOS
where python   # Windows
```

---

### **ðŸ“± Testando Sua InstalaÃ§Ã£o - Checklist Completo**

**âœ… Teste 1: Python e Pandas**
```python
import pandas as pd
print("âœ… Pandas funcionando!")
df = pd.DataFrame({'nome': ['Patrick'], 'cidade': ['FlorianÃ³polis']})
print(df)
```

**âœ… Teste 2: VisualizaÃ§Ã£o**
```python
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0, 10, 100)
y = np.sin(x)
plt.plot(x, y)
plt.title("âœ… Matplotlib funcionando!")
plt.show()
```

**âœ… Teste 3: Big Data (PySpark)**
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Teste").getOrCreate()
df = spark.createDataFrame([("Patrick", "FlorianÃ³polis")], ["nome", "cidade"])
df.show()
print("âœ… PySpark funcionando!")
spark.stop()
```

**âœ… Teste 4: Machine Learning**
```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression

X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, random_state=42)
model = LogisticRegression()
model.fit(X, y)
print("âœ… Scikit-learn funcionando!")
```

---

### **ðŸ“š PrÃ³ximos Passos ApÃ³s InstalaÃ§Ã£o**

1. **ðŸ“– Leia o CapÃ­tulo 1** - Entenda a jornada de Patrick
2. **ðŸ’» Abra Jupyter Lab** - Ambiente interativo para experimentar
3. **ðŸ” Explore os dados** - Cada capÃ­tulo tem exemplos prÃ¡ticos
4. **ðŸ› ï¸ Adapte para sua regiÃ£o** - Use os conceitos em seus projetos
5. **ðŸ¤ Compartilhe resultados** - Contribua com a comunidade

---

## ðŸ“ˆ **EspecificaÃ§Ãµes TÃ©cnicas do Projeto**

### **ðŸ“‹ Requisitos de Sistema**
- **Sistema Operacional**: Windows 10+, macOS 10.15+, Ubuntu 18.04+
- **Python**: 3.8, 3.9, 3.10 ou 3.11 (testado)
- **Java**: OpenJDK 8 ou 11 (para PySpark)
- **RAM**: MÃ­nimo 4GB (recomendado 8GB+)
- **EspaÃ§o em Disco**: 2GB livres

### **ðŸ“¦ DependÃªncias Principais**
```txt
pandas>=2.1.0          # AnÃ¡lise de dados
numpy>=1.24.0           # ComputaÃ§Ã£o numÃ©rica  
matplotlib>=3.7.0       # VisualizaÃ§Ã£o bÃ¡sica
seaborn>=0.12.0         # VisualizaÃ§Ã£o estatÃ­stica
pyspark>=3.5.0          # Big Data processing
scikit-learn>=1.3.0     # Machine Learning
jupyter>=1.0.0          # Notebooks interativos
```

### **â±ï¸ Tempo de InstalaÃ§Ã£o**
- **InstalaÃ§Ã£o bÃ¡sica**: 5-10 minutos
- **ConfiguraÃ§Ã£o Java**: 2-5 minutos
- **Teste completo**: 3-5 minutos
- **Total**: 15-20 minutos

### **ðŸ“Š Estrutura do ConteÃºdo**
- **5 CapÃ­tulos narrativos**: 60+ pÃ¡ginas
- **Exemplos prÃ¡ticos**: 20+ cÃ³digos testados
- **Casos reais**: Dados de Santa Catarina
- **NÃ­vel**: Iniciante â†’ IntermediÃ¡rio
- **DuraÃ§Ã£o estudo**: 8-12 horas

### **ðŸŽ¯ Compatibilidade Testada**
```bash
âœ… Windows 10/11 + Python 3.9 + Java 8
âœ… macOS Monterey + Python 3.10 + Java 11  
âœ… Ubuntu 20.04 + Python 3.8 + Java 8
âœ… Google Colab (online, sem instalaÃ§Ã£o)
âœ… Jupyter Lab + VSCode + PyCharm
```

---

## ðŸŒŸ **Depoimentos (Baseados no Projeto)**

*"Finalmente um repositÃ³rio que explica Big Data atravÃ©s de histÃ³rias! Patrick torna o aprendizado muito mais envolvente."* - **Estudante de CiÃªncia de Dados**

*"Os casos de Santa Catarina sÃ£o perfeitos para entender como aplicar esses conceitos na nossa realidade brasileira."* - **Analista de Dados**

*"A abordagem de menos cÃ³digo e mais explicaÃ§Ã£o foi fundamental para compreender os conceitos de verdade."* - **Gestor PÃºblico**

*"Patrick como personagem Ãºnico dÃ¡ consistÃªncia e facilita o acompanhamento de toda a jornada de aprendizado."* - **Professor de Tecnologia**

---

## ðŸ¤ **Contribuindo para o Projeto**

Este repositÃ³rio estÃ¡ em **constante evoluÃ§Ã£o educativa**:

1. **Leia e dÃª feedback** sobre a clareza das explicaÃ§Ãµes
2. **Sugira melhorias** na narrativa ou nos conceitos
3. **Compartilhe** como aplicou os conhecimentos em seus projetos
4. **Adapte** os casos para sua regiÃ£o e compartilhe os resultados
5. **Contribua** com novos casos prÃ¡ticos baseados em storytelling

### **Como Contribuir**
- Abra issues com sugestÃµes de melhoria
- Proponha novos casos baseados em dados reais
- Sugira melhorias na consistÃªncia narrativa
- Compartilhe aplicaÃ§Ãµes prÃ¡ticas dos conceitos

---

## ðŸ“ž **Sobre o Projeto**

- **Metodologia**: Storytelling educativo para ensino de Big Data
- **Protagonista**: Patrick - personagem consistente em todos os capÃ­tulos
- **LocalizaÃ§Ã£o**: Casos baseados em Santa Catarina, aplicÃ¡veis universalmente
- **Objetivo**: Democratizar conhecimento em Big Data atravÃ©s de narrativas envolventes
- **Diferencial**: Foco em compreensÃ£o conceitual, nÃ£o em cÃ³digo complexo

---

## ðŸ“„ **LicenÃ§a e Uso**

Este conteÃºdo Ã© disponibilizado para fins **educacionais e nÃ£o comerciais**. 

### **Uso Permitido**
- Estudo pessoal e acadÃªmico
- AdaptaÃ§Ã£o dos conceitos para projetos prÃ³prios
- Compartilhamento com fins educativos
- ReferÃªncia em trabalhos acadÃªmicos (com citaÃ§Ã£o)

**Desenvolvido com â¤ï¸ em Santa Catarina para estudantes e profissionais que buscam aprender Big Data atravÃ©s de storytelling educativo.**
>>>>>>> 674e9719d70cbceeaa21432c530ca2e70cd98a61
#   S i t e   r e b u i l d   0 9 / 2 5 / 2 0 2 5   1 2 : 2 1 : 5 8 
 
 