# 🚀 Tópicos de Big Data em Python

**Plataforma educacional completa para aprendizado de Big Data com Apache Hadoop, Spark, análise de dados e casos práticos de Santa Catarina.**

[![Status](https://img.shields.io/badge/Status-Online-brightgreen)](https://datascience-pro.netlify.app)
[![Hadoop](https://img.shields.io/badge/Hadoop-3.3.4-orange)](https://hadoop.apache.org/)
[![Spark](https://img.shields.io/badge/Spark-3.5.0-red)](https://spark.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-blue)](https://python.org/)
[![Questões](https://img.shields.io/badge/Quest%C3%B5es-40+-green)](questoes-concurso-bigdata.md)

## 🌟 Recursos Principais

### 🐘 Apache Hadoop - Laboratório Completo
- **🇧🇷 100% em Português** - Explicações didáticas e claras
- **🗺️ MapReduce Prático** - Exemplos funcionais com Python
- **💾 Tutorial HDFS** - Sistema de arquivos distribuído
- **💻 Guia de Instalação** - Passo a passo no Ubuntu
- **🏋️ Exercícios Hands-on** - Datasets reais para praticar

### ⚡ Apache Spark - Processamento Distribuído
- **PySpark Completo** - Tutoriais práticos e avançados
- **Spark SQL** - Análise de dados com SQL
- **Streaming** - Processamento em tempo real
- **Machine Learning** - MLlib para Big Data

### 📊 Análise de Dados Científicos
- **Upload Inteligente** - CSV, JSON, Excel
- **Análise Automática** - Estatísticas e correlações
- **Visualizações Avançadas** - Gráficos interativos
- **Relatórios Completos** - Exportação de resultados

### 🎓 Conteúdo Acadêmico
- **15 Aulas Completas** - Do básico ao avançado
- **5 Módulos Especializados** - Fundamentos, Hadoop, Spark, Pandas, ML/DL
- **40+ Questões de Concurso** - Preparação para concursos públicos
- **Casos Práticos SC** - Exemplos reais de Santa Catarina

## 🌐 Acesso Online



**🚀 Site Principal:** https://datascience-pro.netlify.app
**🐘 Laboratório Hadoop:** https://datascience-pro.netlify.app/aulas/aula06-hadoop-intro/laboratorio-hadoop-pratico.html
**� Questões de Concurso:** https://datascience-pro.netlify.app/questoes-concurso-bigdata.md

## 🚀 Status Atual - TUDO FUNCIONANDO! ✅

- ✅ **Site Online e Responsivo**
- ✅ **Laboratório Hadoop 100% Português** 
- ✅ **MapReduce com Python Funcionando**
- ✅ **HDFS Tutorial Didático Completo**
- ✅ **Apache Spark PySpark Tutoriais**
- ✅ **15 Aulas Estruturadas**
- ✅ **40+ Questões de Concurso Público**
- ✅ **5 Módulos Especializados**
- ✅ **Design Profissional e Responsivo**
- ✅ **Deploy Netlify Automático**

**Última atualização: 29/09/2025 - Conteúdo expandido e otimizado!** 🎉

## 📚 Estrutura do Curso

### 📖 Aulas Principais (15 Aulas)
```
aulas/
├── aula01-intro-bigdata/          # Introdução e conceitos fundamentais
├── aula02-iot-computacao/         # IoT e computação ubíqua
├── aula03-cloud-streaming/        # Cloud computing e streaming
├── aula04-revisao-python/         # Revisão Python para Big Data
├── aula05-analise-dados-resumo/   # Análise de dados e estatística
├── aula06-hadoop-intro/           # Apache Hadoop - Introdução
├── aula07-spark-fundamentals/     # Apache Spark - Fundamentos
├── aula08-kafka-streaming/        # Apache Kafka - Streaming
├── aula09-ml-bigdata/            # Machine Learning para Big Data
├── aula10-ml-distribuido/        # ML Distribuído
├── aula11-graph-analytics/       # Análise de Grafos
├── aula12-databricks-cloud/      # Databricks e Cloud
├── aula13-deep-learning-bigdata/ # Deep Learning em Big Data
├── aula14-edge-computing-iot/    # Edge Computing e IoT
└── aula15-quantum-computing/     # Computação Quântica
```

### 🎯 Módulos Especializados (5 Módulos)
```
aulas/
├── modulo1-fundamentos/          # Fundamentos de Big Data
├── modulo2-hadoop/              # Ecossistema Hadoop completo
├── modulo3-spark/               # Apache Spark avançado
├── modulo4-pandas/              # Pandas para análise de dados
└── modulo5-ml-dl/               # Machine Learning e Deep Learning
```

### � Questões de Concurso (40+ Questões)
- **Baseadas em concursos reais** - CESPE, FCC, VUNESP, FGV, CONSULPLAN
- **Nível técnico elevado** - Analista de TI, Analista de Sistemas
- **Cobertura completa** - Hadoop, Spark, Kafka, NoSQL, ML, Governança
- **Explicações detalhadas** - Conceitos e aplicação prática
- **Caso governamental** - Tribunal de Contas de SC

## 🎯 Destaques do Projeto

### 🏆 **Big Data mais Completo em Português**
- Tutorial Hadoop passo-a-passo com instalação Ubuntu
- PySpark com exemplos práticos de processamento distribuído
- Kafka para streaming de dados em tempo real
- Machine Learning distribuído com MLlib

### 📊 **Casos Reais de Santa Catarina**
- Análise de tráfego da Ponte Hercílio Luz
- Sistemas IoT de monitoramento urbano em São José
- Dados turísticos de Florianópolis
- Frota de veículos do DETRAN-SC
- Mercado imobiliário da Grande Florianópolis

### 🎓 **Preparação para Concursos**
- 40+ questões baseadas em provas reais
- Cobertura de tecnologias avançadas: Atlas, Ranger, Knox, Ambari
- Governança de dados e compliance LGPD
- Arquiteturas Lambda e Kappa
- Cenários governamentais práticos
## 🚀 Instalação e Primeiros Passos

### 📋 Pré-requisitos
- **Python 3.8+** - [Download oficial](https://python.org/downloads)
- **Java 8 ou 11** - [OpenJDK](https://adoptium.net/) (para PySpark)
- **Git** - [Download](https://git-scm.com/downloads)

### ⚡ Instalação Rápida (5 minutos)
```bash
# 1. Clone o repositório
git clone https://github.com/cordeirotelecom/topicos-bigdata-python.git
cd topicos-bigdata-python

# 2. Crie ambiente virtual
python -m venv venv
# Windows: venv\Scripts\activate
# Linux/macOS: source venv/bin/activate

# 3. Instale dependências
pip install -r requirements.txt

# 4. Teste PySpark
python -c "import pyspark; print('✅ PySpark OK!')"

# 5. Inicie Jupyter
jupyter lab
```

### 📖 Primeiros Passos
1. **Abra Jupyter Lab** e navegue até `aulas/aula01-intro-bigdata/`
2. **Execute os exemplos** interativos de análise de dados
3. **Explore o laboratório Hadoop** em `aulas/aula06-hadoop-intro/`
4. **Teste questões de concurso** em `questoes-concurso-bigdata.md`

## 🔧 Tecnologias e Ferramentas

### 🐍 **Python e Big Data**
```python
# Principais bibliotecas utilizadas
pandas>=2.1.0          # Análise de dados
numpy>=1.24.0           # Computação numérica
pyspark>=3.5.0          # Big Data processing
scikit-learn>=1.3.0     # Machine Learning
matplotlib>=3.7.0       # Visualização
seaborn>=0.12.0         # Gráficos estatísticos
jupyter>=1.0.0          # Notebooks interativos
```

### 🛠️ **Ecossistema Hadoop**
- **HDFS** - Sistema de arquivos distribuído
- **MapReduce** - Processamento paralelo
- **YARN** - Gerenciamento de recursos
- **Hive** - SQL sobre Hadoop
- **HBase** - Banco NoSQL distribuído
- **Sqoop** - Integração com SGBDs

### ⚡ **Apache Spark**
- **PySpark** - API Python para Spark
- **Spark SQL** - Consultas SQL distribuídas
- **Spark Streaming** - Processamento de streams
- **MLlib** - Machine Learning distribuído
- **GraphX** - Análise de grafos

### 🌊 **Streaming e Tempo Real**
- **Apache Kafka** - Plataforma de streaming
- **Apache Storm** - Processamento de streams
- **Apache Flink** - Stream processing avançado

## 💡 Casos de Uso Práticos

### 🏛️ **Setor Público**
- **Transparência governamental** - Análise de gastos públicos
- **Smart cities** - Monitoramento urbano com IoT
- **Saúde pública** - Análise epidemiológica
- **Educação** - Métricas de desempenho escolar
- **Segurança** - Análise de dados criminais

### 🏢 **Setor Privado**
- **E-commerce** - Sistemas de recomendação
- **Finanças** - Detecção de fraudes
- **Telecomunicações** - Análise de tráfego de rede
- **Varejo** - Análise de comportamento do consumidor
- **Logística** - Otimização de rotas e entregas

### 🎓 **Acadêmico e Pesquisa**
- **Preparação para concursos** - 40+ questões reais
- **Dissertações e teses** - Casos práticos para pesquisa
- **Cursos técnicos** - Material didático completo
- **Extensão universitária** - Projetos comunitários

## 📚 Livro Digital: Big Data em Python

*"Big Data em Python: Casos Práticos de Santa Catarina - Um guia educacional através de storytelling com casos reais"*

### 📖 **5 Capítulos Narrativos com Patrick**
1. **O Despertar dos Dados** - Patrick descobre Big Data em Florianópolis
2. **IoT e Cidades Inteligentes** - Patrick explora sensores em São José  
3. **Análise de Dados Turísticos** - Patrick desvenda padrões do turismo
4. **Apache Spark em Ação** - Patrick processa dados do DETRAN-SC
5. **Machine Learning Aplicado** - Patrick prevê preços imobiliários

### 🎭 **Metodologia Narrativa Única**
- **Patrick** como protagonista consistente em todos os capítulos
- **Storytelling educativo** para aprendizado envolvente
- **Menos código, mais explicação** - foco na compreensão
- **Contexto local** mas aplicável universalmente
- **80+ skills** essenciais para o mercado de trabalho

## 🤝 Como Contribuir

### 📝 **Tipos de Contribuição**
- **Melhorar explicações** - Tornar conceitos mais claros
- **Adicionar casos práticos** - Exemplos de sua região
- **Corrigir erros** - Bugs, typos, links quebrados
- **Expandir questões** - Mais questões de concursos reais
- **Traduzir conteúdo** - Versões em outros idiomas

### 🛠️ **Como Contribuir**
```bash
# 1. Fork o projeto no GitHub
# 2. Clone seu fork
git clone https://github.com/SEU-USUARIO/topicos-bigdata-python.git

# 3. Crie uma branch para sua contribuição
git checkout -b minha-contribuicao

# 4. Faça suas alterações e commit
git add .
git commit -m "Adiciona: Nova explicação sobre Kafka"

# 5. Push e abra Pull Request
git push origin minha-contribuicao
```

## 🎯 Público-Alvo

### 👨‍🎓 **Estudantes**
- **Graduação** em Ciência da Computação, Engenharia, Administração
- **Pós-graduação** em Big Data, Data Science, BI
- **Cursos técnicos** em informática e análise de dados
- **Autodidatas** interessados em tecnologia e dados

### 👨‍💼 **Profissionais**
- **Analistas de dados** iniciantes e intermediários
- **Desenvolvedores** que querem aprender Big Data
- **Gestores de TI** buscando conhecimento técnico
- **Consultores** em transformação digital

### 🏛️ **Setor Público**
- **Analistas de TI** em concursos públicos
- **Gestores públicos** interessados em inovação
- **Auditores** que trabalham com grandes volumes de dados
- **Técnicos** em órgãos de controle e transparência

## 📞 Suporte e Comunidade

### 💬 **Canais de Comunicação**
- **GitHub Issues** - [Reportar problemas](https://github.com/cordeirotelecom/topicos-bigdata-python/issues)
- **GitHub Discussions** - [Fórum da comunidade](https://github.com/cordeirotelecom/topicos-bigdata-python/discussions)
- **Site oficial** - [https://datascience-pro.netlify.app](https://datascience-pro.netlify.app)

### 📚 **Recursos Adicionais**
- **[Guia de Instalação Completo](INSTALACAO_COMPLETA.md)** - Tutorial detalhado
- **[Guia de IA](GUIA_INTELIGENCIA_ARTIFICIAL.md)** - Machine Learning e Deep Learning
- **[Casos Práticos](CASOS_PRATICOS.md)** - Projetos completos prontos
- **[Questões de Concurso](questoes-concurso-bigdata.md)** - 40+ questões reais

## 📄 Licença e Uso

Este projeto é disponibilizado sob **licença educacional** para:
- ✅ **Estudo pessoal e acadêmico**
- ✅ **Uso em cursos e treinamentos não comerciais**
- ✅ **Adaptação para projetos educacionais**
- ✅ **Referência em trabalhos acadêmicos** (com citação)

---

## Big Data em Python: Casos Práticos de Santa Catarina

*Um guia educacional de Big Data através de storytelling com casos reais de Florianópolis e região*

*Um guia educacional de Big Data através de storytelling com casos reais de Florianópolis e região*

---

## 📖 Sobre o Projeto

Este repositório apresenta conceitos de **Big Data** e **Python** através de **narrativas educacionais** baseadas em casos reais de Santa Catarina, seguindo o protagonista **Patrick** em suas aventuras com dados.

### 🎯 **Diferenciais**
- **Storytelling educativo**: Aprendizado através de narrativas envolventes
- **Casos reais de SC**: Ponte Hercílio Luz, DETRAN-SC, turismo de Floripa
- **Menos código, mais explicação**: Foco na compreensão conceitual
- **Contexto local**: Exemplos práticos da Grande Florianópolis
- **Protagonista único**: Patrick como guia consistente em todos os capítulos

### 📚 **Estrutura do Livro (5 Capítulos Narrativos)**

#### **Parte I: Fundamentos através de Histórias**
1. **O Despertar dos Dados** - Patrick descobre Big Data em Florianópolis ✅
2. **IoT e Cidades Inteligentes** - Patrick explora sensores em São José ✅
3. **Análise de Dados Turísticos** - Patrick desvenda padrões do turismo ✅

#### **Parte II: Tecnologias Avançadas**  
4. **Apache Spark em Ação** - Patrick processa dados do DETRAN-SC ✅
5. **Machine Learning Aplicado** - Patrick prevê preços imobiliários ✅

### 🎭 **Metodologia Narrativa**
- **Patrick**: Protagonista único e consistente em todos os capítulos
- **Contexto SC**: Todos os casos baseados em Santa Catarina
- **Educação**: Foco em explicações didáticas, não em código complexo
- **Aplicabilidade**: Conceitos aplicáveis a qualquer região do Brasil

---

## 🎓 **Público-Alvo**

- **Estudantes de tecnologia** e ciência de dados
- **Analistas de dados** iniciantes/intermediários
- **Gestores públicos** interessados em transformação digital
- **Profissionais** que buscam aprender Big Data de forma didática
- **Qualquer pessoa** interessada em dados e storytelling educativo

---

## 🛠️ **Tecnologias e Conceitos Abordados**

### **Linguagens e Frameworks**
- **Python**: Pandas, NumPy, Matplotlib, Seaborn
- **Big Data**: Apache Spark, PySpark, processamento distribuído
- **Machine Learning**: Scikit-learn, regressão, classificação
- **Dados**: APIs, CSV, JSON, análise exploratória
- **Visualização**: Gráficos interpretativos e storytelling com dados

### **Conceitos Fundamentais**
- Volume, Velocidade, Variedade, Veracidade, Valor (5 V's)
- ETL/ELT e pipelines de dados
- IoT e sensores urbanos
- Smart cities e transformação digital
- Feature engineering e modelagem preditiva

---

## � **Palavras-Chave para Profissionais de Big Data e Análise de Dados**

*Skills essenciais que aparecem em vagas de emprego e são abordadas neste livro*

### **Linguagens e Frameworks**
Python, PySpark, Apache Spark, SQL, Scala, R, Java, Hadoop, Hive, Apache Kafka, Apache Airflow, Apache Beam, Apache Flink, Databricks, Snowflake, dbt, Great Expectations, MLflow, Kubeflow, TensorFlow, PyTorch, Keras, XGBoost, LightGBM, Scikit-learn, Pandas, NumPy, Matplotlib, Seaborn, Plotly, Streamlit, Dash, FastAPI, Flask, Django

### **Cloud e Infraestrutura**
AWS, Azure, Google Cloud Platform (GCP), Amazon S3, Azure Data Lake, Google BigQuery, Amazon Redshift, Azure Synapse Analytics, Google Cloud Storage, EC2, Azure VM, Google Compute Engine, Lambda, Azure Functions, Google Cloud Functions, Docker, Kubernetes, Terraform, Apache Mesos, Yarn, Spark Cluster, Elasticsearch, MongoDB, Cassandra, Redis, PostgreSQL, MySQL, Oracle, SQL Server

### **Ferramentas de Dados e ETL**
Apache Nifi, Talend, Informatica, Pentaho, SSIS, Azure Data Factory, AWS Glue, Google Cloud Dataflow, Apache Sqoop, Apache Flume, Logstash, Fivetran, Stitch, Airbyte, Singer, Apache Superset, Tableau, Power BI, Looker, Grafana, Metabase, Apache Zeppelin, Jupyter Notebooks, Google Colab, Apache Parquet, Apache Avro, JSON, XML, CSV

### **Machine Learning e IA**
Deep Learning, Neural Networks, CNN, RNN, LSTM, GAN, Transformer, BERT, GPT, Computer Vision, Natural Language Processing (NLP), Time Series Analysis, Recommender Systems, Classification, Regression, Clustering, Dimensionality Reduction, Feature Engineering, Model Selection, Cross-validation, Hyperparameter Tuning, A/B Testing, MLOps, Model Deployment, Model Monitoring, AutoML, Ensemble Methods

### **Metodologias e Conceitos**
Data Engineering, Data Science, Data Analytics, Business Intelligence, ETL/ELT, Data Pipeline, Data Warehouse, Data Lake, Lakehouse, Data Mesh, Real-time Processing, Batch Processing, Stream Processing, Data Governance, Data Quality, Data Lineage, Data Catalog, Metadata Management, GDPR, Data Privacy, Agile, Scrum, DevOps, CI/CD, Git, Version Control

### **Especialização e Soft Skills**
Statistics, Mathematics, Linear Algebra, Probability, Hypothesis Testing, Statistical Modeling, Experimentation Design, Problem Solving, Critical Thinking, Communication, Data Storytelling, Data Visualization, Business Acumen, Domain Knowledge, Project Management, Teamwork, Leadership, Continuous Learning, Adaptability, Innovation

---

## �📊 **Casos Reais Desenvolvidos**

### 🌉 **Capítulo 1: Despertar dos Dados - Ponte Hercílio Luz**
- Patrick descobre padrões no tráfego da ponte
- Análise de 2,8 milhões de veículos/ano
- Introdução aos conceitos de Big Data
- Insights sobre mobilidade urbana em Florianópolis

### 🏙️ **Capítulo 2: São José Conectado**  
- Patrick explora sistemas IoT de monitoramento urbano
- Sensores de qualidade do ar e tráfego
- Conceitos de smart cities aplicados
- Transformação digital em cidades médias

### 🏖️ **Capítulo 3: Turismo de Florianópolis**
- Patrick analisa sazonalidade da ocupação hoteleira
- Padrões de demanda turística na Ilha da Magia
- Dados reais de turismo de SC
- Previsão e otimização para o setor

### 🚗 **Capítulo 4: DETRAN Santa Catarina**
- Patrick processa 4,2 milhões de veículos registrados
- Introdução ao Apache Spark e processamento distribuído
- Análise de frota por município catarinense
- Escalabilidade para grandes volumes de dados

### 🏠 **Capítulo 5: Mercado Imobiliário de Floripa**
- Patrick desenvolve Machine Learning para previsão de preços
- Fatores de valorização na Ilha da Magia
- Feature engineering com dados locais
- Aplicação prática de algoritmos de ML
- 80+ skills essenciais para o mercado de trabalho

---

## 🚀 **Guia Prático: Instalação e Uso**

### **📖 Guias Completos Disponíveis**

#### **🛠️ [INSTALACAO_COMPLETA.md](./INSTALACAO_COMPLETA.md)**
Tutorial detalhado passo-a-passo para configurar todo ambiente:
- Instalação Python, Java, bibliotecas
- Configuração ambiente virtual
- Teste automatizado de validação
- Solução de problemas comuns
- Scripts de verificação completa

#### **🤖 [GUIA_INTELIGENCIA_ARTIFICIAL.md](./GUIA_INTELIGENCIA_ARTIFICIAL.md)**
Guia completo de IA aplicada a Big Data com casos práticos:
- Machine Learning com dados de SC
- Deep Learning para séries temporais
- Processamento de Linguagem Natural (NLP)
- Computer Vision e Algoritmos Genéticos
- 5 casos práticos prontos para executar

#### **🎯 [CASOS_PRATICOS.md](./CASOS_PRATICOS.md)**
Projetos completos prontos para usar e adaptar:
- Dashboard interativo com Streamlit
- Sistema de predição de demanda turística
- API REST para dados urbanos em tempo real
- Exemplos práticos com código completo
- Algoritmos Genéticos para otimização
- Projetos avançados e próximos passos

### **📋 Pré-requisitos**

**Sistema Operacional:**
- Windows 10/11, macOS 10.15+, ou Linux Ubuntu 18.04+

**Software Essencial:**
- **Python 3.8+** - [Download aqui](https://python.org/downloads)
- **Java 8 ou 11** (para PySpark) - [Download OpenJDK](https://adoptium.net/)
- **Git** - [Download aqui](https://git-scm.com/downloads)

**Verificar Instalações:**
```bash
# Verificar Python
python --version
# Deve mostrar: Python 3.8.x ou superior

# Verificar Java  
java -version
# Deve mostrar: openjdk version "8" ou "11"

# Verificar Git
git --version
```

---

### **⚡ Instalação Rápida (5 minutos)**

#### **Passo 1: Clone o Repositório**
```bash
# Abra o terminal/prompt de comando
git clone https://github.com/cordeirotelecom/topicos-bigdata-python.git
cd topicos-bigdata-python
```

#### **Passo 2: Crie um Ambiente Virtual (Recomendado)**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux  
python3 -m venv venv
source venv/bin/activate
```

#### **Passo 3: Instale as Dependências**
```bash
# Instalar todas as bibliotecas necessárias
pip install -r requirements.txt

# Verificar se PySpark foi instalado corretamente
python -c "import pyspark; print('PySpark OK!')"
```

#### **Passo 4: Teste a Instalação**
```bash
# Iniciar Jupyter Lab
jupyter lab

# Ou Jupyter Notebook clássico
jupyter notebook
```

---

### **📖 Como Usar: Guia Passo a Passo**

#### **Para Iniciantes Completos**

**1. Comece pelo Capítulo 1**
```bash
# Navegue até a pasta do livro
cd livro/

# Abra o primeiro capítulo
# Windows: notepad capitulo01-despertar-dos-dados.md
# macOS: open capitulo01-despertar-dos-dados.md  
# Linux: gedit capitulo01-despertar-dos-dados.md
```

**2. Siga a Ordem dos Capítulos**
- 📖 **Capítulo 1**: Conceitos básicos de Big Data
- 🏙️ **Capítulo 2**: IoT e sensores urbanos
- 🏖️ **Capítulo 3**: Análise de dados turísticos
- ⚡ **Capítulo 4**: Processamento distribuído com Spark
- 🤖 **Capítulo 5**: Machine Learning aplicado

**3. Experimente os Conceitos**
```python
# Exemplo prático do Capítulo 1
import pandas as pd
import matplotlib.pyplot as plt

# Simular dados de tráfego da Ponte Hercílio Luz
dados_ponte = {
    'hora': range(0, 24),
    'veiculos': [50, 30, 20, 25, 45, 120, 350, 500, 
                 400, 300, 250, 280, 320, 300, 350, 
                 400, 500, 600, 450, 300, 200, 150, 100, 70]
}

df = pd.DataFrame(dados_ponte)
plt.plot(df['hora'], df['veiculos'])
plt.title('Tráfego na Ponte Hercílio Luz - 24h')
plt.xlabel('Hora do Dia')
plt.ylabel('Número de Veículos')
plt.show()
```

### **💡 Exemplos Práticos Rápidos**

#### **🏙️ Análise de IoT - Sensores em São José**
```python
import pandas as pd
import numpy as np

# Simular dados de sensores de qualidade do ar
np.random.seed(42)
horas = pd.date_range('2025-01-01', periods=168, freq='H')  # 1 semana

dados_iot = pd.DataFrame({
    'timestamp': horas,
    'pm25': np.random.normal(25, 8, 168),  # PM2.5 (μg/m³)
    'temperatura': 20 + 10 * np.sin(np.arange(168) * 2 * np.pi / 24) + np.random.normal(0, 2, 168),
    'umidade': 60 + 20 * np.sin(np.arange(168) * 2 * np.pi / 24 + np.pi/4) + np.random.normal(0, 5, 168)
})

# Análise rápida
print("📊 Qualidade do Ar - São José SC")
print(f"PM2.5 médio: {dados_iot['pm25'].mean():.1f} μg/m³")
print(f"Temperatura média: {dados_iot['temperatura'].mean():.1f}°C")
print(f"Dias com qualidade ruim (PM2.5 > 35): {(dados_iot['pm25'] > 35).sum()}")

# Visualização
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.plot(dados_iot['pm25'])
plt.title('PM2.5')
plt.subplot(1, 3, 2)
plt.plot(dados_iot['temperatura'])
plt.title('Temperatura')
plt.subplot(1, 3, 3)
plt.plot(dados_iot['umidade'])
plt.title('Umidade')
plt.tight_layout()
plt.show()
```

#### **🏖️ Machine Learning - Turismo Floripa**
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Dados sintéticos de ocupação hoteleira
np.random.seed(42)
dados_turismo = pd.DataFrame({
    'mes': np.random.randint(1, 13, 1000),
    'dia_semana': np.random.randint(1, 8, 1000),
    'temperatura': np.random.normal(25, 5, 1000),
    'chuva': np.random.choice([0, 1], 1000, p=[0.7, 0.3]),
    'feriado': np.random.choice([0, 1], 1000, p=[0.9, 0.1])
})

# Simular ocupação baseada nas features
ocupacao = (
    50 +  # base
    dados_turismo['mes'].apply(lambda x: 30 if x in [12, 1, 2] else 0) +  # verão
    dados_turismo['dia_semana'].apply(lambda x: 20 if x in [6, 7] else 0) +  # fim de semana
    dados_turismo['temperatura'] * 0.5 +  # temperatura
    dados_turismo['feriado'] * 25 -  # feriados
    dados_turismo['chuva'] * 15 +  # chuva reduz ocupação
    np.random.normal(0, 10, 1000)  # ruído
)
dados_turismo['ocupacao'] = np.clip(ocupacao, 0, 100)

# Treinar modelo
X = dados_turismo[['mes', 'dia_semana', 'temperatura', 'chuva', 'feriado']]
y = dados_turismo['ocupacao']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

modelo = RandomForestRegressor(n_estimators=100, random_state=42)
modelo.fit(X_train, y_train)

# Predição para um fim de semana de verão
predicao = modelo.predict([[1, 7, 28, 0, 0]])  # Janeiro, domingo, 28°C, sem chuva, sem feriado
print(f"🏨 Ocupação prevista: {predicao[0]:.1f}%")

# Importância das variáveis
importancias = pd.DataFrame({
    'variavel': X.columns,
    'importancia': modelo.feature_importances_
}).sort_values('importancia', ascending=False)
print("\n📈 Fatores mais importantes:")
for _, row in importancias.iterrows():
    print(f"{row['variavel']}: {row['importancia']:.3f}")
```

#### **⚡ Big Data com PySpark - DETRAN SC**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg

# Inicializar Spark
spark = SparkSession.builder \
    .appName("DETRAN-SC Analysis") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# Simular dados de veículos de SC
dados_veiculos = [
    ("Florianópolis", "Carro", 2023, 45000, "Gasoline"),
    ("São José", "Moto", 2023, 12000, "Gasoline"),
    ("Palhoça", "Carro", 2022, 18000, "Flex"),
    ("Biguaçu", "Carro", 2023, 8000, "Flex"),
    ("Florianópolis", "Moto", 2022, 25000, "Gasoline"),
    ("São José", "Carro", 2023, 22000, "Electric"),
    ("Laguna", "Carro", 2021, 5000, "Gasoline"),
    ("Joinville", "Carro", 2023, 35000, "Flex"),
    ("Blumenau", "Moto", 2023, 15000, "Gasoline"),
    ("Itajaí", "Carro", 2022, 12000, "Flex")
]

colunas = ["cidade", "tipo", "ano", "quantidade", "combustivel"]
df_veiculos = spark.createDataFrame(dados_veiculos, colunas)

print("🚗 Análise de Frota - DETRAN SC")
print("=" * 40)

# Análise por cidade
print("\n📍 Veículos por cidade:")
df_veiculos.groupBy("cidade") \
    .agg(count("*").alias("registros"), 
         sum("quantidade").alias("total_veiculos")) \
    .orderBy(col("total_veiculos").desc()) \
    .show()

# Análise por tipo de combustível
print("⛽ Distribuição por combustível:")
df_veiculos.groupBy("combustivel") \
    .agg(sum("quantidade").alias("total")) \
    .orderBy(col("total").desc()) \
    .show()

# Tendência de eletrificação
print("🔋 Veículos elétricos:")
df_veiculos.filter(col("combustivel") == "Electric").show()

spark.stop()
```

---

#### **Para Usuários Intermediários**

**1. Explore os Dados Práticos**
```python
# Exemplo do Capítulo 4: Spark com dados do DETRAN-SC
from pyspark.sql import SparkSession

# Inicializar Spark
spark = SparkSession.builder \
    .appName("DETRAN-SC Analysis") \
    .getOrCreate()

# Simular dados de veículos de SC
dados_veiculos = [
    ("Florianópolis", "Carro", 2020, 15000),
    ("São José", "Moto", 2021, 8000),
    ("Palhoça", "Carro", 2022, 12000),
    ("Biguaçu", "Moto", 2020, 3000)
]

colunas = ["cidade", "tipo", "ano", "quantidade"]
df_spark = spark.createDataFrame(dados_veiculos, colunas)

# Análise por cidade
df_spark.groupBy("cidade").sum("quantidade").show()
```

**2. Aplique Machine Learning**
```python
# Exemplo do Capítulo 5: ML para preços imobiliários
from sklearn.linear_model import LinearRegression
import numpy as np

# Dados simulados de imóveis em Floripa
area = np.array([60, 80, 120, 150, 200]).reshape(-1, 1)
preco = np.array([400000, 550000, 750000, 900000, 1200000])

# Treinar modelo
modelo = LinearRegression()
modelo.fit(area, preco)

# Predizer preço para apartamento de 100m²
preco_100m2 = modelo.predict([[100]])
print(f"Preço estimado para 100m²: R$ {preco_100m2[0]:,.0f}")
```

---

### **🔧 Solução de Problemas Comuns**

#### **Erro: Java não encontrado**
```bash
# Verificar JAVA_HOME
echo $JAVA_HOME  # Linux/macOS
echo %JAVA_HOME%  # Windows

# Se vazio, definir manualmente:
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64  # Linux
export JAVA_HOME=/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home  # macOS
```

#### **Erro: PySpark não inicializa**
```python
# Configuração manual do PySpark
import os
os.environ['JAVA_HOME'] = '/caminho/para/java'
os.environ['SPARK_HOME'] = '/caminho/para/spark'

import findspark
findspark.init()

import pyspark
```

#### **Erro: Módulo não encontrado**
```bash
# Reinstalar dependências
pip install --upgrade -r requirements.txt

# Verificar se está no ambiente virtual correto
which python  # Linux/macOS
where python   # Windows
```

---

### **📱 Testando Sua Instalação - Checklist Completo**

**✅ Teste 1: Python e Pandas**
```python
import pandas as pd
print("✅ Pandas funcionando!")
df = pd.DataFrame({'nome': ['Patrick'], 'cidade': ['Florianópolis']})
print(df)
```

**✅ Teste 2: Visualização**
```python
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0, 10, 100)
y = np.sin(x)
plt.plot(x, y)
plt.title("✅ Matplotlib funcionando!")
plt.show()
```

**✅ Teste 3: Big Data (PySpark)**
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Teste").getOrCreate()
df = spark.createDataFrame([("Patrick", "Florianópolis")], ["nome", "cidade"])
df.show()
print("✅ PySpark funcionando!")
spark.stop()
```

**✅ Teste 4: Machine Learning**
```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression

X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, random_state=42)
model = LogisticRegression()
model.fit(X, y)
print("✅ Scikit-learn funcionando!")
```

---

### **📚 Próximos Passos Após Instalação**

1. **📖 Leia o Capítulo 1** - Entenda a jornada de Patrick
2. **💻 Abra Jupyter Lab** - Ambiente interativo para experimentar
3. **🔍 Explore os dados** - Cada capítulo tem exemplos práticos
4. **🛠️ Adapte para sua região** - Use os conceitos em seus projetos
5. **🤝 Compartilhe resultados** - Contribua com a comunidade

---

## 📈 **Especificações Técnicas do Projeto**

### **📋 Requisitos de Sistema**
- **Sistema Operacional**: Windows 10+, macOS 10.15+, Ubuntu 18.04+
- **Python**: 3.8, 3.9, 3.10 ou 3.11 (testado)
- **Java**: OpenJDK 8 ou 11 (para PySpark)
- **RAM**: Mínimo 4GB (recomendado 8GB+)
- **Espaço em Disco**: 2GB livres

### **📦 Dependências Principais**
```txt
pandas>=2.1.0          # Análise de dados
numpy>=1.24.0           # Computação numérica  
matplotlib>=3.7.0       # Visualização básica
seaborn>=0.12.0         # Visualização estatística
pyspark>=3.5.0          # Big Data processing
scikit-learn>=1.3.0     # Machine Learning
jupyter>=1.0.0          # Notebooks interativos
```

### **⏱️ Tempo de Instalação**
- **Instalação básica**: 5-10 minutos
- **Configuração Java**: 2-5 minutos
- **Teste completo**: 3-5 minutos
- **Total**: 15-20 minutos

### **📊 Estrutura do Conteúdo**
- **5 Capítulos narrativos**: 60+ páginas
- **Exemplos práticos**: 20+ códigos testados
- **Casos reais**: Dados de Santa Catarina
- **Nível**: Iniciante → Intermediário
- **Duração estudo**: 8-12 horas

### **🎯 Compatibilidade Testada**
```bash
✅ Windows 10/11 + Python 3.9 + Java 8
✅ macOS Monterey + Python 3.10 + Java 11  
✅ Ubuntu 20.04 + Python 3.8 + Java 8
✅ Google Colab (online, sem instalação)
✅ Jupyter Lab + VSCode + PyCharm
```

---

## 🌟 **Depoimentos (Baseados no Projeto)**

*"Finalmente um repositório que explica Big Data através de histórias! Patrick torna o aprendizado muito mais envolvente."* - **Estudante de Ciência de Dados**

*"Os casos de Santa Catarina são perfeitos para entender como aplicar esses conceitos na nossa realidade brasileira."* - **Analista de Dados**

*"A abordagem de menos código e mais explicação foi fundamental para compreender os conceitos de verdade."* - **Gestor Público**

*"Patrick como personagem único dá consistência e facilita o acompanhamento de toda a jornada de aprendizado."* - **Professor de Tecnologia**

---

## 🤝 **Contribuindo para o Projeto**

Este repositório está em **constante evolução educativa**:

1. **Leia e dê feedback** sobre a clareza das explicações
2. **Sugira melhorias** na narrativa ou nos conceitos
3. **Compartilhe** como aplicou os conhecimentos em seus projetos
4. **Adapte** os casos para sua região e compartilhe os resultados
5. **Contribua** com novos casos práticos baseados em storytelling

### **Como Contribuir**
- Abra issues com sugestões de melhoria
- Proponha novos casos baseados em dados reais
- Sugira melhorias na consistência narrativa
- Compartilhe aplicações práticas dos conceitos

---

## 📞 **Sobre o Projeto**

- **Metodologia**: Storytelling educativo para ensino de Big Data
- **Protagonista**: Patrick - personagem consistente em todos os capítulos
- **Localização**: Casos baseados em Santa Catarina, aplicáveis universalmente
- **Objetivo**: Democratizar conhecimento em Big Data através de narrativas envolventes
- **Diferencial**: Foco em compreensão conceitual, não em código complexo

---

## 📄 **Licença e Uso**

Este conteúdo é disponibilizado para fins **educacionais e não comerciais**. 

### **Uso Permitido**
- Estudo pessoal e acadêmico
- Adaptação dos conceitos para projetos próprios
- Compartilhamento com fins educativos
- Referência em trabalhos acadêmicos (com citação)

**Desenvolvido com ❤️ em Santa Catarina para estudantes e profissionais de Big Data.**

---

### 🎯 **Estatísticas do Projeto**
- **15 Aulas completas** - Do básico ao avançado
- **5 Módulos especializados** - Fundamentos, Hadoop, Spark, Pandas, ML
- **40+ Questões de concurso** - Preparação profissional
- **5 Capítulos narrativos** - Storytelling com Patrick
- **20+ Exemplos práticos** - Códigos funcionais
- **100% Português** - Conteúdo nacional
- **Casos reais SC** - Aplicação prática local

**Última atualização: 29/09/2025** ✅
#   S i t e   r e b u i l d   0 9 / 2 5 / 2 0 2 5   1 2 : 2 1 : 5 8 
 
 #   D e p l o y   t r i g g e r   0 9 / 2 5 / 2 0 2 5   1 3 : 4 2 : 2 3 
 
 