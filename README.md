# Big Data em Python: Casos PrÃ¡ticos de Santa Catarina

*Um guia educacional de Big Data atravÃ©s de storytelling com casos reais de FlorianÃ³polis e regiÃ£o*

---

## ğŸ“– Sobre o Projeto

Este repositÃ³rio apresenta conceitos de **Big Data** e **Python** atravÃ©s de **narrativas educacionais** baseadas em casos reais de Santa Catarina, seguindo o protagonista **Patrick** em suas aventuras com dados.

### ğŸ¯ **Diferenciais**
- **Storytelling educativo**: Aprendizado atravÃ©s de narrativas envolventes
- **Casos reais de SC**: Ponte HercÃ­lio Luz, DETRAN-SC, turismo de Floripa
- **Menos cÃ³digo, mais explicaÃ§Ã£o**: Foco na compreensÃ£o conceitual
- **Contexto local**: Exemplos prÃ¡ticos da Grande FlorianÃ³polis
- **Protagonista Ãºnico**: Patrick como guia consistente em todos os capÃ­tulos

### ğŸ“š **Estrutura do Livro (5 CapÃ­tulos Narrativos)**

#### **Parte I: Fundamentos atravÃ©s de HistÃ³rias**
1. **O Despertar dos Dados** - Patrick descobre Big Data em FlorianÃ³polis âœ…
2. **IoT e Cidades Inteligentes** - Patrick explora sensores em SÃ£o JosÃ© âœ…
3. **AnÃ¡lise de Dados TurÃ­sticos** - Patrick desvenda padrÃµes do turismo âœ…

#### **Parte II: Tecnologias AvanÃ§adas**  
4. **Apache Spark em AÃ§Ã£o** - Patrick processa dados do DETRAN-SC âœ…
5. **Machine Learning Aplicado** - Patrick prevÃª preÃ§os imobiliÃ¡rios âœ…

### ğŸ­ **Metodologia Narrativa**
- **Patrick**: Protagonista Ãºnico e consistente em todos os capÃ­tulos
- **Contexto SC**: Todos os casos baseados em Santa Catarina
- **EducaÃ§Ã£o**: Foco em explicaÃ§Ãµes didÃ¡ticas, nÃ£o em cÃ³digo complexo
- **Aplicabilidade**: Conceitos aplicÃ¡veis a qualquer regiÃ£o do Brasil

---

## ğŸ“ **PÃºblico-Alvo**

- **Estudantes de tecnologia** e ciÃªncia de dados
- **Analistas de dados** iniciantes/intermediÃ¡rios
- **Gestores pÃºblicos** interessados em transformaÃ§Ã£o digital
- **Profissionais** que buscam aprender Big Data de forma didÃ¡tica
- **Qualquer pessoa** interessada em dados e storytelling educativo

---

## ğŸ› ï¸ **Tecnologias e Conceitos Abordados**

### **Linguagens e Frameworks**
- **Python**: Pandas, NumPy, Matplotlib, Seaborn
- **Big Data**: Apache Spark, PySpark, processamento distribuÃ­do
- **Machine Learning**: Scikit-learn, regressÃ£o, classificaÃ§Ã£o
- **Dados**: APIs, CSV, JSON, anÃ¡lise exploratÃ³ria
- **VisualizaÃ§Ã£o**: GrÃ¡ficos interpretativos e storytelling com dados

### **Conceitos Fundamentais**
- Volume, Velocidade, Variedade, Veracidade, Valor (5 V's)
- ETL/ELT e pipelines de dados
- IoT e sensores urbanos
- Smart cities e transformaÃ§Ã£o digital
- Feature engineering e modelagem preditiva

---

## ï¿½ **Palavras-Chave para Profissionais de Big Data e AnÃ¡lise de Dados**

*Skills essenciais que aparecem em vagas de emprego e sÃ£o abordadas neste livro*

### **Linguagens e Frameworks**
Python, PySpark, Apache Spark, SQL, Scala, R, Java, Hadoop, Hive, Apache Kafka, Apache Airflow, Apache Beam, Apache Flink, Databricks, Snowflake, dbt, Great Expectations, MLflow, Kubeflow, TensorFlow, PyTorch, Keras, XGBoost, LightGBM, Scikit-learn, Pandas, NumPy, Matplotlib, Seaborn, Plotly, Streamlit, Dash, FastAPI, Flask, Django

### **Cloud e Infraestrutura**
AWS, Azure, Google Cloud Platform (GCP), Amazon S3, Azure Data Lake, Google BigQuery, Amazon Redshift, Azure Synapse Analytics, Google Cloud Storage, EC2, Azure VM, Google Compute Engine, Lambda, Azure Functions, Google Cloud Functions, Docker, Kubernetes, Terraform, Apache Mesos, Yarn, Spark Cluster, Elasticsearch, MongoDB, Cassandra, Redis, PostgreSQL, MySQL, Oracle, SQL Server

### **Ferramentas de Dados e ETL**
Apache Nifi, Talend, Informatica, Pentaho, SSIS, Azure Data Factory, AWS Glue, Google Cloud Dataflow, Apache Sqoop, Apache Flume, Logstash, Fivetran, Stitch, Airbyte, Singer, Apache Superset, Tableau, Power BI, Looker, Grafana, Metabase, Apache Zeppelin, Jupyter Notebooks, Google Colab, Apache Parquet, Apache Avro, JSON, XML, CSV

### **Machine Learning e IA**
Deep Learning, Neural Networks, CNN, RNN, LSTM, GAN, Transformer, BERT, GPT, Computer Vision, Natural Language Processing (NLP), Time Series Analysis, Recommender Systems, Classification, Regression, Clustering, Dimensionality Reduction, Feature Engineering, Model Selection, Cross-validation, Hyperparameter Tuning, A/B Testing, MLOps, Model Deployment, Model Monitoring, AutoML, Ensemble Methods

### **Metodologias e Conceitos**
Data Engineering, Data Science, Data Analytics, Business Intelligence, ETL/ELT, Data Pipeline, Data Warehouse, Data Lake, Lakehouse, Data Mesh, Real-time Processing, Batch Processing, Stream Processing, Data Governance, Data Quality, Data Lineage, Data Catalog, Metadata Management, GDPR, Data Privacy, Agile, Scrum, DevOps, CI/CD, Git, Version Control

### **EspecializaÃ§Ã£o e Soft Skills**
Statistics, Mathematics, Linear Algebra, Probability, Hypothesis Testing, Statistical Modeling, Experimentation Design, Problem Solving, Critical Thinking, Communication, Data Storytelling, Data Visualization, Business Acumen, Domain Knowledge, Project Management, Teamwork, Leadership, Continuous Learning, Adaptability, Innovation

---

## ï¿½ğŸ“Š **Casos Reais Desenvolvidos**

### ğŸŒ‰ **CapÃ­tulo 1: Despertar dos Dados - Ponte HercÃ­lio Luz**
- Patrick descobre padrÃµes no trÃ¡fego da ponte
- AnÃ¡lise de 2,8 milhÃµes de veÃ­culos/ano
- IntroduÃ§Ã£o aos conceitos de Big Data
- Insights sobre mobilidade urbana em FlorianÃ³polis

### ğŸ™ï¸ **CapÃ­tulo 2: SÃ£o JosÃ© Conectado**  
- Patrick explora sistemas IoT de monitoramento urbano
- Sensores de qualidade do ar e trÃ¡fego
- Conceitos de smart cities aplicados
- TransformaÃ§Ã£o digital em cidades mÃ©dias

### ğŸ–ï¸ **CapÃ­tulo 3: Turismo de FlorianÃ³polis**
- Patrick analisa sazonalidade da ocupaÃ§Ã£o hoteleira
- PadrÃµes de demanda turÃ­stica na Ilha da Magia
- Dados reais de turismo de SC
- PrevisÃ£o e otimizaÃ§Ã£o para o setor

### ğŸš— **CapÃ­tulo 4: DETRAN Santa Catarina**
- Patrick processa 4,2 milhÃµes de veÃ­culos registrados
- IntroduÃ§Ã£o ao Apache Spark e processamento distribuÃ­do
- AnÃ¡lise de frota por municÃ­pio catarinense
- Escalabilidade para grandes volumes de dados

### ğŸ  **CapÃ­tulo 5: Mercado ImobiliÃ¡rio de Floripa**
- Patrick desenvolve Machine Learning para previsÃ£o de preÃ§os
- Fatores de valorizaÃ§Ã£o na Ilha da Magia
- Feature engineering com dados locais
- AplicaÃ§Ã£o prÃ¡tica de algoritmos de ML
- 80+ skills essenciais para o mercado de trabalho

---

## ğŸš€ **Guia PrÃ¡tico: InstalaÃ§Ã£o e Uso**

### **ğŸ“– Guias Completos DisponÃ­veis**

#### **ğŸ› ï¸ [INSTALACAO_COMPLETA.md](./INSTALACAO_COMPLETA.md)**
Tutorial detalhado passo-a-passo para configurar todo ambiente:
- InstalaÃ§Ã£o Python, Java, bibliotecas
- ConfiguraÃ§Ã£o ambiente virtual
- Teste automatizado de validaÃ§Ã£o
- SoluÃ§Ã£o de problemas comuns
- Scripts de verificaÃ§Ã£o completa

#### **ğŸ¤– [GUIA_INTELIGENCIA_ARTIFICIAL.md](./GUIA_INTELIGENCIA_ARTIFICIAL.md)**
Guia completo de IA aplicada a Big Data com casos prÃ¡ticos:
- Machine Learning com dados de SC
- Deep Learning para sÃ©ries temporais
- Processamento de Linguagem Natural (NLP)
- Algoritmos GenÃ©ticos para otimizaÃ§Ã£o
- Projetos avanÃ§ados e prÃ³ximos passos

### **ğŸ“‹ PrÃ©-requisitos**

**Sistema Operacional:**
- Windows 10/11, macOS 10.15+, ou Linux Ubuntu 18.04+

**Software Essencial:**
- **Python 3.8+** - [Download aqui](https://python.org/downloads)
- **Java 8 ou 11** (para PySpark) - [Download OpenJDK](https://adoptium.net/)
- **Git** - [Download aqui](https://git-scm.com/downloads)

**Verificar InstalaÃ§Ãµes:**
```bash
# Verificar Python
python --version
# Deve mostrar: Python 3.8.x ou superior

# Verificar Java  
java -version
# Deve mostrar: openjdk version "8" ou "11"

# Verificar Git
git --version
```

---

### **âš¡ InstalaÃ§Ã£o RÃ¡pida (5 minutos)**

#### **Passo 1: Clone o RepositÃ³rio**
```bash
# Abra o terminal/prompt de comando
git clone https://github.com/cordeirotelecom/topicos-bigdata-python.git
cd topicos-bigdata-python
```

#### **Passo 2: Crie um Ambiente Virtual (Recomendado)**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux  
python3 -m venv venv
source venv/bin/activate
```

#### **Passo 3: Instale as DependÃªncias**
```bash
# Instalar todas as bibliotecas necessÃ¡rias
pip install -r requirements.txt

# Verificar se PySpark foi instalado corretamente
python -c "import pyspark; print('PySpark OK!')"
```

#### **Passo 4: Teste a InstalaÃ§Ã£o**
```bash
# Iniciar Jupyter Lab
jupyter lab

# Ou Jupyter Notebook clÃ¡ssico
jupyter notebook
```

---

### **ğŸ“– Como Usar: Guia Passo a Passo**

#### **Para Iniciantes Completos**

**1. Comece pelo CapÃ­tulo 1**
```bash
# Navegue atÃ© a pasta do livro
cd livro/

# Abra o primeiro capÃ­tulo
# Windows: notepad capitulo01-despertar-dos-dados.md
# macOS: open capitulo01-despertar-dos-dados.md  
# Linux: gedit capitulo01-despertar-dos-dados.md
```

**2. Siga a Ordem dos CapÃ­tulos**
- ğŸ“– **CapÃ­tulo 1**: Conceitos bÃ¡sicos de Big Data
- ğŸ™ï¸ **CapÃ­tulo 2**: IoT e sensores urbanos
- ğŸ–ï¸ **CapÃ­tulo 3**: AnÃ¡lise de dados turÃ­sticos
- âš¡ **CapÃ­tulo 4**: Processamento distribuÃ­do com Spark
- ğŸ¤– **CapÃ­tulo 5**: Machine Learning aplicado

**3. Experimente os Conceitos**
```python
# Exemplo prÃ¡tico do CapÃ­tulo 1
import pandas as pd
import matplotlib.pyplot as plt

# Simular dados de trÃ¡fego da Ponte HercÃ­lio Luz
dados_ponte = {
    'hora': range(0, 24),
    'veiculos': [50, 30, 20, 25, 45, 120, 350, 500, 
                 400, 300, 250, 280, 320, 300, 350, 
                 400, 500, 600, 450, 300, 200, 150, 100, 70]
}

df = pd.DataFrame(dados_ponte)
plt.plot(df['hora'], df['veiculos'])
plt.title('TrÃ¡fego na Ponte HercÃ­lio Luz - 24h')
plt.xlabel('Hora do Dia')
plt.ylabel('NÃºmero de VeÃ­culos')
plt.show()
```

### **ğŸ’¡ Exemplos PrÃ¡ticos RÃ¡pidos**

#### **ğŸ™ï¸ AnÃ¡lise de IoT - Sensores em SÃ£o JosÃ©**
```python
import pandas as pd
import numpy as np

# Simular dados de sensores de qualidade do ar
np.random.seed(42)
horas = pd.date_range('2025-01-01', periods=168, freq='H')  # 1 semana

dados_iot = pd.DataFrame({
    'timestamp': horas,
    'pm25': np.random.normal(25, 8, 168),  # PM2.5 (Î¼g/mÂ³)
    'temperatura': 20 + 10 * np.sin(np.arange(168) * 2 * np.pi / 24) + np.random.normal(0, 2, 168),
    'umidade': 60 + 20 * np.sin(np.arange(168) * 2 * np.pi / 24 + np.pi/4) + np.random.normal(0, 5, 168)
})

# AnÃ¡lise rÃ¡pida
print("ğŸ“Š Qualidade do Ar - SÃ£o JosÃ© SC")
print(f"PM2.5 mÃ©dio: {dados_iot['pm25'].mean():.1f} Î¼g/mÂ³")
print(f"Temperatura mÃ©dia: {dados_iot['temperatura'].mean():.1f}Â°C")
print(f"Dias com qualidade ruim (PM2.5 > 35): {(dados_iot['pm25'] > 35).sum()}")

# VisualizaÃ§Ã£o
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.plot(dados_iot['pm25'])
plt.title('PM2.5')
plt.subplot(1, 3, 2)
plt.plot(dados_iot['temperatura'])
plt.title('Temperatura')
plt.subplot(1, 3, 3)
plt.plot(dados_iot['umidade'])
plt.title('Umidade')
plt.tight_layout()
plt.show()
```

#### **ğŸ–ï¸ Machine Learning - Turismo Floripa**
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Dados sintÃ©ticos de ocupaÃ§Ã£o hoteleira
np.random.seed(42)
dados_turismo = pd.DataFrame({
    'mes': np.random.randint(1, 13, 1000),
    'dia_semana': np.random.randint(1, 8, 1000),
    'temperatura': np.random.normal(25, 5, 1000),
    'chuva': np.random.choice([0, 1], 1000, p=[0.7, 0.3]),
    'feriado': np.random.choice([0, 1], 1000, p=[0.9, 0.1])
})

# Simular ocupaÃ§Ã£o baseada nas features
ocupacao = (
    50 +  # base
    dados_turismo['mes'].apply(lambda x: 30 if x in [12, 1, 2] else 0) +  # verÃ£o
    dados_turismo['dia_semana'].apply(lambda x: 20 if x in [6, 7] else 0) +  # fim de semana
    dados_turismo['temperatura'] * 0.5 +  # temperatura
    dados_turismo['feriado'] * 25 -  # feriados
    dados_turismo['chuva'] * 15 +  # chuva reduz ocupaÃ§Ã£o
    np.random.normal(0, 10, 1000)  # ruÃ­do
)
dados_turismo['ocupacao'] = np.clip(ocupacao, 0, 100)

# Treinar modelo
X = dados_turismo[['mes', 'dia_semana', 'temperatura', 'chuva', 'feriado']]
y = dados_turismo['ocupacao']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

modelo = RandomForestRegressor(n_estimators=100, random_state=42)
modelo.fit(X_train, y_train)

# PrediÃ§Ã£o para um fim de semana de verÃ£o
predicao = modelo.predict([[1, 7, 28, 0, 0]])  # Janeiro, domingo, 28Â°C, sem chuva, sem feriado
print(f"ğŸ¨ OcupaÃ§Ã£o prevista: {predicao[0]:.1f}%")

# ImportÃ¢ncia das variÃ¡veis
importancias = pd.DataFrame({
    'variavel': X.columns,
    'importancia': modelo.feature_importances_
}).sort_values('importancia', ascending=False)
print("\nğŸ“ˆ Fatores mais importantes:")
for _, row in importancias.iterrows():
    print(f"{row['variavel']}: {row['importancia']:.3f}")
```

#### **âš¡ Big Data com PySpark - DETRAN SC**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg

# Inicializar Spark
spark = SparkSession.builder \
    .appName("DETRAN-SC Analysis") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# Simular dados de veÃ­culos de SC
dados_veiculos = [
    ("FlorianÃ³polis", "Carro", 2023, 45000, "Gasoline"),
    ("SÃ£o JosÃ©", "Moto", 2023, 12000, "Gasoline"),
    ("PalhoÃ§a", "Carro", 2022, 18000, "Flex"),
    ("BiguaÃ§u", "Carro", 2023, 8000, "Flex"),
    ("FlorianÃ³polis", "Moto", 2022, 25000, "Gasoline"),
    ("SÃ£o JosÃ©", "Carro", 2023, 22000, "Electric"),
    ("Laguna", "Carro", 2021, 5000, "Gasoline"),
    ("Joinville", "Carro", 2023, 35000, "Flex"),
    ("Blumenau", "Moto", 2023, 15000, "Gasoline"),
    ("ItajaÃ­", "Carro", 2022, 12000, "Flex")
]

colunas = ["cidade", "tipo", "ano", "quantidade", "combustivel"]
df_veiculos = spark.createDataFrame(dados_veiculos, colunas)

print("ğŸš— AnÃ¡lise de Frota - DETRAN SC")
print("=" * 40)

# AnÃ¡lise por cidade
print("\nğŸ“ VeÃ­culos por cidade:")
df_veiculos.groupBy("cidade") \
    .agg(count("*").alias("registros"), 
         sum("quantidade").alias("total_veiculos")) \
    .orderBy(col("total_veiculos").desc()) \
    .show()

# AnÃ¡lise por tipo de combustÃ­vel
print("â›½ DistribuiÃ§Ã£o por combustÃ­vel:")
df_veiculos.groupBy("combustivel") \
    .agg(sum("quantidade").alias("total")) \
    .orderBy(col("total").desc()) \
    .show()

# TendÃªncia de eletrificaÃ§Ã£o
print("ğŸ”‹ VeÃ­culos elÃ©tricos:")
df_veiculos.filter(col("combustivel") == "Electric").show()

spark.stop()
```

---

#### **Para UsuÃ¡rios IntermediÃ¡rios**

**1. Explore os Dados PrÃ¡ticos**
```python
# Exemplo do CapÃ­tulo 4: Spark com dados do DETRAN-SC
from pyspark.sql import SparkSession

# Inicializar Spark
spark = SparkSession.builder \
    .appName("DETRAN-SC Analysis") \
    .getOrCreate()

# Simular dados de veÃ­culos de SC
dados_veiculos = [
    ("FlorianÃ³polis", "Carro", 2020, 15000),
    ("SÃ£o JosÃ©", "Moto", 2021, 8000),
    ("PalhoÃ§a", "Carro", 2022, 12000),
    ("BiguaÃ§u", "Moto", 2020, 3000)
]

colunas = ["cidade", "tipo", "ano", "quantidade"]
df_spark = spark.createDataFrame(dados_veiculos, colunas)

# AnÃ¡lise por cidade
df_spark.groupBy("cidade").sum("quantidade").show()
```

**2. Aplique Machine Learning**
```python
# Exemplo do CapÃ­tulo 5: ML para preÃ§os imobiliÃ¡rios
from sklearn.linear_model import LinearRegression
import numpy as np

# Dados simulados de imÃ³veis em Floripa
area = np.array([60, 80, 120, 150, 200]).reshape(-1, 1)
preco = np.array([400000, 550000, 750000, 900000, 1200000])

# Treinar modelo
modelo = LinearRegression()
modelo.fit(area, preco)

# Predizer preÃ§o para apartamento de 100mÂ²
preco_100m2 = modelo.predict([[100]])
print(f"PreÃ§o estimado para 100mÂ²: R$ {preco_100m2[0]:,.0f}")
```

---

### **ğŸ”§ SoluÃ§Ã£o de Problemas Comuns**

#### **Erro: Java nÃ£o encontrado**
```bash
# Verificar JAVA_HOME
echo $JAVA_HOME  # Linux/macOS
echo %JAVA_HOME%  # Windows

# Se vazio, definir manualmente:
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64  # Linux
export JAVA_HOME=/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home  # macOS
```

#### **Erro: PySpark nÃ£o inicializa**
```python
# ConfiguraÃ§Ã£o manual do PySpark
import os
os.environ['JAVA_HOME'] = '/caminho/para/java'
os.environ['SPARK_HOME'] = '/caminho/para/spark'

import findspark
findspark.init()

import pyspark
```

#### **Erro: MÃ³dulo nÃ£o encontrado**
```bash
# Reinstalar dependÃªncias
pip install --upgrade -r requirements.txt

# Verificar se estÃ¡ no ambiente virtual correto
which python  # Linux/macOS
where python   # Windows
```

---

### **ğŸ“± Testando Sua InstalaÃ§Ã£o - Checklist Completo**

**âœ… Teste 1: Python e Pandas**
```python
import pandas as pd
print("âœ… Pandas funcionando!")
df = pd.DataFrame({'nome': ['Patrick'], 'cidade': ['FlorianÃ³polis']})
print(df)
```

**âœ… Teste 2: VisualizaÃ§Ã£o**
```python
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(0, 10, 100)
y = np.sin(x)
plt.plot(x, y)
plt.title("âœ… Matplotlib funcionando!")
plt.show()
```

**âœ… Teste 3: Big Data (PySpark)**
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Teste").getOrCreate()
df = spark.createDataFrame([("Patrick", "FlorianÃ³polis")], ["nome", "cidade"])
df.show()
print("âœ… PySpark funcionando!")
spark.stop()
```

**âœ… Teste 4: Machine Learning**
```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression

X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, random_state=42)
model = LogisticRegression()
model.fit(X, y)
print("âœ… Scikit-learn funcionando!")
```

---

### **ğŸ“š PrÃ³ximos Passos ApÃ³s InstalaÃ§Ã£o**

1. **ğŸ“– Leia o CapÃ­tulo 1** - Entenda a jornada de Patrick
2. **ğŸ’» Abra Jupyter Lab** - Ambiente interativo para experimentar
3. **ğŸ” Explore os dados** - Cada capÃ­tulo tem exemplos prÃ¡ticos
4. **ğŸ› ï¸ Adapte para sua regiÃ£o** - Use os conceitos em seus projetos
5. **ğŸ¤ Compartilhe resultados** - Contribua com a comunidade

---

## ğŸ“ˆ **EspecificaÃ§Ãµes TÃ©cnicas do Projeto**

### **ğŸ“‹ Requisitos de Sistema**
- **Sistema Operacional**: Windows 10+, macOS 10.15+, Ubuntu 18.04+
- **Python**: 3.8, 3.9, 3.10 ou 3.11 (testado)
- **Java**: OpenJDK 8 ou 11 (para PySpark)
- **RAM**: MÃ­nimo 4GB (recomendado 8GB+)
- **EspaÃ§o em Disco**: 2GB livres

### **ğŸ“¦ DependÃªncias Principais**
```txt
pandas>=2.1.0          # AnÃ¡lise de dados
numpy>=1.24.0           # ComputaÃ§Ã£o numÃ©rica  
matplotlib>=3.7.0       # VisualizaÃ§Ã£o bÃ¡sica
seaborn>=0.12.0         # VisualizaÃ§Ã£o estatÃ­stica
pyspark>=3.5.0          # Big Data processing
scikit-learn>=1.3.0     # Machine Learning
jupyter>=1.0.0          # Notebooks interativos
```

### **â±ï¸ Tempo de InstalaÃ§Ã£o**
- **InstalaÃ§Ã£o bÃ¡sica**: 5-10 minutos
- **ConfiguraÃ§Ã£o Java**: 2-5 minutos
- **Teste completo**: 3-5 minutos
- **Total**: 15-20 minutos

### **ğŸ“Š Estrutura do ConteÃºdo**
- **5 CapÃ­tulos narrativos**: 60+ pÃ¡ginas
- **Exemplos prÃ¡ticos**: 20+ cÃ³digos testados
- **Casos reais**: Dados de Santa Catarina
- **NÃ­vel**: Iniciante â†’ IntermediÃ¡rio
- **DuraÃ§Ã£o estudo**: 8-12 horas

### **ğŸ¯ Compatibilidade Testada**
```bash
âœ… Windows 10/11 + Python 3.9 + Java 8
âœ… macOS Monterey + Python 3.10 + Java 11  
âœ… Ubuntu 20.04 + Python 3.8 + Java 8
âœ… Google Colab (online, sem instalaÃ§Ã£o)
âœ… Jupyter Lab + VSCode + PyCharm
```

---

## ğŸŒŸ **Depoimentos (Baseados no Projeto)**

*"Finalmente um repositÃ³rio que explica Big Data atravÃ©s de histÃ³rias! Patrick torna o aprendizado muito mais envolvente."* - **Estudante de CiÃªncia de Dados**

*"Os casos de Santa Catarina sÃ£o perfeitos para entender como aplicar esses conceitos na nossa realidade brasileira."* - **Analista de Dados**

*"A abordagem de menos cÃ³digo e mais explicaÃ§Ã£o foi fundamental para compreender os conceitos de verdade."* - **Gestor PÃºblico**

*"Patrick como personagem Ãºnico dÃ¡ consistÃªncia e facilita o acompanhamento de toda a jornada de aprendizado."* - **Professor de Tecnologia**

---

## ğŸ¤ **Contribuindo para o Projeto**

Este repositÃ³rio estÃ¡ em **constante evoluÃ§Ã£o educativa**:

1. **Leia e dÃª feedback** sobre a clareza das explicaÃ§Ãµes
2. **Sugira melhorias** na narrativa ou nos conceitos
3. **Compartilhe** como aplicou os conhecimentos em seus projetos
4. **Adapte** os casos para sua regiÃ£o e compartilhe os resultados
5. **Contribua** com novos casos prÃ¡ticos baseados em storytelling

### **Como Contribuir**
- Abra issues com sugestÃµes de melhoria
- Proponha novos casos baseados em dados reais
- Sugira melhorias na consistÃªncia narrativa
- Compartilhe aplicaÃ§Ãµes prÃ¡ticas dos conceitos

---

## ğŸ“ **Sobre o Projeto**

- **Metodologia**: Storytelling educativo para ensino de Big Data
- **Protagonista**: Patrick - personagem consistente em todos os capÃ­tulos
- **LocalizaÃ§Ã£o**: Casos baseados em Santa Catarina, aplicÃ¡veis universalmente
- **Objetivo**: Democratizar conhecimento em Big Data atravÃ©s de narrativas envolventes
- **Diferencial**: Foco em compreensÃ£o conceitual, nÃ£o em cÃ³digo complexo

---

## ğŸ“„ **LicenÃ§a e Uso**

Este conteÃºdo Ã© disponibilizado para fins **educacionais e nÃ£o comerciais**. 

### **Uso Permitido**
- Estudo pessoal e acadÃªmico
- AdaptaÃ§Ã£o dos conceitos para projetos prÃ³prios
- Compartilhamento com fins educativos
- ReferÃªncia em trabalhos acadÃªmicos (com citaÃ§Ã£o)

**Desenvolvido com â¤ï¸ em Santa Catarina para estudantes e profissionais que buscam aprender Big Data atravÃ©s de storytelling educativo.**
