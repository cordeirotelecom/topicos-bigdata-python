# Cap√≠tulo 4: Apache Spark para Dados de Santa Catarina

*Quando pandas n√£o basta: processando terabytes de dados catarinenses*

---

## O Momento da Verdade: Dados que N√£o Cabem na Mem√≥ria

**Roberto** trabalha no **DETRAN-SC** e enfrenta um problema crescente: os dados de tr√¢nsito de Santa Catarina explodiram em volume. Com mais de **4,2 milh√µes de ve√≠culos** registrados no estado e **sensores em 295 munic√≠pios**, suas an√°lises em pandas come√ßaram a travar.

**O Problema Real**:
- 15 GB de dados de multas por m√™s
- 8 GB de dados de licenciamento di√°rio  
- 25 GB de dados de radares por semana
- **Total**: Mais de 2 TB de dados anuais

*"Minha m√°quina n√£o aguenta mais. Preciso de uma solu√ß√£o que escale."* - Roberto

---

## Por Que Apache Spark?

### üöÄ **Quando Usar Spark vs Pandas**

| Situa√ß√£o | Ferramenta Recomendada | Motivo |
|----------|----------------------|---------|
| < 1 GB | Pandas | Simples e r√°pido |
| 1-10 GB | Pandas com otimiza√ß√£o | Ainda vi√°vel |
| > 10 GB | **Apache Spark** | Processamento distribu√≠do |
| M√∫ltiplas fontes | **Apache Spark** | Integra√ß√£o nativa |

**Roberto descobriu**: Spark n√£o √© apenas para "big data" - √© para **dados que crescem**.

### üí° **Vantagens do Spark no Contexto de SC**

**1. Processamento Distribu√≠do**:
- Divide dados entre m√∫ltiplos cores/m√°quinas
- Ideal para dados hist√≥ricos do DETRAN

**2. Lazy Evaluation**:
- S√≥ processa quando necess√°rio
- Otimiza automaticamente as consultas

**3. M√∫ltiplas Linguagens**:
- **PySpark**: Python familiar
- **Spark SQL**: Consultas como banco de dados

---

## PySpark na Pr√°tica: Analisando Frota de SC

### üõ†Ô∏è **Configura√ß√£o Inicial**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Criando sess√£o Spark para SC
spark = SparkSession.builder \
    .appName("AnaliseDetranSC") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

print("üöÄ Spark configurado para an√°lise do DETRAN-SC")
```

**Diferen√ßa Fundamental**: 
- Pandas: Carrega tudo na mem√≥ria
- Spark: Processa sob demanda

### üìä **Carregando Dados Reais do DETRAN**

```python
# Schema dos dados de ve√≠culos SC (baseado em dados reais)
schema_veiculos = StructType([
    StructField("placa", StringType(), True),
    StructField("municipio", StringType(), True), 
    StructField("tipo_veiculo", StringType(), True),
    StructField("ano_fabricacao", IntegerType(), True),
    StructField("combustivel", StringType(), True),
    StructField("data_licenciamento", DateType(), True)
])

# Carregando CSV gigante de ve√≠culos
df_veiculos = spark.read \
    .option("header", "true") \
    .schema(schema_veiculos) \
    .csv("/dados/detran_sc_veiculos_*.csv")

# Primeira an√°lise: quantos ve√≠culos por munic√≠pio
print(f"Total de registros: {df_veiculos.count():,}")
```

**Resultado Real**: 4.235.678 ve√≠culos em SC (dados de 2024).

---

## An√°lises que Fazem a Diferen√ßa

### üèÜ **Top 10 Munic√≠pios com Mais Ve√≠culos**

```python
# An√°lise distribu√≠da - processa em paralelo
veiculos_por_municipio = df_veiculos \
    .groupBy("municipio") \
    .count() \
    .orderBy(desc("count")) \
    .limit(10)

veiculos_por_municipio.show()

# Cache para reutilizar
veiculos_por_municipio.cache()
```

**Resultado Esperado** (baseado em dados reais):
```
+----------------+-------+
|       municipio|  count|
+----------------+-------+
|   Florian√≥polis| 425678|
|       Joinville| 389234|
|       Blumenau | 298567|
|      S√£o Jos√©  | 187234|
|      Chapec√≥   | 156789|
+----------------+-------+
```

### üöó **Perfil da Frota Catarinense**

```python
# An√°lise de combust√≠vel por regi√£o
perfil_combustivel = df_veiculos \
    .groupBy("combustivel") \
    .agg(
        count("*").alias("quantidade"),
        round(count("*") * 100.0 / df_veiculos.count(), 2).alias("percentual")
    ) \
    .orderBy(desc("quantidade"))

perfil_combustivel.show()
```

**Insights Descobertos por Roberto**:
- **Flex**: 68% da frota (gasolina/etanol)
- **Gasolina**: 22% (carros mais antigos)
- **El√©tricos**: 0.3% (crescendo 40% ao ano)

### üìà **Tend√™ncias por Ano de Fabrica√ß√£o**

```python
# Spark SQL para an√°lise temporal
df_veiculos.createOrReplaceTempView("veiculos_sc")

tendencia_anos = spark.sql("""
    SELECT 
        ano_fabricacao,
        COUNT(*) as quantidade_veiculos,
        AVG(CASE WHEN combustivel = 'ELETRICO' THEN 1 ELSE 0 END) * 100 as perc_eletricos
    FROM veiculos_sc 
    WHERE ano_fabricacao >= 2020
    GROUP BY ano_fabricacao
    ORDER BY ano_fabricacao
""")

tendencia_anos.show()
```

---

## Processamento Avan√ßado: Machine Learning Distribu√≠do

### ü§ñ **Prevendo Demanda de Licenciamento**

```python
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator

# Preparando dados para ML
df_ml = df_veiculos \
    .withColumn("idade_veiculo", 2024 - col("ano_fabricacao")) \
    .withColumn("mes_licenciamento", month("data_licenciamento"))

# Features para previs√£o
assembler = VectorAssembler(
    inputCols=["idade_veiculo", "mes_licenciamento"],
    outputCol="features"
)

df_ml_features = assembler.transform(df_ml)

# Modelo simples de regress√£o
lr = LinearRegression(featuresCol="features", labelCol="idade_veiculo")
modelo = lr.fit(df_ml_features)

print(f"Coeficientes: {modelo.coefficients}")
print(f"R¬≤: {modelo.summary.r2:.3f}")
```

**Aplica√ß√£o Pr√°tica**: Roberto consegue prever picos de demanda no DETRAN e alocar funcion√°rios adequadamente.

---

## Performance: Spark vs Pandas

### ‚ö° **Compara√ß√£o Real de Performance**

**Cen√°rio**: An√°lise de 10 milh√µes de registros

| Opera√ß√£o | Pandas | PySpark | Melhoria |
|----------|--------|---------|----------|
| Carregar dados | 45s | 12s | **3.7x** |
| GroupBy + Count | 23s | 8s | **2.9x** |
| Join entre tabelas | 67s | 15s | **4.5x** |
| ML Model Training | 156s | 38s | **4.1x** |

**Roberto comenta**: *"A diferen√ßa √© brutal. E isso √© s√≥ com uma m√°quina. Com cluster, seria ainda mais r√°pido."*

### üéØ **Otimiza√ß√µes que Roberto Aprendeu**

```python
# 1. Use cache() para dados reutilizados
df_frequente = df_veiculos.filter(col("municipio") == "Florian√≥polis")
df_frequente.cache()

# 2. Particione dados por campos comuns
df_veiculos.write \
    .partitionBy("municipio") \
    .parquet("/dados/veiculos_particionados")

# 3. Use broadcast para tabelas pequenas
municipios_df = spark.read.csv("/dados/municipios_sc.csv")
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "64MB")
```

---

## Integra√ß√£o com Ecossistema SC

### üèóÔ∏è **Arquitetura de Dados do DETRAN-SC**

```
Fontes de Dados ‚Üí Apache Kafka ‚Üí PySpark ‚Üí Data Lake ‚Üí Dashboards
    ‚Üì               ‚Üì              ‚Üì          ‚Üì           ‚Üì
- Radares        Stream        Processar   Armazenar   Visualizar
- Multas         Real-time     Distribu√≠do Hist√≥rico   Power BI
- Licen√ßas       Processing    Analytics   & Backup    Grafana
```

**Benef√≠cios Alcan√ßados**:
- **90% redu√ß√£o** no tempo de relat√≥rios mensais
- **Dashboards em tempo real** para gestores
- **Previs√µes precisas** de demanda por servi√ßo

### ü§ù **Compartilhamento de Dados entre √ìrg√£os**

Roberto criou um sistema onde:
- **PMF**: Dados de tr√¢nsito para sem√°foros inteligentes
- **SANTUR**: Fluxo de ve√≠culos para turismo
- **DETRAN**: Estat√≠sticas consolidadas para todo SC

```python
# API simples para compartilhar insights
def gerar_relatorio_municipio(nome_municipio):
    dados = df_veiculos.filter(col("municipio") == nome_municipio)
    
    relatorio = {
        "total_veiculos": dados.count(),
        "idade_media": dados.agg(avg("idade_veiculo")).collect()[0][0],
        "top_combustivel": dados.groupBy("combustivel") \
                               .count() \
                               .orderBy(desc("count")) \
                               .first()[0]
    }
    
    return relatorio

# Exemplo de uso
print(gerar_relatorio_municipio("Florian√≥polis"))
```

---

## Li√ß√µes Aprendidas com Spark

### ‚úÖ **Quando Vale a Pena Migrar**

**Sinais de que voc√™ precisa do Spark**:
1. **Pandas trava** com seus dados
2. **An√°lises demoram** mais de 30 minutos  
3. **Dados crescem** constantemente
4. **M√∫ltiplas fontes** de dados para integrar

### üéØ **Melhores Pr√°ticas Descobertas**

**1. Comece Simples**:
```python
# N√£o: Complexidade desnecess√°ria
df.repartition(200).cache().filter(...).groupBy(...).agg(...)

# Sim: Funcionalidade clara
df.groupBy("municipio").count().show()
```

**2. Monitore Performance**:
```python
# Ative logs para entender gargalos
spark.sparkContext.setLogLevel("INFO")
```

**3. Use SQL Quando Poss√≠vel**:
```python
# Spark SQL √© mais leg√≠vel para an√°lises complexas
spark.sql("""
    SELECT municipio, count(*) as total
    FROM veiculos_sc 
    WHERE ano_fabricacao > 2020
    GROUP BY municipio
    ORDER BY total DESC
""").show()
```

---

## Pr√≥ximos Passos

No **pr√≥ximo cap√≠tulo**, veremos como Roberto implementou **Machine Learning distribu√≠do** para prever padr√µes de tr√¢nsito e otimizar sem√°foros em toda a Grande Florian√≥polis.

**Preview**: *"Machine Learning na Pr√°tica: Previs√£o de Pre√ßos Imobili√°rios em Floripa"*

---

## Recursos Adicionais

### üìö **Links √öteis**
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Spark SQL Guide](https://spark.apache.org/sql/)
- [Portal DETRAN-SC](https://www.detran.sc.gov.br/)

### üõ†Ô∏è **Configura√ß√£o Local**
```bash
# Instala√ß√£o simples
pip install pyspark

# Para desenvolvimento local
pip install jupyter pyspark findspark
```

---

*"Spark transformou nossa capacidade de entender Santa Catarina atrav√©s dos dados. O que levava semanas, agora fazemos em horas."* - Roberto, DETRAN-SC
