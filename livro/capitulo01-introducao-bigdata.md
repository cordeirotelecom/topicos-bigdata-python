# CAP√çTULO 1: INTRODU√á√ÉO AO BIG DATA

## 1.1 O Que √© Big Data?

Big Data refere-se a conjuntos de dados t√£o grandes, complexos ou de crescimento t√£o r√°pido que as ferramentas tradicionais de processamento de dados s√£o inadequadas para lidar com eles de forma eficaz.

### Os 5 V's do Big Data

#### 1. Volume üìä
- **Defini√ß√£o**: Quantidade massiva de dados gerados continuamente
- **Escala**: Terabytes, Petabytes, Exabytes
- **Exemplo**: O Facebook gera mais de 4 petabytes de dados diariamente

```python
# Exemplo: Simula√ß√£o de crescimento de volume de dados
import numpy as np
import matplotlib.pyplot as plt

anos = np.array([2010, 2015, 2020, 2025])
dados_globais_zb = np.array([2, 15, 64, 175])  # Zettabytes

plt.figure(figsize=(10, 6))
plt.plot(anos, dados_globais_zb, 'bo-', linewidth=2, markersize=8)
plt.title('Crescimento do Volume Global de Dados', fontsize=14)
plt.xlabel('Ano')
plt.ylabel('Volume (Zettabytes)')
plt.grid(True, alpha=0.3)
plt.show()

print(f"Crescimento projetado: {dados_globais_zb[-1]/dados_globais_zb[0]:.1f}x em 15 anos")
```

#### 2. Velocidade ‚ö°
- **Defini√ß√£o**: Rapidez com que os dados s√£o gerados e processados
- **Tipos**: Batch, Near Real-time, Real-time, Streaming
- **Exemplo**: Transa√ß√µes financeiras (milh√µes por segundo)

```python
import time
from datetime import datetime

class DataVelocityDemo:
    def __init__(self):
        self.data_buffer = []
    
    def simulate_high_velocity_data(self, duration_seconds=10):
        """Simula gera√ß√£o de dados em alta velocidade"""
        start_time = time.time()
        count = 0
        
        while time.time() - start_time < duration_seconds:
            # Simula dados de sensor IoT
            data_point = {
                'timestamp': datetime.now().isoformat(),
                'sensor_id': f'sensor_{count % 1000}',
                'temperature': 20 + np.random.normal(0, 5),
                'humidity': 60 + np.random.normal(0, 10),
                'pressure': 1013 + np.random.normal(0, 20)
            }
            self.data_buffer.append(data_point)
            count += 1
            time.sleep(0.001)  # 1000 registros/segundo
        
        return count, len(self.data_buffer)

# Demonstra√ß√£o
demo = DataVelocityDemo()
records, buffer_size = demo.simulate_high_velocity_data(5)
print(f"Gerados {records} registros em 5 segundos")
print(f"Taxa: {records/5:.0f} registros/segundo")
```

#### 3. Variedade üé≠
- **Estruturados**: Tabelas, bancos relacionais
- **Semi-estruturados**: JSON, XML, logs
- **N√£o-estruturados**: Texto, imagens, v√≠deos, √°udio

```python
import json
import pandas as pd
from datetime import datetime

# Exemplo de dados com diferentes estruturas
def demonstrate_data_variety():
    # 1. Dados Estruturados (CSV/DataFrame)
    structured_data = pd.DataFrame({
        'id': [1, 2, 3, 4, 5],
        'nome': ['Jo√£o', 'Maria', 'Pedro', 'Ana', 'Carlos'],
        'idade': [25, 30, 35, 28, 42],
        'salario': [5000, 7500, 6200, 5800, 9200]
    })
    
    # 2. Dados Semi-estruturados (JSON)
    semi_structured_data = [
        {
            "usuario": "joao123",
            "acao": "login",
            "timestamp": "2025-08-25T10:30:00Z",
            "metadata": {
                "ip": "192.168.1.100",
                "dispositivo": "mobile",
                "localizacao": {"lat": -27.5954, "lon": -48.5480}
            }
        },
        {
            "usuario": "maria456", 
            "acao": "compra",
            "timestamp": "2025-08-25T11:15:30Z",
            "produto": {
                "id": "ABC123",
                "categoria": "eletr√¥nicos",
                "preco": 1299.99
            }
        }
    ]
    
    # 3. Dados N√£o-estruturados (Texto)
    unstructured_data = [
        "Cliente muito satisfeito com o produto. Recomenda para amigos!",
        "Entrega atrasou 3 dias. Produto chegou com defeito.",
        "Excelente qualidade, vale cada centavo gasto. Voltarei a comprar.",
        "Atendimento p√©ssimo, n√£o respondem no chat de suporte."
    ]
    
    return structured_data, semi_structured_data, unstructured_data

# Demonstra√ß√£o
struct, semi_struct, unstruct = demonstrate_data_variety()
print("=== DADOS ESTRUTURADOS ===")
print(struct.head())
print("\n=== DADOS SEMI-ESTRUTURADOS ===")
print(json.dumps(semi_struct[0], indent=2, ensure_ascii=False))
print("\n=== DADOS N√ÉO-ESTRUTURADOS ===")
print(unstruct[0])
```

#### 4. Veracidade ‚úÖ
- **Defini√ß√£o**: Qualidade e confiabilidade dos dados
- **Desafios**: Dados incompletos, incorretos, inconsistentes
- **Import√¢ncia**: "Garbage in, garbage out"

```python
import pandas as pd
import numpy as np

def data_quality_analysis(df):
    """An√°lise de qualidade dos dados"""
    quality_report = {
        'total_registros': len(df),
        'valores_nulos': df.isnull().sum().to_dict(),
        'duplicatas': df.duplicated().sum(),
        'completude': ((df.notna().sum() / len(df)) * 100).to_dict()
    }
    
    return quality_report

# Exemplo com dataset com problemas de qualidade
problematic_data = pd.DataFrame({
    'id': [1, 2, 3, 4, 5, 5],  # ID duplicado
    'nome': ['Jo√£o', 'Maria', None, 'Ana', 'Carlos', 'Carlos'],  # Valor nulo
    'idade': [25, -5, 35, 150, 42, 42],  # Idades imposs√≠veis
    'email': ['joao@email.com', 'maria@', 'pedro@email.com', 
              'ana@email.com', None, 'carlos@email.com']  # Email inv√°lido e nulo
})

quality_report = data_quality_analysis(problematic_data)
print("=== RELAT√ìRIO DE QUALIDADE ===")
for key, value in quality_report.items():
    print(f"{key}: {value}")

# Fun√ß√£o para limpeza de dados
def clean_data(df):
    """Aplicar regras de limpeza"""
    df_clean = df.copy()
    
    # Remover duplicatas
    df_clean = df_clean.drop_duplicates()
    
    # Tratar idades imposs√≠veis
    df_clean = df_clean[(df_clean['idade'] >= 0) & (df_clean['idade'] <= 120)]
    
    # Remover registros com emails inv√°lidos
    df_clean = df_clean[df_clean['email'].str.contains('@', na=False)]
    
    return df_clean

clean_df = clean_data(problematic_data)
print(f"\nRegistros antes da limpeza: {len(problematic_data)}")
print(f"Registros ap√≥s limpeza: {len(clean_df)}")
```

#### 5. Valor üíé
- **Defini√ß√£o**: Capacidade de extrair insights acion√°veis
- **Objetivo**: Transformar dados em conhecimento e vantagem competitiva
- **ROI**: Retorno sobre investimento em Big Data

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def extract_business_value(sales_data):
    """Extrai valor de neg√≥cio dos dados de vendas"""
    
    # An√°lise de padr√µes sazonais
    sales_data['data'] = pd.to_datetime(sales_data['data'])
    sales_data['mes'] = sales_data['data'].dt.month
    sales_data['dia_semana'] = sales_data['data'].dt.day_name()
    
    # Insights valiosos
    insights = {
        'vendas_por_mes': sales_data.groupby('mes')['valor'].sum(),
        'vendas_por_dia_semana': sales_data.groupby('dia_semana')['valor'].sum(),
        'produto_mais_vendido': sales_data.groupby('produto')['quantidade'].sum().idxmax(),
        'receita_total': sales_data['valor'].sum(),
        'ticket_medio': sales_data['valor'].mean()
    }
    
    return insights

# Dados simulados de vendas
np.random.seed(42)
n_records = 10000

sales_data = pd.DataFrame({
    'data': pd.date_range('2024-01-01', periods=n_records, freq='H'),
    'produto': np.random.choice(['Produto A', 'Produto B', 'Produto C'], n_records),
    'quantidade': np.random.randint(1, 10, n_records),
    'preco_unitario': np.random.uniform(10, 100, n_records)
})
sales_data['valor'] = sales_data['quantidade'] * sales_data['preco_unitario']

business_insights = extract_business_value(sales_data)

print("=== INSIGHTS DE NEG√ìCIO ===")
print(f"Receita Total: R$ {business_insights['receita_total']:,.2f}")
print(f"Ticket M√©dio: R$ {business_insights['ticket_medio']:,.2f}")
print(f"Produto Mais Vendido: {business_insights['produto_mais_vendido']}")

# Visualiza√ß√£o dos insights
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Vendas por m√™s
business_insights['vendas_por_mes'].plot(kind='bar', ax=axes[0])
axes[0].set_title('Vendas por M√™s')
axes[0].set_ylabel('Valor (R$)')

# Vendas por dia da semana
day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
vendas_dia = business_insights['vendas_por_dia_semana'].reindex(day_order)
vendas_dia.plot(kind='bar', ax=axes[1])
axes[1].set_title('Vendas por Dia da Semana')
axes[1].set_ylabel('Valor (R$)')

plt.tight_layout()
plt.show()
```

## 1.2 Evolu√ß√£o Hist√≥rica do Big Data

### Timeline do Big Data

```python
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime

# Timeline hist√≥rico
timeline_data = [
    (datetime(1944, 1, 1), "Termo 'Big Data' usado pela primeira vez", "Conceitual"),
    (datetime(1997, 1, 1), "Termo moderno 'Big Data' cunhado", "Acad√™mico"),
    (datetime(2003, 1, 1), "Google File System (GFS)", "Tecnol√≥gico"),
    (datetime(2004, 1, 1), "Google MapReduce", "Framework"),
    (datetime(2006, 1, 1), "Hadoop (Apache)", "Open Source"),
    (datetime(2009, 1, 1), "NoSQL ganha tra√ß√£o", "Banco de Dados"),
    (datetime(2010, 1, 1), "Apache Spark", "Performance"),
    (datetime(2011, 1, 1), "Apache Kafka", "Streaming"),
    (datetime(2014, 1, 1), "Apache Flink", "Real-time"),
    (datetime(2020, 1, 1), "Edge Computing + AI", "Moderniza√ß√£o"),
    (datetime(2025, 1, 1), "Quantum + Big Data", "Futuro")
]

# Visualiza√ß√£o da timeline
fig, ax = plt.subplots(figsize=(15, 8))

dates = [item[0] for item in timeline_data]
events = [item[1] for item in timeline_data]
categories = [item[2] for item in timeline_data]

# Cores por categoria
color_map = {
    'Conceitual': 'red', 'Acad√™mico': 'blue', 'Tecnol√≥gico': 'green',
    'Framework': 'orange', 'Open Source': 'purple', 'Banco de Dados': 'brown',
    'Performance': 'pink', 'Streaming': 'gray', 'Real-time': 'olive',
    'Moderniza√ß√£o': 'cyan', 'Futuro': 'magenta'
}

colors = [color_map[cat] for cat in categories]

ax.scatter(dates, range(len(dates)), c=colors, s=100, alpha=0.7)

for i, (date, event, category) in enumerate(timeline_data):
    ax.annotate(f"{event}\n({category})", 
                (date, i), 
                xytext=(10, 0), 
                textcoords='offset points',
                fontsize=9,
                bbox=dict(boxstyle='round,pad=0.3', facecolor=color_map[category], alpha=0.3))

ax.set_xlabel('Ano')
ax.set_ylabel('Marcos Hist√≥ricos')
ax.set_title('Evolu√ß√£o Hist√≥rica do Big Data', fontsize=16)
ax.grid(True, alpha=0.3)

# Formata√ß√£o do eixo X
ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
ax.xaxis.set_major_locator(mdates.YearLocator(5))

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

## 1.3 Caracter√≠sticas dos Sistemas Big Data

### Escalabilidade

```python
class ScalabilityDemo:
    """Demonstra√ß√£o de conceitos de escalabilidade"""
    
    @staticmethod
    def horizontal_vs_vertical_scaling():
        """Compare escalabilidade horizontal vs vertical"""
        
        # Cen√°rio: Processamento de 1TB de dados
        data_size_tb = 1
        
        # Escala Vertical (Scale Up)
        vertical_configs = [
            {'cpu_cores': 4, 'ram_gb': 16, 'cost_per_hour': 0.50},
            {'cpu_cores': 8, 'ram_gb': 32, 'cost_per_hour': 1.00},
            {'cpu_cores': 16, 'ram_gb': 64, 'cost_per_hour': 2.00},
            {'cpu_cores': 32, 'ram_gb': 128, 'cost_per_hour': 4.00}
        ]
        
        # Escala Horizontal (Scale Out)
        horizontal_configs = [
            {'nodes': 4, 'cpu_per_node': 4, 'ram_per_node': 16, 'cost_per_node_hour': 0.50},
            {'nodes': 8, 'cpu_per_node': 4, 'ram_per_node': 16, 'cost_per_node_hour': 0.50},
            {'nodes': 16, 'cpu_per_node': 4, 'ram_per_node': 16, 'cost_per_node_hour': 0.50},
            {'nodes': 32, 'cpu_per_node': 4, 'ram_per_node': 16, 'cost_per_node_hour': 0.50}
        ]
        
        print("=== ESCALABILIDADE VERTICAL ===")
        for config in vertical_configs:
            throughput = config['cpu_cores'] * 0.1  # TB/hora simplificado
            time_hours = data_size_tb / throughput
            total_cost = time_hours * config['cost_per_hour']
            print(f"CPU: {config['cpu_cores']} cores, RAM: {config['ram_gb']}GB")
            print(f"Tempo: {time_hours:.1f}h, Custo: ${total_cost:.2f}\n")
        
        print("=== ESCALABILIDADE HORIZONTAL ===")
        for config in horizontal_configs:
            total_cpu = config['nodes'] * config['cpu_per_node']
            throughput = total_cpu * 0.1  # TB/hora simplificado
            time_hours = data_size_tb / throughput
            total_cost = time_hours * config['nodes'] * config['cost_per_node_hour']
            print(f"N√≥s: {config['nodes']}, CPU Total: {total_cpu} cores")
            print(f"Tempo: {time_hours:.1f}h, Custo: ${total_cost:.2f}\n")

# Executar demonstra√ß√£o
demo = ScalabilityDemo()
demo.horizontal_vs_vertical_scaling()
```

### Toler√¢ncia a Falhas

```python
import random
import time

class FaultToleranceDemo:
    """Demonstra√ß√£o de toler√¢ncia a falhas"""
    
    def __init__(self, num_nodes=5):
        self.nodes = [f"node_{i}" for i in range(num_nodes)]
        self.node_status = {node: True for node in self.nodes}  # True = online
        self.replication_factor = 3
    
    def simulate_node_failure(self, failure_probability=0.1):
        """Simula falhas aleat√≥rias de n√≥s"""
        failed_nodes = []
        for node in self.nodes:
            if random.random() < failure_probability:
                self.node_status[node] = False
                failed_nodes.append(node)
        return failed_nodes
    
    def check_data_availability(self):
        """Verifica se dados ainda est√£o dispon√≠veis com replica√ß√£o"""
        online_nodes = sum(1 for status in self.node_status.values() if status)
        return online_nodes >= self.replication_factor
    
    def demonstrate_fault_tolerance(self, simulation_hours=24):
        """Simula 24h de opera√ß√£o com poss√≠veis falhas"""
        print("=== SIMULA√á√ÉO DE TOLER√ÇNCIA A FALHAS ===")
        print(f"N√≥s iniciais: {len(self.nodes)}")
        print(f"Fator de replica√ß√£o: {self.replication_factor}")
        print(f"Simula√ß√£o: {simulation_hours} horas\n")
        
        availability_history = []
        
        for hour in range(simulation_hours):
            # Simular falhas hor√°rias
            failed = self.simulate_node_failure(0.05)  # 5% chance de falha por hora
            
            # Verificar disponibilidade
            is_available = self.check_data_availability()
            availability_history.append(is_available)
            
            online_count = sum(1 for status in self.node_status.values() if status)
            
            if failed:
                print(f"Hora {hour:02d}: Falha em {failed}, N√≥s online: {online_count}, Dispon√≠vel: {'‚úÖ' if is_available else '‚ùå'}")
            
            # Simular recupera√ß√£o de alguns n√≥s
            if hour % 6 == 0:  # A cada 6 horas, tentar recuperar n√≥s
                for node in self.nodes:
                    if not self.node_status[node] and random.random() < 0.3:
                        self.node_status[node] = True
                        print(f"Hora {hour:02d}: N√≥ {node} recuperado")
        
        # Calcular uptime
        uptime_percentage = (sum(availability_history) / len(availability_history)) * 100
        print(f"\nUptime total: {uptime_percentage:.1f}%")
        
        return availability_history

# Executar simula√ß√£o
fault_demo = FaultToleranceDemo(num_nodes=8)
availability = fault_demo.demonstrate_fault_tolerance(24)

# Visualizar uptime
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(range(24), [1 if available else 0 for available in availability], 'bo-')
plt.title('Disponibilidade do Sistema ao Longo de 24 Horas')
plt.xlabel('Hora')
plt.ylabel('Sistema Dispon√≠vel (1=Sim, 0=N√£o)')
plt.grid(True, alpha=0.3)
plt.ylim(-0.1, 1.1)
plt.show()
```

## 1.4 Casos de Uso do Big Data

### 1. E-commerce e Recomenda√ß√µes

```python
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class RecommendationEngine:
    """Sistema de recomenda√ß√£o baseado em Big Data"""
    
    def __init__(self):
        self.user_item_matrix = None
        self.item_similarity = None
        
    def generate_sample_data(self, n_users=1000, n_products=100):
        """Gera dados simulados de e-commerce"""
        np.random.seed(42)
        
        # Produtos com caracter√≠sticas
        products = pd.DataFrame({
            'product_id': range(n_products),
            'category': np.random.choice(['Eletr√¥nicos', 'Roupas', 'Casa', 'Livros'], n_products),
            'price': np.random.uniform(10, 500, n_products),
            'description': [f"Produto de qualidade {np.random.choice(['premium', 'b√°sico', 'intermedi√°rio'])}" 
                          for _ in range(n_products)]
        })
        
        # Intera√ß√µes usu√°rio-produto (compras, visualiza√ß√µes, etc.)
        interactions = []
        for user_id in range(n_users):
            # Cada usu√°rio interage com 5-20 produtos
            n_interactions = np.random.randint(5, 21)
            user_products = np.random.choice(n_products, n_interactions, replace=False)
            
            for product_id in user_products:
                interaction = {
                    'user_id': user_id,
                    'product_id': product_id,
                    'rating': np.random.randint(1, 6),  # 1-5 estrelas
                    'interaction_type': np.random.choice(['view', 'cart', 'purchase'], 
                                                       p=[0.6, 0.3, 0.1])
                }
                interactions.append(interaction)
        
        interactions_df = pd.DataFrame(interactions)
        return products, interactions_df
    
    def build_recommendation_model(self, interactions_df):
        """Constr√≥i modelo de recomenda√ß√£o colaborativa"""
        # Matriz usu√°rio-item
        self.user_item_matrix = interactions_df.pivot_table(
            index='user_id', 
            columns='product_id', 
            values='rating', 
            fill_value=0
        )
        
        # Similaridade entre itens
        self.item_similarity = cosine_similarity(self.user_item_matrix.T)
        
    def recommend_products(self, user_id, n_recommendations=5):
        """Recomenda produtos para um usu√°rio"""
        if self.user_item_matrix is None:
            return []
        
        user_ratings = self.user_item_matrix.loc[user_id]
        
        # Produtos que o usu√°rio j√° avaliou
        rated_products = user_ratings[user_ratings > 0].index
        
        # Calcular scores para produtos n√£o avaliados
        recommendations = {}
        
        for product_id in self.user_item_matrix.columns:
            if product_id not in rated_products:
                # Score baseado em similaridade com produtos j√° avaliados
                score = 0
                similarity_sum = 0
                
                for rated_product in rated_products:
                    similarity = self.item_similarity[product_id][rated_product]
                    score += similarity * user_ratings[rated_product]
                    similarity_sum += similarity
                
                if similarity_sum > 0:
                    recommendations[product_id] = score / similarity_sum
        
        # Top N recomenda√ß√µes
        top_recommendations = sorted(recommendations.items(), 
                                   key=lambda x: x[1], 
                                   reverse=True)[:n_recommendations]
        
        return [product_id for product_id, score in top_recommendations]

# Demonstra√ß√£o do sistema de recomenda√ß√£o
print("=== SISTEMA DE RECOMENDA√á√ÉO E-COMMERCE ===")

rec_engine = RecommendationEngine()
products, interactions = rec_engine.generate_sample_data(1000, 100)

print(f"Dados gerados:")
print(f"- {len(products)} produtos")
print(f"- {len(interactions)} intera√ß√µes")
print(f"- {interactions['user_id'].nunique()} usu√°rios √∫nicos")

# An√°lise de padr√µes
print(f"\nPadr√µes identificados:")
print(f"- Categoria mais popular: {interactions.merge(products, on='product_id')['category'].mode()[0]}")
print(f"- Rating m√©dio: {interactions['rating'].mean():.2f}")
print(f"- Intera√ß√µes por usu√°rio: {interactions.groupby('user_id').size().mean():.1f}")

# Construir modelo
rec_engine.build_recommendation_model(interactions)

# Fazer recomenda√ß√µes para usu√°rio exemplo
sample_user = 0
recommendations = rec_engine.recommend_products(sample_user, 5)
print(f"\nRecomenda√ß√µes para usu√°rio {sample_user}: {recommendations}")
```

### 2. An√°lise de Sentimentos em Tempo Real

```python
import re
import numpy as np
from textblob import TextBlob
from collections import defaultdict
import matplotlib.pyplot as plt

class SentimentAnalysisEngine:
    """Engine de an√°lise de sentimentos para Big Data"""
    
    def __init__(self):
        self.sentiment_history = defaultdict(list)
        self.keywords_tracking = defaultdict(int)
    
    def clean_text(self, text):
        """Limpa e preprocessa texto"""
        # Remove URLs, men√ß√µes, hashtags
        text = re.sub(r'http\S+', '', text)
        text = re.sub(r'@\w+', '', text)
        text = re.sub(r'#\w+', '', text)
        text = re.sub(r'[^a-zA-Z√Ä-√ø\s]', '', text)
        return text.lower().strip()
    
    def analyze_sentiment(self, text):
        """Analisa sentimento do texto"""
        cleaned_text = self.clean_text(text)
        blob = TextBlob(cleaned_text)
        
        # Polaridade: -1 (negativo) a 1 (positivo)
        polarity = blob.sentiment.polarity
        
        # Classifica√ß√£o
        if polarity > 0.1:
            sentiment = 'positivo'
        elif polarity < -0.1:
            sentiment = 'negativo'
        else:
            sentiment = 'neutro'
            
        return {
            'text': text,
            'cleaned_text': cleaned_text,
            'polarity': polarity,
            'sentiment': sentiment,
            'subjectivity': blob.sentiment.subjectivity
        }
    
    def process_social_media_stream(self, posts, topic):
        """Processa stream de posts de redes sociais"""
        results = []
        sentiment_counts = {'positivo': 0, 'negativo': 0, 'neutro': 0}
        
        for post in posts:
            analysis = self.analyze_sentiment(post)
            results.append(analysis)
            sentiment_counts[analysis['sentiment']] += 1
            
            # Rastrear palavras-chave
            words = analysis['cleaned_text'].split()
            for word in words:
                if len(word) > 3:  # Palavras com mais de 3 caracteres
                    self.keywords_tracking[word] += 1
        
        # Armazenar hist√≥rico
        self.sentiment_history[topic] = results
        
        return {
            'total_posts': len(posts),
            'sentiment_distribution': sentiment_counts,
            'average_polarity': np.mean([r['polarity'] for r in results]),
            'top_keywords': sorted(self.keywords_tracking.items(), 
                                 key=lambda x: x[1], reverse=True)[:10]
        }
    
    def generate_sample_social_data(self, topic="Big Data", n_posts=1000):
        """Gera dados simulados de redes sociais"""
        templates_positive = [
            f"Adorei o curso de {topic}! Muito bem explicado.",
            f"{topic} est√° revolucionando nossa empresa.",
            f"Excelente palestra sobre {topic} hoje!",
            f"Consegui meu primeiro emprego em {topic}!",
            f"O futuro √© {topic}, sem d√∫vidas."
        ]
        
        templates_negative = [
            f"{topic} √© muito complicado de entender.",
            f"Terr√≠vel implementa√ß√£o de {topic} na empresa.",
            f"Curso de {topic} p√©ssimo, perda de tempo.",
            f"Frustrado com os resultados de {topic}.",
            f"N√£o recomendo trabalhar com {topic}."
        ]
        
        templates_neutral = [
            f"Estudando {topic} nas horas vagas.",
            f"Algu√©m conhece bom material sobre {topic}?",
            f"Evento sobre {topic} amanh√£ √†s 14h.",
            f"Pesquisando vagas em {topic}.",
            f"Qual a melhor ferramenta de {topic}?"
        ]
        
        posts = []
        for _ in range(n_posts):
            sentiment_type = np.random.choice(['positive', 'negative', 'neutral'], 
                                            p=[0.4, 0.2, 0.4])
            
            if sentiment_type == 'positive':
                post = np.random.choice(templates_positive)
            elif sentiment_type == 'negative':
                post = np.random.choice(templates_negative)
            else:
                post = np.random.choice(templates_neutral)
            
            # Adicionar ru√≠do realista
            post += f" #{topic.replace(' ', '')}" if np.random.random() < 0.3 else ""
            posts.append(post)
        
        return posts

# Demonstra√ß√£o de an√°lise de sentimentos
print("=== AN√ÅLISE DE SENTIMENTOS EM TEMPO REAL ===")

sentiment_engine = SentimentAnalysisEngine()

# Simular dados de diferentes t√≥picos
topics = ["Big Data", "Python", "Machine Learning", "Intelig√™ncia Artificial"]

results_summary = {}

for topic in topics:
    print(f"\nAnalisando sentimentos sobre: {topic}")
    posts = sentiment_engine.generate_sample_social_data(topic, 500)
    results = sentiment_engine.process_social_media_stream(posts, topic)
    results_summary[topic] = results
    
    print(f"Posts analisados: {results['total_posts']}")
    print(f"Sentimento m√©dio: {results['average_polarity']:.3f}")
    print(f"Distribui√ß√£o: {results['sentiment_distribution']}")
    print(f"Top palavras-chave: {results['top_keywords'][:5]}")

# Visualiza√ß√£o comparativa
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
axes = axes.flatten()

for i, (topic, results) in enumerate(results_summary.items()):
    sentiment_dist = results['sentiment_distribution']
    
    # Gr√°fico de pizza para cada t√≥pico
    axes[i].pie(sentiment_dist.values(), 
                labels=sentiment_dist.keys(),
                autopct='%1.1f%%',
                startangle=90)
    axes[i].set_title(f'Sentimentos: {topic}')

plt.tight_layout()
plt.show()

# Dashboard de m√©tricas
print("\n=== DASHBOARD DE SENTIMENTOS ===")
df_summary = pd.DataFrame(results_summary).T
print(df_summary[['total_posts', 'average_polarity']])
```

## 1.5 Desafios do Big Data

### Desafios T√©cnicos

```python
class BigDataChallenges:
    """Demonstra√ß√£o dos principais desafios t√©cnicos"""
    
    @staticmethod
    def storage_challenge():
        """Desafio de armazenamento"""
        print("=== DESAFIO DE ARMAZENAMENTO ===")
        
        # Crescimento exponencial de dados
        data_growth = {
            'Email (1GB)': 1,
            'M√∫sica (100GB)': 100,
            'Fotos HD (1TB)': 1000,
            'V√≠deos 4K (10TB)': 10000,
            'Dados IoT (100TB/dia)': 100000,
            'Dados Cient√≠ficos (1PB)': 1000000
        }
        
        for source, size_gb in data_growth.items():
            storage_cost = size_gb * 0.02  # $0.02 por GB/m√™s
            print(f"{source}: {size_gb:,} GB - Custo mensal: ${storage_cost:,.2f}")
    
    @staticmethod
    def processing_challenge():
        """Desafio de processamento"""
        print("\n=== DESAFIO DE PROCESSAMENTO ===")
        
        # Compara√ß√£o de tempos de processamento
        data_sizes = [1, 10, 100, 1000, 10000]  # GB
        
        print("Tempo estimado para diferentes volumes:")
        print("Volume (GB) | CPU Single | CPU Multi | Cluster")
        print("-" * 50)
        
        for size in data_sizes:
            single_cpu = size * 10  # 10 min/GB
            multi_cpu = size * 2    # 2 min/GB (5 cores)
            cluster = size * 0.2    # 0.2 min/GB (50 n√≥s)
            
            print(f"{size:8} GB | {single_cpu:8.1f} min | {multi_cpu:7.1f} min | {cluster:7.1f} min")
    
    @staticmethod
    def network_challenge():
        """Desafio de rede"""
        print("\n=== DESAFIO DE REDE ===")
        
        # Largura de banda vs tempo de transfer√™ncia
        bandwidths = {
            '10 Mbps (DSL)': 10,
            '100 Mbps (Fibra)': 100,
            '1 Gbps (Ethernet)': 1000,
            '10 Gbps (Data Center)': 10000,
            '100 Gbps (Backbone)': 100000
        }
        
        data_size_gb = 1000  # 1TB
        
        print("Tempo para transferir 1TB:")
        for connection, mbps in bandwidths.items():
            # Convers√£o: GB para bits, depois para segundos
            time_seconds = (data_size_gb * 8 * 1024) / mbps
            time_hours = time_seconds / 3600
            
            print(f"{connection}: {time_hours:.1f} horas")

# Executar demonstra√ß√µes dos desafios
challenges = BigDataChallenges()
challenges.storage_challenge()
challenges.processing_challenge()
challenges.network_challenge()
```

### Desafios de Privacidade e √âtica

```python
import hashlib
import random

class PrivacyProtection:
    """Demonstra√ß√£o de t√©cnicas de prote√ß√£o de privacidade"""
    
    @staticmethod
    def data_anonymization():
        """Anonimiza√ß√£o de dados pessoais"""
        print("=== ANONIMIZA√á√ÉO DE DADOS ===")
        
        # Dados originais (simulados)
        original_data = [
            {'nome': 'Jo√£o Silva', 'cpf': '123.456.789-00', 'email': 'joao@email.com', 'idade': 35},
            {'nome': 'Maria Santos', 'cpf': '987.654.321-00', 'email': 'maria@email.com', 'idade': 28},
            {'nome': 'Pedro Costa', 'cpf': '456.789.123-00', 'email': 'pedro@email.com', 'idade': 42}
        ]
        
        print("Dados originais:")
        for person in original_data:
            print(f"  {person}")
        
        # Anonimiza√ß√£o
        anonymized_data = []
        for person in original_data:
            # Hash dos identificadores
            name_hash = hashlib.sha256(person['nome'].encode()).hexdigest()[:8]
            cpf_hash = hashlib.sha256(person['cpf'].encode()).hexdigest()[:8]
            
            # Generaliza√ß√£o da idade (faixas et√°rias)
            age_range = f"{(person['idade'] // 10) * 10}-{(person['idade'] // 10) * 10 + 9}"
            
            anonymized = {
                'id_hash': name_hash,
                'cpf_hash': cpf_hash,
                'domain': person['email'].split('@')[1],  # Apenas dom√≠nio
                'faixa_etaria': age_range
            }
            anonymized_data.append(anonymized)
        
        print("\nDados anonimizados:")
        for person in anonymized_data:
            print(f"  {person}")
    
    @staticmethod
    def differential_privacy():
        """Demonstra√ß√£o de privacidade diferencial"""
        print("\n=== PRIVACIDADE DIFERENCIAL ===")
        
        # Dataset original
        salaries = [50000, 55000, 60000, 65000, 70000, 75000, 80000, 85000, 90000, 95000]
        true_average = sum(salaries) / len(salaries)
        
        print(f"M√©dia real dos sal√°rios: ${true_average:,.2f}")
        
        # Aplicar ru√≠do para privacidade diferencial
        epsilon = 1.0  # Par√¢metro de privacidade
        sensitivity = max(salaries) - min(salaries)  # Sensibilidade
        
        # Adicionar ru√≠do Laplaciano
        noise_scale = sensitivity / epsilon
        noisy_averages = []
        
        for i in range(5):
            noise = random.gauss(0, noise_scale)
            noisy_average = true_average + noise
            noisy_averages.append(noisy_average)
            print(f"Consulta {i+1}: ${noisy_average:,.2f} (ru√≠do: {noise:+.2f})")
        
        print(f"Erro m√©dio: ${abs(sum(noisy_averages)/len(noisy_averages) - true_average):,.2f}")

# Executar demonstra√ß√µes de privacidade
privacy_demo = PrivacyProtection()
privacy_demo.data_anonymization()
privacy_demo.differential_privacy()
```

## 1.6 Conclus√£o do Cap√≠tulo

Big Data representa uma mudan√ßa paradigm√°tica na forma como lidamos com informa√ß√£o. Os cinco V's (Volume, Velocidade, Variedade, Veracidade e Valor) definem n√£o apenas as caracter√≠sticas t√©cnicas, mas tamb√©m os desafios e oportunidades desta era.

### Pontos-Chave:

1. **Volume**: Capacidade de processar petabytes e exabytes de dados
2. **Velocidade**: Processamento em tempo real e streaming
3. **Variedade**: Integra√ß√£o de dados estruturados, semi-estruturados e n√£o-estruturados
4. **Veracidade**: Garantia de qualidade e confiabilidade dos dados
5. **Valor**: Extra√ß√£o de insights acion√°veis para tomada de decis√£o

### Prepara√ß√£o para os Pr√≥ximos Cap√≠tulos:

Nos pr√≥ximos cap√≠tulos, exploraremos como Python e seu ecossistema de bibliotecas nos permitem implementar solu√ß√µes pr√°ticas para cada um desses desafios, come√ßando pela prepara√ß√£o do ambiente de desenvolvimento e explorando as principais tecnologias do ecossistema Big Data.

---

**Exerc√≠cios Pr√°ticos:**

1. Implemente um sistema de monitoramento de qualidade de dados
2. Crie uma simula√ß√£o de crescimento exponencial de dados
3. Desenvolva um pipeline de anonimiza√ß√£o de dados pessoais
4. Analise um dataset brasileiro usando os conceitos dos 5 V's

**Pr√≥ximo Cap√≠tulo:** Python para Big Data - Fundamentos e Bibliotecas Essenciais
