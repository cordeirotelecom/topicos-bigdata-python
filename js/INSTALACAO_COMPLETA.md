# üõ†Ô∏è Guia Pr√°tico: Instala√ß√£o e Configura√ß√£o Completa

*Tutorial passo-a-passo para configurar seu ambiente de Big Data e IA*

---

## üìã **Pr√©-requisitos do Sistema**

### **Windows 10/11**
```powershell
# Verificar vers√£o do Windows
winver

# Verificar se tem PowerShell 5.1+
$PSVersionTable.PSVersion
```

### **macOS**
```bash
# Verificar vers√£o do macOS
sw_vers

# Instalar Homebrew (se n√£o tiver)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

### **Ubuntu/Linux**
```bash
# Verificar vers√£o do Ubuntu
lsb_release -a

# Atualizar sistema
sudo apt update && sudo apt upgrade -y
```

---

## üêç **Instala√ß√£o do Python**

### **Op√ß√£o 1: Python.org (Recomendado para iniciantes)**

**Windows:**
1. Acesse [python.org/downloads](https://python.org/downloads)
2. Baixe Python 3.11.x (vers√£o mais est√°vel)
3. **IMPORTANTE**: Marque "Add Python to PATH"
4. Execute a instala√ß√£o
5. Teste no PowerShell:
```powershell
python --version
pip --version
```

**macOS:**
```bash
# Via Homebrew (recomendado)
brew install python@3.11

# Verificar instala√ß√£o
python3 --version
pip3 --version
```

**Ubuntu:**
```bash
# Instalar Python 3.11
sudo apt install python3.11 python3.11-pip python3.11-venv

# Criar link simb√≥lico (opcional)
sudo ln -sf /usr/bin/python3.11 /usr/bin/python

# Verificar
python --version
```

### **Op√ß√£o 2: Anaconda (Recomendado para cientistas de dados)**

1. Baixe [Anaconda](https://www.anaconda.com/download)
2. Instale seguindo o assistente
3. Abra Anaconda Navigator ou Anaconda Prompt
4. Teste:
```bash
conda --version
python --version
```

---

## ‚òï **Instala√ß√£o do Java (Essencial para PySpark)**

### **Por que preciso do Java?**
Apache Spark √© escrito em Scala (que roda na JVM). PySpark √© uma interface Python para Spark, mas ainda precisa do Java.

### **Windows:**
```powershell
# Op√ß√£o 1: Chocolatey (package manager)
choco install openjdk11

# Op√ß√£o 2: Download manual
# 1. Acesse https://adoptium.net/
# 2. Baixe OpenJDK 11 LTS
# 3. Instale e configure JAVA_HOME

# Verificar instala√ß√£o
java -version
echo $env:JAVA_HOME
```

### **macOS:**
```bash
# Via Homebrew
brew install openjdk@11

# Configurar JAVA_HOME no .zshrc ou .bash_profile
echo 'export JAVA_HOME=/opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home' >> ~/.zshrc
source ~/.zshrc

# Verificar
java -version
echo $JAVA_HOME
```

### **Ubuntu:**
```bash
# Instalar OpenJDK 11
sudo apt install openjdk-11-jdk

# Configurar JAVA_HOME
echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc
source ~/.bashrc

# Verificar
java -version
echo $JAVA_HOME
```

---

## üì¶ **Cria√ß√£o do Ambiente Virtual**

### **Por que usar ambiente virtual?**
- Isola depend√™ncias do projeto
- Evita conflitos entre bibliotecas
- Facilita reprodu√ß√£o em outros sistemas

### **Criando o ambiente:**

```bash
# 1. Navegar para pasta do projeto
cd "caminho/para/BigData em Python"

# 2. Criar ambiente virtual
python -m venv venv

# 3. Ativar ambiente virtual
# Windows:
venv\Scripts\activate

# macOS/Linux:
source venv/bin/activate

# 4. Verificar se est√° ativo (deve aparecer (venv) no prompt)
which python  # macOS/Linux
where python   # Windows
```

---

## üìö **Instala√ß√£o das Bibliotecas**

### **Passo 1: Instalar depend√™ncias b√°sicas**
```bash
# Atualizar pip
python -m pip install --upgrade pip

# Instalar bibliotecas essenciais
pip install pandas numpy matplotlib seaborn

# Verificar instala√ß√£o
python -c "import pandas as pd; print('‚úÖ Pandas OK!')"
python -c "import numpy as np; print('‚úÖ NumPy OK!')"
python -c "import matplotlib.pyplot as plt; print('‚úÖ Matplotlib OK!')"
```

### **Passo 2: Instalar PySpark**
```bash
# Instalar PySpark
pip install pyspark findspark

# Testar instala√ß√£o
python -c "
import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Teste').getOrCreate()
print('‚úÖ PySpark funcionando!')
spark.stop()
"
```

### **Passo 3: Machine Learning e IA**
```bash
# Bibliotecas de ML tradicionais
pip install scikit-learn xgboost

# Deep Learning (opcional)
pip install tensorflow torch torchvision

# Processamento de texto (NLP)
pip install nltk spacy transformers

# Verificar ML
python -c "from sklearn.datasets import make_classification; print('‚úÖ Scikit-learn OK!')"
```

### **Passo 4: Jupyter e visualiza√ß√£o**
```bash
# Jupyter ecosystem
pip install jupyter jupyterlab ipywidgets

# Visualiza√ß√£o avan√ßada
pip install plotly bokeh streamlit

# Iniciar Jupyter Lab
jupyter lab
```

---

## üîß **Configura√ß√£o Avan√ßada**

### **Configurar PySpark no Jupyter**

Criar arquivo `spark_config.py`:
```python
import os
import findspark

# Configurar caminhos (adapte para seu sistema)
os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'  # Linux
# os.environ['JAVA_HOME'] = 'C:/Program Files/Java/jdk-11.0.x'  # Windows
# os.environ['JAVA_HOME'] = '/opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home'  # macOS

findspark.init()

from pyspark.sql import SparkSession
from pyspark.conf import SparkConf

def criar_spark_session(app_name="BigData_SC"):
    """Cria sess√£o Spark configurada para aprendizado"""
    conf = SparkConf()
    conf.set("spark.sql.adaptive.enabled", "true")
    conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
    
    spark = SparkSession.builder \
        .appName(app_name) \
        .config(conf=conf) \
        .getOrCreate()
    
    return spark

# Usar assim:
# spark = criar_spark_session()
```

### **Configurar GPU para Deep Learning (Opcional)**

**Para NVIDIA GPU:**
```bash
# Verificar se tem GPU NVIDIA
nvidia-smi

# Instalar CUDA toolkit (vers√£o compat√≠vel com TensorFlow)
# Windows/Linux: https://developer.nvidia.com/cuda-toolkit
# Verificar compatibilidade: https://www.tensorflow.org/install/source#gpu

# Instalar TensorFlow com GPU
pip install tensorflow[and-cuda]

# Testar GPU
python -c "
import tensorflow as tf
print('GPUs dispon√≠veis:', tf.config.list_physical_devices('GPU'))
"
```

---

## üß™ **Testes de Valida√ß√£o Completa**

### **Script de teste autom√°tico:**

Criar `teste_instalacao.py`:
```python
#!/usr/bin/env python3
"""
Script de valida√ß√£o completa do ambiente Big Data + IA
"""

def teste_bibliotecas_basicas():
    print("üîç Testando bibliotecas b√°sicas...")
    try:
        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import seaborn as sns
        print("‚úÖ Pandas, NumPy, Matplotlib, Seaborn: OK")
        
        # Teste r√°pido
        df = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})
        assert len(df) == 3
        print("‚úÖ Teste funcional Pandas: OK")
        
    except Exception as e:
        print(f"‚ùå Erro nas bibliotecas b√°sicas: {e}")
        return False
    return True

def teste_pyspark():
    print("\nüîç Testando PySpark...")
    try:
        import findspark
        findspark.init()
        
        from pyspark.sql import SparkSession
        spark = SparkSession.builder \
            .appName("TesteValidacao") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
            .getOrCreate()
        
        # Teste com dados
        dados = [("Patrick", 25, "Florian√≥polis"), ("Ana", 30, "S√£o Jos√©")]
        df = spark.createDataFrame(dados, ["nome", "idade", "cidade"])
        
        assert df.count() == 2
        print("‚úÖ PySpark: OK")
        
        spark.stop()
        
    except Exception as e:
        print(f"‚ùå Erro no PySpark: {e}")
        print("üí° Dica: Verifique se Java est√° instalado (java -version)")
        return False
    return True

def teste_machine_learning():
    print("\nüîç Testando Machine Learning...")
    try:
        from sklearn.datasets import make_classification
        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import accuracy_score
        
        # Criar dataset sint√©tico
        X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        
        # Treinar modelo
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        model.fit(X_train, y_train)
        
        # Predizer
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        
        assert acc > 0.7  # Pelo menos 70% de acur√°cia
        print(f"‚úÖ Scikit-learn: OK (Acur√°cia: {acc:.3f})")
        
    except Exception as e:
        print(f"‚ùå Erro no Machine Learning: {e}")
        return False
    return True

def teste_deep_learning():
    print("\nüîç Testando Deep Learning (opcional)...")
    try:
        import tensorflow as tf
        
        # Verificar GPU
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            print(f"‚úÖ TensorFlow com {len(gpus)} GPU(s): OK")
        else:
            print("‚úÖ TensorFlow (CPU apenas): OK")
        
        # Teste simples
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(10, activation='relu', input_shape=(5,)),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])
        
        # Dados sint√©ticos
        import numpy as np
        X = np.random.random((100, 5))
        y = np.random.randint(0, 2, (100, 1))
        
        model.compile(optimizer='adam', loss='binary_crossentropy')
        model.fit(X, y, epochs=1, verbose=0)
        
        print("‚úÖ Teste TensorFlow: OK")
        
    except ImportError:
        print("‚ö†Ô∏è  TensorFlow n√£o instalado (opcional)")
        return True
    except Exception as e:
        print(f"‚ùå Erro no TensorFlow: {e}")
        return False
    return True

def teste_jupyter():
    print("\nüîç Testando Jupyter...")
    try:
        import jupyter
        import jupyterlab
        print("‚úÖ Jupyter Lab: OK")
        print("üí° Para iniciar: jupyter lab")
        
    except Exception as e:
        print(f"‚ùå Erro no Jupyter: {e}")
        return False
    return True

def relatorio_sistema():
    print("\nüìä Relat√≥rio do Sistema:")
    import sys
    import platform
    
    print(f"üêç Python: {sys.version.split()[0]}")
    print(f"üíª Sistema: {platform.system()} {platform.release()}")
    print(f"üèóÔ∏è  Arquitetura: {platform.architecture()[0]}")
    
    # Verificar Java
    import subprocess
    try:
        result = subprocess.run(['java', '-version'], 
                              capture_output=True, text=True, stderr=subprocess.STDOUT)
        java_version = result.stdout.split('\n')[0]
        print(f"‚òï Java: {java_version}")
    except:
        print("‚ùå Java: N√£o encontrado")

if __name__ == "__main__":
    print("üöÄ Iniciando valida√ß√£o completa do ambiente Big Data + IA\n")
    
    relatorio_sistema()
    
    testes = [
        teste_bibliotecas_basicas,
        teste_pyspark,
        teste_machine_learning,
        teste_deep_learning,
        teste_jupyter
    ]
    
    resultados = []
    for teste in testes:
        resultado = teste()
        resultados.append(resultado)
    
    print(f"\nüìã Resumo: {sum(resultados)}/{len(resultados)} testes passaram")
    
    if all(resultados):
        print("üéâ Ambiente configurado com sucesso!")
        print("\nüìö Pr√≥ximos passos:")
        print("1. jupyter lab")
        print("2. Abrir livro/capitulo01-despertar-dos-dados.md")
        print("3. Seguir a jornada de Patrick em Big Data!")
    else:
        print("‚ö†Ô∏è  Alguns testes falharam. Verifique a instala√ß√£o.")
```

### **Executar valida√ß√£o:**
```bash
python teste_instalacao.py
```

---

## üö® **Solu√ß√£o de Problemas Comuns**

### **1. "Java not found" no PySpark**
```bash
# Verificar se Java est√° no PATH
java -version

# Se n√£o encontrar, definir JAVA_HOME
# Windows (PowerShell):
$env:JAVA_HOME = "C:\Program Files\Java\jdk-11.0.x"

# macOS/Linux:
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

### **2. "Permission denied" no macOS/Linux**
```bash
# Dar permiss√µes de execu√ß√£o
chmod +x teste_instalacao.py

# Usar sudo apenas se necess√°rio
sudo pip install nome_biblioteca
```

### **3. Conflitos de vers√£o**
```bash
# Limpar cache do pip
pip cache purge

# Reinstalar tudo
pip uninstall -y -r requirements.txt
pip install -r requirements.txt
```

### **4. Problemas com Jupyter**
```bash
# Recriar kernels
python -m ipykernel install --user --name=venv

# Iniciar Jupyter com configura√ß√µes padr√£o
jupyter lab --generate-config
```

---

## üéØ **Pr√≥ximos Passos**

1. **‚úÖ Executar teste de valida√ß√£o**
2. **üìö Ler Cap√≠tulo 1** - Patrick e os fundamentos de Big Data
3. **üíª Abrir Jupyter Lab** - ambiente interativo
4. **üî¨ Experimentar c√≥digos** - cada cap√≠tulo tem exemplos
5. **üöÄ Criar seu primeiro projeto** - adaptar casos para sua regi√£o

**Ambiente configurado? Hora de come√ßar a jornada com Patrick! üåü**
