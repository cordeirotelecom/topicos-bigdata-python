# Aula 06 - Introdu√ß√£o ao Hadoop Ecosystem

## üéØ Objetivos da Aula
- Compreender a arquitetura do Hadoop
- Configurar um cluster Hadoop local
- Trabalhar com HDFS, YARN e MapReduce
- Implementar jobs MapReduce em Python

## üìã Conte√∫do Program√°tico

### 1. Fundamentos do Hadoop
- Hist√≥rico e evolu√ß√£o
- Arquitetura distribu√≠da
- Componentes principais

### 2. HDFS (Hadoop Distributed File System)
- Arquitetura NameNode/DataNode
- Replica√ß√£o e toler√¢ncia a falhas
- Comandos b√°sicos

### 3. YARN (Yet Another Resource Negotiator)
- Resource Manager
- Node Manager
- Container management

### 4. MapReduce Framework
- Paradigma Map-Reduce
- Job execution flow
- Optimization techniques

## üõ†Ô∏è Atividades Pr√°ticas

### Exerc√≠cio 1: Setup Hadoop Cluster
- Configura√ß√£o pseudo-distribu√≠da
- Inicializa√ß√£o dos servi√ßos
- Verifica√ß√£o do cluster

### Exerc√≠cio 2: HDFS Operations
- Upload/download de arquivos
- Opera√ß√µes de diret√≥rio
- Monitoramento de espa√ßo

### Exerc√≠cio 3: MapReduce Jobs
- Word Count cl√°ssico
- Data aggregation
- Custom reducers

## üìä Projeto Pr√°tico
Implementa√ß√£o de um pipeline de an√°lise de logs de servidor usando MapReduce para identificar:
- IPs mais ativos
- P√°ginas mais acessadas
- Padr√µes de erro

## üìö Material de Apoio
- [Documenta√ß√£o oficial Hadoop](https://hadoop.apache.org/docs/)
- [Guia de instala√ß√£o](../INSTALACAO.md)
- [Scripts de configura√ß√£o](./scripts/)

# ROTEIRO DE LABORAT√ìRIO HADOOP
## Tutorial Completo: Introdu√ß√£o, Instala√ß√£o e Pr√°tica com Apache Hadoop

---

## √çNDICE

1. [Introdu√ß√£o ao Big Data e Hadoop](#1-introdu√ß√£o-ao-big-data-e-hadoop)
   - 1.1 O que √© Big Data?
   - 1.2 Os 5Vs do Big Data
   - 1.3 O que √© Apache Hadoop?
   - 1.4 Por que o Hadoop √© Importante?
   - 1.5 Casos de Uso Reais

2. [Arquitetura do Hadoop](#2-arquitetura-do-hadoop)
   - 2.1 Vis√£o Geral da Arquitetura
   - 2.2 HDFS (Hadoop Distributed File System)
   - 2.3 MapReduce
   - 2.4 YARN (Yet Another Resource Negotiator)
   - 2.5 Ecossistema Hadoop

3. [Pr√©-requisitos e Prepara√ß√£o do Ambiente](#3-pr√©-requisitos-e-prepara√ß√£o-do-ambiente)
   - 3.1 Requisitos de Sistema
   - 3.2 Conhecimentos Pr√©vios Necess√°rios
   - 3.3 Ferramentas Auxiliares

4. [Instala√ß√£o do Hadoop](#4-instala√ß√£o-do-hadoop)
   - 4.1 M√©todos de Instala√ß√£o
   - 4.2 Instala√ß√£o Automatizada com Scripts
   - 4.3 Verifica√ß√£o da Instala√ß√£o
   - 4.4 Troubleshooting Comum

5. [Exerc√≠cios Pr√°ticos](#5-exerc√≠cios-pr√°ticos)
   - 5.1 Exerc√≠cio 1: Grep (Busca de Padr√µes)
   - 5.2 Exerc√≠cio 2: WordCount (Contagem de Palavras)
   - 5.3 Exerc√≠cio 3: An√°lise de Logs (Novo)
   - 5.4 Exerc√≠cio 4: Processamento de Dados CSV (Novo)

6. [Monitoramento e Interface Web](#6-monitoramento-e-interface-web)
   - 6.1 Interface Web do Hadoop
   - 6.2 Monitoramento de Jobs
   - 6.3 An√°lise de Performance

7. [Integra√ß√£o com Python](#7-integra√ß√£o-com-python)
   - 7.1 Bibliotecas Python para Hadoop
   - 7.2 Exemplos Pr√°ticos com Python
   - 7.3 Desenvolvimento de Jobs MapReduce em Python

8. [Troubleshooting e FAQ](#8-troubleshooting-e-faq)
   - 8.1 Problemas Comuns
   - 8.2 Solu√ß√µes e Workarounds
   - 8.3 Perguntas Frequentes

9. [Recursos Adicionais](#9-recursos-adicionais)
   - 9.1 Documenta√ß√£o Oficial
   - 9.2 Tutoriais Complementares
   - 9.3 Comunidade e Suporte

10. [Refer√™ncias](#10-refer√™ncias)

---



## 1. INTRODU√á√ÉO AO BIG DATA E HADOOP

### 1.1 O que √© Big Data?

Big Data refere-se a conjuntos de dados extremamente grandes, complexos e diversos que crescem exponencialmente ao longo do tempo. Esses dados s√£o t√£o volumosos, variados e gerados em alta velocidade que as ferramentas tradicionais de processamento de dados e sistemas de gerenciamento de banco de dados n√£o conseguem capturar, armazenar, gerenciar e analisar eficientemente [1].

O conceito de Big Data emergiu da necessidade de lidar com a explos√£o de dados digitais que come√ßou no final dos anos 1990 e se acelerou dramaticamente com o advento das redes sociais, dispositivos m√≥veis, Internet das Coisas (IoT) e computa√ß√£o em nuvem. Segundo estimativas recentes, o volume global de dados deve atingir 180 zettabytes at√© 2025, representando um crescimento exponencial que desafia os m√©todos tradicionais de processamento [2].

### 1.2 Os 5Vs do Big Data

![Diagrama dos 5Vs do Big Data](https://private-us-east-1.manuscdn.com/sessionFile/Kv3nXRCXCLHmEa8ubHl8P8/sandbox/WewAvHF1P8YRJu9LP7jPJa-images_1756742930678_na1fn_L2hvbWUvdWJ1bnR1L3VwbG9hZC9zZWFyY2hfaW1hZ2VzL0xETEE4b1BHR2xZSA.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvS3YzblhSQ1hDTEhtRWE4dWJIbDhQOC9zYW5kYm94L1dld0F2SEYxUDhZUkp1OUxQN2pQSmEtaW1hZ2VzXzE3NTY3NDI5MzA2NzhfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwzVndiRzloWkM5elpXRnlZMmhmYVcxaFoyVnpMMHhFVEVFNGIxQkhSMnhaU0EucG5nIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzk4NzYxNjAwfX19XX0_&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=WYiYrv9bngYPA1MY9e39JT19u3IafkF1BcV7Kj4Rx-Bty0HHnFk6M4-p6g~1bnXxyWUv3ixGwybEy8g~CGWoOkr0ptZP822O1ncIzRDvwKDp7pks9Zfo9D0TdDRTFsUnO3TwyHTrURW1L-sd3C1Yt3sPZUV1kbErXbDDICgZtzVoK9WqPMIuwWu5do53TpeXBXGMfqJ4cXgGkXX8sELSV8y8B8mUayNrID49UlmIFEKAH7hN3TVBAEaBhawML-DWmspJsyrJO9-VgmgzGdEJMS0agjS55Fi84Y7ooZAQjcTTkMa2ZETlUeRrskza8XpmJfUgucDVP13NtBGbh3Ze9Q__)

Em 2025, o Big Data √© caracterizado pelos **5Vs fundamentais**, uma evolu√ß√£o do modelo original de 3Vs que agora inclui duas dimens√µes cr√≠ticas adicionais [3]:

#### **1. Volume (Escala dos Dados)**
O Volume refere-se √† quantidade massiva de dados gerados e coletados. Em 2025, estamos lidando com:
- **Petabytes e Exabytes** de dados corporativos
- **2.5 quintilh√µes de bytes** de dados criados diariamente
- **Crescimento de 40% ao ano** no volume global de dados

**Exemplos pr√°ticos em 2025:**
- Netflix processa mais de 15 petabytes de dados diariamente para recomenda√ß√µes
- Google processa mais de 20 petabytes de dados por dia
- Walmart coleta mais de 2.5 petabytes de dados de transa√ß√µes de clientes por hora

#### **2. Velocity (Velocidade dos Dados)**
A Velocity representa a velocidade com que os dados s√£o gerados, processados e analisados. Em 2025, isso inclui:
- **Processamento em tempo real** (real-time)
- **Streaming de dados** cont√≠nuo
- **An√°lise instant√¢nea** para tomada de decis√µes

**Exemplos pr√°ticos em 2025:**
- Sistemas de detec√ß√£o de fraude banc√°ria processam transa√ß√µes em milissegundos
- Plataformas de trading algor√≠tmico executam milh√µes de opera√ß√µes por segundo
- Sistemas de IoT em carros aut√¥nomos processam dados de sensores em tempo real

#### **3. Variety (Variedade dos Dados)**
A Variety abrange os diferentes tipos e formatos de dados. Em 2025, isso inclui:
- **Dados estruturados** (bancos de dados relacionais)
- **Dados semi-estruturados** (XML, JSON)
- **Dados n√£o estruturados** (textos, imagens, v√≠deos, √°udios)

**Exemplos pr√°ticos em 2025:**
- An√°lise de sentimentos combinando texto, emojis e imagens de redes sociais
- Sistemas de sa√∫de integrando exames m√©dicos, hist√≥rico cl√≠nico e dados de wearables
- Plataformas de e-commerce analisando comportamento de navega√ß√£o, reviews e dados demogr√°ficos

#### **4. Veracity (Veracidade dos Dados)**
A Veracity refere-se √† qualidade, precis√£o e confiabilidade dos dados. Em 2025, isso √© crucial devido a:
- **Dados incompletos ou inconsistentes**
- **Informa√ß√µes falsas** (fake news, deepfakes)
- **Ru√≠do nos dados** de sensores IoT

**Exemplos pr√°ticos em 2025:**
- Algoritmos de verifica√ß√£o de fatos em plataformas de m√≠dia social
- Sistemas de valida√ß√£o de dados m√©dicos para diagn√≥sticos por IA
- Filtros de qualidade em dados de sensores industriais

#### **5. Value (Valor dos Dados)**
O Value representa o benef√≠cio e insights acion√°veis extra√≠dos dos dados. Em 2025, o foco est√° em:
- **ROI mensur√°vel** de iniciativas de Big Data
- **Insights acion√°veis** para decis√µes estrat√©gicas
- **Monetiza√ß√£o de dados** como ativo empresarial

**Exemplos pr√°ticos em 2025:**
- Modelos preditivos que reduzem custos operacionais em 20-30%
- Personaliza√ß√£o que aumenta convers√µes de vendas em at√© 50%
- Otimiza√ß√£o de supply chain que reduz desperd√≠cios em 25%

### 1.3 O que √© Apache Hadoop?

Apache Hadoop √© uma estrutura de software de c√≥digo aberto projetada especificamente para armazenar e processar grandes volumes de dados de forma distribu√≠da em clusters de computadores comuns (commodity hardware). Desenvolvido originalmente pelo Yahoo! em 2006, baseado nos papers do Google sobre MapReduce e Google File System, o Hadoop se tornou o padr√£o de facto para processamento de Big Data [4].

O nome "Hadoop" vem do elefante de brinquedo amarelo do filho de Doug Cutting, um dos criadores do projeto. Esta origem l√∫dica contrasta com a robustez e seriedade da plataforma, que hoje processa exabytes de dados em organiza√ß√µes ao redor do mundo.

**Caracter√≠sticas fundamentais do Hadoop:**

- **Escalabilidade horizontal**: Capacidade de adicionar mais n√≥s ao cluster para aumentar a capacidade de processamento e armazenamento
- **Toler√¢ncia a falhas**: Replica√ß√£o autom√°tica de dados em m√∫ltiplos n√≥s para garantir disponibilidade
- **Processamento distribu√≠do**: Divis√£o de tarefas complexas em subtarefas menores executadas em paralelo
- **Custo-efetivo**: Utiliza√ß√£o de hardware comum ao inv√©s de servidores especializados caros
- **Flexibilidade**: Capacidade de processar dados estruturados e n√£o estruturados

### 1.4 Por que o Hadoop √© Importante em 2025?

Em 2025, o Hadoop continua sendo uma tecnologia fundamental para Big Data, especialmente quando integrado com tecnologias modernas de IA e machine learning. Sua import√¢ncia se manifesta em v√°rias dimens√µes [5]:

#### **Escalabilidade Massiva**
O Hadoop pode processar desde gigabytes at√© exabytes de dados, escalando horizontalmente conforme a necessidade. Empresas como Facebook e LinkedIn operam clusters Hadoop com milhares de n√≥s, processando petabytes de dados diariamente.

#### **Economia de Custos**
Comparado a solu√ß√µes propriet√°rias tradicionais, o Hadoop oferece uma redu√ß√£o de custos de 10x a 100x para armazenamento e processamento de Big Data. Isso democratiza o acesso a an√°lises avan√ßadas para empresas de todos os tamanhos.

#### **Integra√ß√£o com IA e Machine Learning**
Em 2025, o Hadoop serve como base para pipelines de machine learning, fornecendo dados limpos e processados para algoritmos de IA. Ferramentas como Apache Spark e TensorFlow se integram nativamente com o ecossistema Hadoop.

#### **Processamento de Dados N√£o Estruturados**
Com 80% dos dados corporativos sendo n√£o estruturados (textos, imagens, v√≠deos), o Hadoop oferece a flexibilidade necess√°ria para processar esses formatos diversos, algo que bancos de dados relacionais tradicionais n√£o conseguem fazer eficientemente.

### 1.5 Casos de Uso Reais em 2025

#### **1. Sa√∫de Digital e Medicina Personalizada**
Hospitais e sistemas de sa√∫de utilizam Hadoop para:
- Processar imagens m√©dicas (raios-X, resson√¢ncias) para diagn√≥sticos assistidos por IA
- Analisar dados gen√¥micos para medicina personalizada
- Monitorar dados de dispositivos wearables para preven√ß√£o de doen√ßas
- Otimizar opera√ß√µes hospitalares e reduzir custos

**Exemplo:** O Hospital Johns Hopkins utiliza Hadoop para processar mais de 50TB de dados m√©dicos di√°rios, melhorando diagn√≥sticos em 30% e reduzindo tempo de an√°lise de semanas para horas.

#### **2. Cidades Inteligentes (Smart Cities)**
Governos municipais implementam Hadoop para:
- Otimizar tr√°fego urbano com dados de sensores e c√¢meras
- Gerenciar consumo de energia e recursos h√≠dricos
- Melhorar seguran√ßa p√∫blica com an√°lise preditiva
- Otimizar coleta de lixo e servi√ßos urbanos

**Exemplo:** A cidade de Barcelona processa dados de mais de 20.000 sensores IoT usando Hadoop, reduzindo congestionamentos em 25% e economizando 30% no consumo de √°gua.

#### **3. Fintech e Detec√ß√£o de Fraudes**
Institui√ß√µes financeiras usam Hadoop para:
- Detectar fraudes em tempo real analisando padr√µes de transa√ß√£o
- Avaliar riscos de cr√©dito com dados alternativos
- Personalizar produtos financeiros baseados em comportamento
- Cumprir regulamenta√ß√µes de compliance automaticamente

**Exemplo:** O Banco Santander processa mais de 100 milh√µes de transa√ß√µes di√°rias usando Hadoop, detectando fraudes com 99.5% de precis√£o e reduzindo falsos positivos em 60%.

#### **4. E-commerce e Personaliza√ß√£o**
Plataformas de com√©rcio eletr√¥nico utilizam Hadoop para:
- Sistemas de recomenda√ß√£o em tempo real
- Otimiza√ß√£o de pre√ßos din√¢mica
- An√°lise de sentimentos de reviews e redes sociais
- Previs√£o de demanda e gest√£o de estoque

**Exemplo:** A Amazon processa mais de 15 petabytes de dados de clientes diariamente com Hadoop, gerando 35% de suas receitas atrav√©s de recomenda√ß√µes personalizadas.

#### **5. Agricultura de Precis√£o**
O agroneg√≥cio moderno emprega Hadoop para:
- An√°lise de dados de sat√©lites e drones para monitoramento de culturas
- Otimiza√ß√£o de irriga√ß√£o baseada em dados clim√°ticos
- Previs√£o de safras e detec√ß√£o precoce de pragas
- Rastreabilidade completa da cadeia alimentar

**Exemplo:** A John Deere utiliza Hadoop para processar dados de mais de 500.000 m√°quinas agr√≠colas conectadas, aumentando produtividade em 20% e reduzindo uso de pesticidas em 15%.

---



## 2. ARQUITETURA DO HADOOP

### 2.1 Vis√£o Geral da Arquitetura

![Arquitetura Hadoop](https://private-us-east-1.manuscdn.com/sessionFile/Kv3nXRCXCLHmEa8ubHl8P8/sandbox/WewAvHF1P8YRJu9LP7jPJa-images_1756742930681_na1fn_L2hvbWUvdWJ1bnR1L3VwbG9hZC9zZWFyY2hfaW1hZ2VzL05EaHBwUVlMZVFvWg.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvS3YzblhSQ1hDTEhtRWE4dWJIbDhQOC9zYW5kYm94L1dld0F2SEYxUDhZUkp1OUxQN2pQSmEtaW1hZ2VzXzE3NTY3NDI5MzA2ODFfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwzVndiRzloWkM5elpXRnlZMmhmYVcxaFoyVnpMMDVFYUhCd1VWbE1aVkZ2V2cucG5nIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzk4NzYxNjAwfX19XX0_&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=M75niLEpXOvOFcAUMr9T9xiCl5dHILticMwNWByH44znmjZt~X-h6lgfZ-mfFQlLeetzF13qy4XaIZGD9ImvCLDsa~iOEaCiGit9fcn2SInWgKEdux6Cj6syTcYPKfWl6W~4kCFXz2AOML34wACXD3gFTgSJ8E0s5qN4RceBabxFLEScyt4SIPTaj4dVTNMlrJaC5aMIHsK23DS4k2ij~yj382Q6QdsKpciJ7Rx7igRCSDjqh4c4lfIuBlsMsRoSF8ZP2h0~~FSPAa1LFLaNa2ozKCv99UKkMNd1EopiVyznIeFMBqRXBFX~ET-R3-VjtXI5Aa4WJ5KDlN2FPn6cUg__)

A arquitetura do Apache Hadoop √© baseada em um design distribu√≠do que permite o processamento paralelo de grandes volumes de dados em clusters de computadores comuns. A arquitetura √© composta por quatro componentes principais que trabalham em conjunto para fornecer uma plataforma robusta e escal√°vel [6]:

1. **Hadoop Common**: Bibliotecas e utilit√°rios comuns necess√°rios para outros m√≥dulos Hadoop
2. **HDFS (Hadoop Distributed File System)**: Sistema de arquivos distribu√≠do para armazenamento
3. **YARN (Yet Another Resource Negotiator)**: Gerenciador de recursos e agendador de tarefas
4. **MapReduce**: Framework de programa√ß√£o para processamento distribu√≠do

Esta arquitetura modular permite que cada componente seja otimizado independentemente, mantendo a interoperabilidade e facilitando a manuten√ß√£o e evolu√ß√£o do sistema.

### 2.2 HDFS (Hadoop Distributed File System)

![Arquitetura HDFS](https://private-us-east-1.manuscdn.com/sessionFile/Kv3nXRCXCLHmEa8ubHl8P8/sandbox/WewAvHF1P8YRJu9LP7jPJa-images_1756742930682_na1fn_L2hvbWUvdWJ1bnR1L3VwbG9hZC9zZWFyY2hfaW1hZ2VzL05WWm91YnlEam43cg.gif?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvS3YzblhSQ1hDTEhtRWE4dWJIbDhQOC9zYW5kYm94L1dld0F2SEYxUDhZUkp1OUxQN2pQSmEtaW1hZ2VzXzE3NTY3NDI5MzA2ODJfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwzVndiRzloWkM5elpXRnlZMmhmYVcxaFoyVnpMMDVXV205MVlubEVhbTQzY2cuZ2lmIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzk4NzYxNjAwfX19XX0_&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=J44lviJNPRibDx0KNt1wy3PJ0N6f~Y5taSiCsySbgLiO24R3g~RCLAyddhrNfMQXB5lCAYoOjq7rF2KMXGsypXZr9tzVnpyBt3zMKZnNb1h6ll0tgK6LCi59C5u53PyDFgnpvszwTXwaNee4sRaf~miKXsQssK77sYZPyKdVJYI8dXxnExC6b-kr7CQjLQgH3yDPBcnGCng1Es5UFOjXeQUY9zi1Ig3Un7i85aT26Ma6jZkQXsaQWCrmza1BOkzpUNOG00plNHaD6sFLdbc-8w-P1qdufr0fy1LoFS0TP20n9U1xtHX5rDN0mi-JcceGwMbw4wHOedHRsfsVTOY~Uw__)

O HDFS √© o sistema de arquivos distribu√≠do do Hadoop, projetado para armazenar grandes arquivos de dados em clusters de m√°quinas comuns de forma confi√°vel e eficiente. Baseado no Google File System (GFS), o HDFS √© otimizado para throughput alto ao inv√©s de baixa lat√™ncia [7].

#### **Componentes Principais do HDFS:**

**NameNode (N√≥ Mestre)**
O NameNode √© o componente central do HDFS que gerencia o namespace do sistema de arquivos e regula o acesso aos arquivos pelos clientes. Suas responsabilidades incluem:

- **Metadados**: Armazena informa√ß√µes sobre a estrutura de diret√≥rios, permiss√µes de arquivos e localiza√ß√£o dos blocos
- **Mapeamento de blocos**: Mant√©m o mapeamento de quais DataNodes cont√™m quais blocos de dados
- **Opera√ß√µes de namespace**: Executa opera√ß√µes como abrir, fechar, renomear arquivos e diret√≥rios
- **Replica√ß√£o**: Determina a estrat√©gia de replica√ß√£o de blocos para garantir disponibilidade

**DataNodes (N√≥s Escravos)**
Os DataNodes s√£o respons√°veis pelo armazenamento real dos dados e executam opera√ß√µes de leitura e escrita conforme solicitado pelos clientes. Suas fun√ß√µes incluem:

- **Armazenamento de blocos**: Cada arquivo √© dividido em blocos (padr√£o de 128MB) armazenados nos DataNodes
- **Replica√ß√£o**: Mant√©m m√∫ltiplas c√≥pias de cada bloco (padr√£o de 3 r√©plicas)
- **Heartbeat**: Envia sinais peri√≥dicos ao NameNode para confirmar que est√° operacional
- **Relat√≥rios de blocos**: Informa ao NameNode sobre os blocos que possui

#### **Caracter√≠sticas T√©cnicas do HDFS:**

**Toler√¢ncia a Falhas**
O HDFS implementa toler√¢ncia a falhas atrav√©s de m√∫ltiplas estrat√©gias:
- **Replica√ß√£o de dados**: Cada bloco √© replicado em m√∫ltiplos DataNodes (padr√£o: 3 r√©plicas)
- **Detec√ß√£o de falhas**: Monitoramento cont√≠nuo da sa√∫de dos n√≥s atrav√©s de heartbeats
- **Recupera√ß√£o autom√°tica**: Re-replica√ß√£o autom√°tica quando um n√≥ falha
- **Checksums**: Verifica√ß√£o de integridade dos dados para detectar corrup√ß√£o

**Escalabilidade**
O HDFS escala horizontalmente adicionando mais DataNodes ao cluster:
- **Capacidade linear**: Cada novo DataNode adiciona capacidade de armazenamento e throughput
- **Balanceamento autom√°tico**: Redistribui√ß√£o autom√°tica de dados para otimizar utiliza√ß√£o
- **Suporte a milhares de n√≥s**: Clusters com mais de 4.000 n√≥s em produ√ß√£o

### 2.3 MapReduce

![MapReduce Workflow](https://private-us-east-1.manuscdn.com/sessionFile/Kv3nXRCXCLHmEa8ubHl8P8/sandbox/WewAvHF1P8YRJu9LP7jPJa-images_1756742930683_na1fn_L2hvbWUvdWJ1bnR1L3VwbG9hZC9zZWFyY2hfaW1hZ2VzL2UzdXAyUlA3NWExMw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvS3YzblhSQ1hDTEhtRWE4dWJIbDhQOC9zYW5kYm94L1dld0F2SEYxUDhZUkp1OUxQN2pQSmEtaW1hZ2VzXzE3NTY3NDI5MzA2ODNfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwzVndiRzloWkM5elpXRnlZMmhmYVcxaFoyVnpMMlV6ZFhBeVVsQTNOV0V4TXcucG5nIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzk4NzYxNjAwfX19XX0_&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=MCDSkODfXG5S-aWLWOOFCpuiQIaL8wMNB23P9mYLKlbFnxR8K8IC6jj-fYfACh~doQWnfLkyekHdylyUGn2Ae0ZLuJO0R3jid79mo73XFWzfvgB53Eq1uNIMFLFTFLJmJSzgTQKAk~OK7Nkisoit2t8F3AqdB3Rw4oEtAKXHNpTmXBXkXKXW23oNB8nCfFZb4MH-pTq6RBm4yQcy5RrSAukYW8g-YwU038RvEt58prK7fZXyldvCoMWh-DikSuUYvVWwKLchTNJikZbodZJXclajAvpGuJW8KxKMOeBxNhan1VSFadtAK2-cw47K4tH643lQhD5XOUHUpc2Op3wp1A__)

MapReduce √© um modelo de programa√ß√£o e uma implementa√ß√£o associada para processar e gerar grandes conjuntos de dados de forma distribu√≠da. Inspirado nas fun√ß√µes map e reduce da programa√ß√£o funcional, o MapReduce divide o processamento em duas fases principais [8].

#### **Fases do MapReduce:**

**Fase Map**
A fase Map processa dados de entrada em paralelo, aplicando uma fun√ß√£o definida pelo usu√°rio a cada registro:

```
Map(chave1, valor1) ‚Üí lista(chave2, valor2)
```

- **Paraleliza√ß√£o**: Cada mapper processa uma por√ß√£o dos dados de entrada independentemente
- **Transforma√ß√£o**: Converte dados de entrada em pares chave-valor intermedi√°rios
- **Localidade de dados**: Mappers s√£o executados preferencialmente nos n√≥s que cont√™m os dados

**Fase Shuffle e Sort**
Entre as fases Map e Reduce, o sistema executa shuffle e sort:
- **Shuffle**: Redistribui dados intermedi√°rios baseado nas chaves
- **Sort**: Ordena dados por chave para otimizar o processamento Reduce
- **Particionamento**: Determina qual Reducer processar√° cada chave

**Fase Reduce**
A fase Reduce agrega os valores intermedi√°rios por chave:

```
Reduce(chave2, lista(valor2)) ‚Üí lista(chave3, valor3)
```

- **Agrega√ß√£o**: Combina todos os valores associados a uma chave espec√≠fica
- **Paraleliza√ß√£o**: M√∫ltiplos reducers processam diferentes chaves simultaneamente
- **Sa√≠da final**: Produz o resultado final do processamento

#### **Exemplo Pr√°tico: WordCount**

O WordCount √© o "Hello World" do MapReduce, demonstrando como contar palavras em documentos:

**Entrada:**
```
"Hello World"
"Hello Hadoop"
```

**Fase Map:**
```
Mapper 1: "Hello World" ‚Üí [("Hello", 1), ("World", 1)]
Mapper 2: "Hello Hadoop" ‚Üí [("Hello", 1), ("Hadoop", 1)]
```

**Fase Shuffle:**
```
"Hello" ‚Üí [1, 1]
"World" ‚Üí [1]
"Hadoop" ‚Üí [1]
```

**Fase Reduce:**
```
Reducer 1: ("Hello", [1, 1]) ‚Üí ("Hello", 2)
Reducer 2: ("World", [1]) ‚Üí ("World", 1)
Reducer 3: ("Hadoop", [1]) ‚Üí ("Hadoop", 1)
```

### 2.4 YARN (Yet Another Resource Negotiator)

YARN √© o sistema de gerenciamento de recursos do Hadoop 2.x que separa as fun√ß√µes de gerenciamento de recursos e agendamento de jobs da l√≥gica de processamento de dados. Esta separa√ß√£o permite que o Hadoop suporte m√∫ltiplos frameworks de processamento al√©m do MapReduce [9].

#### **Componentes do YARN:**

**ResourceManager (RM)**
O ResourceManager √© o componente central que gerencia recursos em todo o cluster:
- **Agendamento global**: Aloca recursos para aplica√ß√µes baseado em pol√≠ticas configuradas
- **Monitoramento**: Rastreia recursos dispon√≠veis e utilizados em cada n√≥
- **Gerenciamento de aplica√ß√µes**: Coordena o ciclo de vida das aplica√ß√µes

**NodeManager (NM)**
Cada n√≥ do cluster executa um NodeManager que gerencia recursos locais:
- **Monitoramento local**: Monitora uso de CPU, mem√≥ria e disco no n√≥
- **Execu√ß√£o de containers**: Inicia e monitora containers de aplica√ß√µes
- **Relat√≥rios**: Envia informa√ß√µes de status para o ResourceManager

**ApplicationMaster (AM)**
Cada aplica√ß√£o tem seu pr√≥prio ApplicationMaster que coordena sua execu√ß√£o:
- **Negocia√ß√£o de recursos**: Solicita recursos necess√°rios ao ResourceManager
- **Coordena√ß√£o de tarefas**: Gerencia a execu√ß√£o de tarefas da aplica√ß√£o
- **Monitoramento**: Monitora progresso e trata falhas de tarefas

#### **Vantagens do YARN:**

**Utiliza√ß√£o Eficiente de Recursos**
- **Compartilhamento din√¢mico**: Recursos s√£o alocados dinamicamente baseado na demanda
- **M√∫ltiplos frameworks**: Suporte simult√¢neo a MapReduce, Spark, Storm, etc.
- **Isolamento**: Aplica√ß√µes s√£o isoladas em containers para evitar interfer√™ncia

**Escalabilidade Melhorada**
- **Descentraliza√ß√£o**: ApplicationMasters distribuem a carga de gerenciamento
- **Suporte a clusters maiores**: Clusters com mais de 10.000 n√≥s
- **Melhor throughput**: Redu√ß√£o de gargalos de comunica√ß√£o

### 2.5 Ecossistema Hadoop

![Ecossistema Hadoop](https://private-us-east-1.manuscdn.com/sessionFile/Kv3nXRCXCLHmEa8ubHl8P8/sandbox/WewAvHF1P8YRJu9LP7jPJa-images_1756742930684_na1fn_L2hvbWUvdWJ1bnR1L3VwbG9hZC9zZWFyY2hfaW1hZ2VzL1pMTVRtRzg0UlBMdg.jpg?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvS3YzblhSQ1hDTEhtRWE4dWJIbDhQOC9zYW5kYm94L1dld0F2SEYxUDhZUkp1OUxQN2pQSmEtaW1hZ2VzXzE3NTY3NDI5MzA2ODRfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwzVndiRzloWkM5elpXRnlZMmhmYVcxaFoyVnpMMXBNVFZSdFJ6ZzBVbEJNZGcuanBnIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzk4NzYxNjAwfX19XX0_&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=rBHRWpMlEdQs2u6~29OMvGO0Lqa3~l6-awIb7nEsf5J19Fajo7L1Nu1d~Qoj98gODPCGR8VQwchhb4IK1ogeoz-2V-R3e840Fc4wodWsGa3X13jCRkfziQ~~-sBoEqLhSbawH622RixNHvMGylKbpFwyext4kYr5rqmRXOeVyIipi3l~4Qcwh437Tz7NuG~VCFzHyTojqNWR9aqtZwnniOgYjecWp7UapHZCiTV5jeeV2K1UBqAXd~A~ftE-Or8ovwXuPj9A9auUP04qfC3r0SQ9sB4m7oZFqH3f-GdNo-qK0VVmVlpsuFKTt8si5816-~xAzrpk7ftH~9kfena3xg__)

O ecossistema Hadoop √© uma cole√ß√£o de ferramentas e frameworks que estendem as capacidades b√°sicas do Hadoop para diferentes casos de uso. Em 2025, este ecossistema inclui mais de 100 projetos ativos [10].

#### **Componentes Principais do Ecossistema:**

**Armazenamento e Gest√£o de Dados**
- **HBase**: Banco de dados NoSQL distribu√≠do para acesso aleat√≥rio em tempo real
- **Cassandra**: Banco de dados distribu√≠do para alta disponibilidade
- **MongoDB**: Banco de dados de documentos para dados semi-estruturados

**Processamento de Dados**
- **Apache Spark**: Engine de processamento unificado para batch e streaming
- **Apache Storm**: Processamento de streams em tempo real
- **Apache Flink**: Processamento de streams com baixa lat√™ncia

**An√°lise e Consulta**
- **Apache Hive**: Data warehouse para consultas SQL em dados Hadoop
- **Apache Pig**: Linguagem de alto n√≠vel para an√°lise de dados
- **Presto**: Engine de consulta SQL distribu√≠da para an√°lise interativa

**Ingest√£o de Dados**
- **Apache Flume**: Coleta e agrega√ß√£o de dados de logs
- **Apache Sqoop**: Transfer√™ncia de dados entre Hadoop e bancos relacionais
- **Apache Kafka**: Plataforma de streaming distribu√≠da

**Coordena√ß√£o e Workflow**
- **Apache Zookeeper**: Servi√ßo de coordena√ß√£o para aplica√ß√µes distribu√≠das
- **Apache Oozie**: Sistema de workflow para jobs Hadoop
- **Apache Airflow**: Plataforma para desenvolvimento e agendamento de workflows

#### **Integra√ß√£o com Tecnologias Modernas (2025)**

**Intelig√™ncia Artificial e Machine Learning**
- **TensorFlow on Hadoop**: Treinamento distribu√≠do de modelos de deep learning
- **MLlib (Spark)**: Biblioteca de machine learning escal√°vel
- **H2O.ai**: Plataforma de machine learning em mem√≥ria

**Cloud Computing**
- **Amazon EMR**: Hadoop gerenciado na AWS
- **Google Cloud Dataproc**: Hadoop e Spark gerenciados no Google Cloud
- **Azure HDInsight**: Hadoop gerenciado no Microsoft Azure

**Containers e Orquestra√ß√£o**
- **Kubernetes**: Orquestra√ß√£o de containers para aplica√ß√µes Hadoop
- **Docker**: Containeriza√ß√£o de componentes do ecossistema
- **Apache Mesos**: Gerenciamento de recursos para datacenters

---


## 3. PR√â-REQUISITOS E PREPARA√á√ÉO DO AMBIENTE

### 3.1 Requisitos de Sistema

Antes de iniciar a instala√ß√£o do Hadoop, √© fundamental verificar se o sistema atende aos requisitos m√≠nimos e recomendados para uma opera√ß√£o eficiente.

#### **Requisitos de Hardware**

**Configura√ß√£o M√≠nima (Ambiente de Desenvolvimento/Teste):**
- **CPU**: 2 cores, 2.0 GHz
- **RAM**: 4 GB (m√≠nimo), 8 GB (recomendado)
- **Armazenamento**: 20 GB de espa√ßo livre
- **Rede**: Conex√£o de internet est√°vel

**Configura√ß√£o Recomendada (Ambiente de Produ√ß√£o):**
- **CPU**: 8+ cores, 2.4+ GHz
- **RAM**: 32+ GB
- **Armazenamento**: 1+ TB SSD/NVMe para logs, 10+ TB HDD para dados
- **Rede**: Gigabit Ethernet ou superior

#### **Requisitos de Software**

**Sistema Operacional Suportado:**
- **Linux**: Ubuntu 18.04+, CentOS 7+, Red Hat Enterprise Linux 7+
- **Windows**: Windows 10/11 (com WSL2 recomendado)
- **macOS**: macOS 10.14+ (para desenvolvimento apenas)

**Depend√™ncias Obrigat√≥rias:**
- **Java**: OpenJDK 8 ou Oracle JDK 8/11 (Java 17+ suportado no Hadoop 3.3+)
- **SSH**: OpenSSH para comunica√ß√£o entre n√≥s
- **Python**: Python 3.7+ para scripts e ferramentas auxiliares

### 3.2 Conhecimentos Pr√©vios Necess√°rios

Para aproveitar ao m√°ximo este laborat√≥rio, √© recomend√°vel ter conhecimento b√°sico em:

#### **Conceitos Fundamentais**
- **Sistemas distribu√≠dos**: Compreens√£o b√°sica de como sistemas distribu√≠dos funcionam
- **Linha de comando Linux**: Navega√ß√£o, manipula√ß√£o de arquivos, permiss√µes
- **Redes de computadores**: Conceitos b√°sicos de TCP/IP, portas, firewalls
- **Programa√ß√£o**: Conhecimento b√°sico em Java ou Python √© √∫til

#### **Conceitos de Big Data**
- **Processamento batch vs. streaming**: Diferen√ßas entre processamento em lote e tempo real
- **Dados estruturados vs. n√£o estruturados**: Tipos de dados e suas caracter√≠sticas
- **Escalabilidade horizontal vs. vertical**: Estrat√©gias de crescimento de sistemas

### 3.3 Ferramentas Auxiliares

#### **Editores de Texto Recomendados**
- **nano**: Editor simples para configura√ß√µes r√°pidas
- **vim/vi**: Editor avan√ßado para usu√°rios experientes
- **Visual Studio Code**: IDE com extens√µes para Big Data

#### **Ferramentas de Monitoramento**
- **htop**: Monitor de processos e recursos do sistema
- **iotop**: Monitor de I/O de disco
- **netstat**: Monitor de conex√µes de rede

## 4. INSTALA√á√ÉO DO HADOOP

### 4.1 M√©todos de Instala√ß√£o

Existem v√°rias abordagens para instalar o Hadoop, cada uma adequada para diferentes cen√°rios e n√≠veis de experi√™ncia.

#### **M√©todos Dispon√≠veis:**

**1. Instala√ß√£o Manual**
- **Vantagens**: Controle total sobre configura√ß√£o, aprendizado profundo
- **Desvantagens**: Complexa, propensa a erros, demorada
- **Recomendado para**: Usu√°rios avan√ßados, ambientes de produ√ß√£o customizados

**2. Distribui√ß√µes Comerciais**
- **Cloudera**: Cloudera Data Platform (CDP)
- **Hortonworks**: Hortonworks Data Platform (HDP) - agora parte da Cloudera
- **MapR**: MapR Data Platform - descontinuado em 2019
- **Vantagens**: Suporte comercial, ferramentas de gerenciamento, integra√ß√£o facilitada
- **Desvantagens**: Custo, depend√™ncia de fornecedor

**3. Instala√ß√£o com Scripts Automatizados**
- **Vantagens**: R√°pida, reduz erros, reproduz√≠vel
- **Desvantagens**: Menos flexibilidade, pode ocultar detalhes importantes
- **Recomendado para**: Ambientes de aprendizado, prototipagem r√°pida

**4. Containers e Cloud**
- **Docker**: Containers pr√©-configurados
- **Kubernetes**: Orquestra√ß√£o de clusters Hadoop
- **Cloud Services**: EMR (AWS), Dataproc (GCP), HDInsight (Azure)

### 4.2 Instala√ß√£o Automatizada com Scripts

Para este laborat√≥rio, utilizaremos os scripts desenvolvidos pelo Professor Vagner Cordeiro, que automatizam o processo de instala√ß√£o e configura√ß√£o do Hadoop em modo pseudo-distribu√≠do. Esta abordagem √© ideal para aprendizado e desenvolvimento.

> **‚ö†Ô∏è ATEN√á√ÉO IMPORTANTE:**
> A instala√ß√£o manual do Hadoop pode ser extremamente complexa devido √†s m√∫ltiplas depend√™ncias, configura√ß√µes espec√≠ficas de sistema operacional e vers√µes de software que mudam constantemente. Os scripts automatizados eliminam a maioria desses problemas, permitindo foco no aprendizado dos conceitos fundamentais.

#### **Vis√£o Geral dos Scripts**

O Professor Vagner Cordeiro desenvolveu tr√™s scripts que automatizam todo o processo de instala√ß√£o:

**1. script_hadoop1.sh (prepare_hadoop.sh)**
- **Fun√ß√£o**: Prepara√ß√£o do ambiente e download do Hadoop
- **A√ß√µes executadas**:
  - Limpeza de instala√ß√µes anteriores
  - Verifica√ß√£o de conectividade com internet
  - Checagem de recursos do sistema (mem√≥ria e espa√ßo em disco)
  - Instala√ß√£o de depend√™ncias (Java 8, SSH)
  - Download do Hadoop 3.4.1 (vers√£o otimizada)
  - Verifica√ß√£o de integridade do arquivo baixado

**2. script_hadoop2.sh**
- **Fun√ß√£o**: Instala√ß√£o e configura√ß√£o do Hadoop
- **A√ß√µes executadas**:
  - Descompacta√ß√£o do arquivo Hadoop
  - Configura√ß√£o do SSH para acesso sem senha (passwordless)
  - Configura√ß√£o dos arquivos de configura√ß√£o do Hadoop
  - Formata√ß√£o do NameNode
  - Inicializa√ß√£o dos servi√ßos Hadoop
  - Verifica√ß√£o da instala√ß√£o

**3. setup_exercises.sh**
- **Fun√ß√£o**: Configura√ß√£o dos exerc√≠cios pr√°ticos
- **A√ß√µes executadas**:
  - Cria√ß√£o de diret√≥rios para exerc√≠cios
  - C√≥pia de arquivos de configura√ß√£o para o HDFS
  - Prepara√ß√£o de dados de exemplo
  - Configura√ß√£o de exerc√≠cios de MapReduce

#### **Passo a Passo da Instala√ß√£o**

**ETAPA 1: Download dos Scripts**

Acesse o link fornecido pelo professor para baixar os tr√™s scripts necess√°rios:

```
https://drive.google.com/drive/folders/1C7hRMz7N1OJrYxk7vIA-E9qyiw5ZLEpu?usp=sharing
```

Os arquivos que voc√™ deve baixar s√£o:
- `script_hadoop1.sh` (ou `prepare_hadoop.sh`)
- `script_hadoop2.sh`
- `setup_exercises.sh`

**ETAPA 2: Configura√ß√£o de Permiss√µes**

Ap√≥s o download, √© necess√°rio dar permiss√µes de execu√ß√£o aos scripts:

```bash
# Navegue at√© o diret√≥rio onde os scripts foram baixados
cd ~/Downloads

# Conceda permiss√µes de execu√ß√£o
chmod +x script_hadoop1.sh
chmod +x script_hadoop2.sh
chmod +x setup_exercises.sh

# Verificar permiss√µes (opcional)
ls -la *.sh
```

**ETAPA 3: Execu√ß√£o do Primeiro Script**

Execute o script de prepara√ß√£o do ambiente:

```bash
# Execute o primeiro script
sudo ./script_hadoop1.sh
```

**O que acontece durante a execu√ß√£o:**
- O script verifica se h√° instala√ß√µes anteriores do Hadoop e as remove
- Testa a conectividade com a internet
- Verifica se h√° mem√≥ria suficiente (m√≠nimo 2GB) e espa√ßo em disco (m√≠nimo 10GB)
- Instala o Java 8 (OpenJDK) se n√£o estiver presente
- Configura as vari√°veis de ambiente JAVA_HOME
- Instala e configura o SSH
- Baixa o Hadoop 3.4.1 do reposit√≥rio oficial Apache
- Verifica a integridade do arquivo baixado usando checksums

**Indicadores de sucesso:**
- Mensagem "Hadoop download completed successfully"
- Arquivo `/tmp/hadoop.tar.gz` criado
- Nenhuma mensagem de erro durante a execu√ß√£o

**ETAPA 4: Execu√ß√£o do Segundo Script**

Execute o script de instala√ß√£o e configura√ß√£o:

```bash
# Execute o segundo script
sudo ./script_hadoop2.sh
```

**O que acontece durante a execu√ß√£o:**
- Descompacta√ß√£o do Hadoop em `/usr/local/hadoop`
- Configura√ß√£o do usu√°rio hadoop (se necess√°rio)
- Gera√ß√£o de chaves SSH para comunica√ß√£o sem senha
- Configura√ß√£o dos arquivos principais do Hadoop:
  - `core-site.xml`: Configura√ß√µes centrais do Hadoop
  - `hdfs-site.xml`: Configura√ß√µes espec√≠ficas do HDFS
  - `mapred-site.xml`: Configura√ß√µes do MapReduce
  - `yarn-site.xml`: Configura√ß√µes do YARN
- Formata√ß√£o do sistema de arquivos HDFS
- Inicializa√ß√£o dos servi√ßos:
  - NameNode
  - DataNode
  - ResourceManager
  - NodeManager

**Indicadores de sucesso:**
- Mensagem "Hadoop installation completed successfully"
- Servi√ßos Hadoop rodando (verific√°vel com `jps`)
- Interface web acess√≠vel em `http://localhost:9870`

### 4.3 Verifica√ß√£o da Instala√ß√£o

Ap√≥s a execu√ß√£o dos scripts, √© importante verificar se a instala√ß√£o foi bem-sucedida.

#### **Verifica√ß√£o dos Processos Java**

Use o comando `jps` para verificar se os processos Hadoop est√£o rodando:

```bash
jps
```

**Sa√≠da esperada:**
```
12345 NameNode
12346 DataNode
12347 ResourceManager
12348 NodeManager
12349 Jps
```

#### **Verifica√ß√£o dos Arquivos de Configura√ß√£o**

Verifique o conte√∫do do arquivo `core-site.xml`:

```bash
cat /usr/local/hadoop/etc/hadoop/core-site.xml
```

**Conte√∫do esperado:**
```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

#### **Verifica√ß√£o da Interface Web**

Acesse a interface web do Hadoop atrav√©s do navegador:
- **NameNode**: http://localhost:9870
- **ResourceManager**: http://localhost:8088

A interface deve mostrar informa√ß√µes sobre o cluster, incluindo:
- Status dos n√≥s
- Capacidade de armazenamento
- Jobs em execu√ß√£o
- M√©tricas de performance

#### **Teste B√°sico do HDFS**

Execute comandos b√°sicos para testar o HDFS:

```bash
# Criar diret√≥rio no HDFS
/usr/local/hadoop/bin/hdfs dfs -mkdir /test

# Listar conte√∫do do diret√≥rio raiz
/usr/local/hadoop/bin/hdfs dfs -ls /

# Verificar status do sistema de arquivos
/usr/local/hadoop/bin/hdfs dfsadmin -report
```

### 4.4 Troubleshooting Comum

#### **Problema: Java n√£o encontrado**

**Sintomas:**
- Erro "JAVA_HOME is not set"
- Comandos Hadoop falham com erro de Java

**Solu√ß√£o:**
```bash
# Verificar se Java est√° instalado
java -version

# Se n√£o estiver instalado
sudo apt update
sudo apt install openjdk-8-jdk

# Configurar JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' >> ~/.bashrc
```

#### **Problema: SSH n√£o configurado**

**Sintomas:**
- Erro "Permission denied (publickey)"
- Falha na inicializa√ß√£o dos servi√ßos

**Solu√ß√£o:**
```bash
# Gerar chaves SSH
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

# Adicionar chave p√∫blica ao authorized_keys
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# Configurar permiss√µes
chmod 0600 ~/.ssh/authorized_keys

# Testar conex√£o SSH
ssh localhost
```

#### **Problema: Portas em uso**

**Sintomas:**
- Erro "Address already in use"
- Servi√ßos n√£o inicializam

**Solu√ß√£o:**
```bash
# Verificar portas em uso
netstat -tulpn | grep :9000
netstat -tulpn | grep :9870

# Matar processos usando as portas (se necess√°rio)
sudo kill -9 <PID>

# Reiniciar servi√ßos Hadoop
/usr/local/hadoop/sbin/stop-all.sh
/usr/local/hadoop/sbin/start-all.sh
```

#### **Problema: Espa√ßo em disco insuficiente**

**Sintomas:**
- Erro "No space left on device"
- HDFS reporta 0% de espa√ßo dispon√≠vel

**Solu√ß√£o:**
```bash
# Verificar espa√ßo em disco
df -h

# Limpar logs antigos
sudo rm -rf /usr/local/hadoop/logs/*

# Limpar arquivos tempor√°rios
sudo rm -rf /tmp/hadoop-*

# Verificar configura√ß√£o de espa√ßo reservado no HDFS
/usr/local/hadoop/bin/hdfs dfsadmin -report
```

---


## 5. EXERC√çCIOS PR√ÅTICOS

### 5.1 Prepara√ß√£o dos Exerc√≠cios

Antes de iniciar os exerc√≠cios pr√°ticos, execute o script de configura√ß√£o dos exerc√≠cios:

```bash
# Execute o terceiro script
sudo ./setup_exercises.sh
```

**O que o script faz:**
- Cria a estrutura de diret√≥rios para os exerc√≠cios
- Configura arquivos de entrada no HDFS
- Prepara dados de exemplo para os exerc√≠cios
- Cria o diret√≥rio `/exercicios_alunos/vagner_cordeiro`

**Navega√ß√£o para o diret√≥rio de exerc√≠cios:**
```bash
cd exercicios_alunos/vagner_cordeiro
```

### 5.2 Exerc√≠cio 1: Grep (Busca de Padr√µes)

#### **Objetivo do Exerc√≠cio**
O exerc√≠cio Grep demonstra como usar o MapReduce para buscar padr√µes espec√≠ficos em arquivos de texto. Este √© um exemplo fundamental que ilustra como o MapReduce pode ser usado para filtrar e processar dados baseado em crit√©rios espec√≠ficos.

#### **Conceitos Aprendidos**
- **Processamento de texto**: Como o MapReduce processa arquivos de texto linha por linha
- **Express√µes regulares**: Uso de regex para busca de padr√µes complexos
- **Filtragem distribu√≠da**: Como filtros s√£o aplicados em paralelo em um cluster
- **Agrega√ß√£o de resultados**: Como resultados de m√∫ltiplos mappers s√£o combinados

#### **Cen√°rio Pr√°tico**
Imagine que voc√™ trabalha em uma empresa de telecomunica√ß√µes e precisa analisar logs de configura√ß√£o de equipamentos de rede para encontrar todas as configura√ß√µes relacionadas ao sistema de arquivos distribu√≠do (DFS). Este tipo de an√°lise √© comum em:
- **Auditoria de sistemas**: Verifica√ß√£o de configura√ß√µes de seguran√ßa
- **Troubleshooting**: Identifica√ß√£o de problemas em configura√ß√µes
- **Compliance**: Verifica√ß√£o de conformidade com pol√≠ticas corporativas

#### **Execu√ß√£o do Exerc√≠cio**

**Comando:**
```bash
/usr/local/hadoop/bin/hadoop jar \
/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar \
grep /user/root/input /user/root/output 'dfs[a-z.]+'
```

**Explica√ß√£o detalhada do comando:**
- `hadoop jar`: Executa um arquivo JAR no Hadoop
- `hadoop-mapreduce-examples-3.4.1.jar`: Arquivo contendo exemplos pr√©-compilados
- `grep`: Nome da classe/programa a ser executado
- `/user/root/input`: Diret√≥rio de entrada no HDFS contendo os arquivos a serem processados
- `/user/root/output`: Diret√≥rio de sa√≠da onde os resultados ser√£o armazenados
- `'dfs[a-z.]+'`: Express√£o regular que busca palavras come√ßando com "dfs" seguidas de letras min√∫sculas ou pontos

**O que acontece internamente:**

**Fase Map:**
```java
// Pseudoc√≥digo do Mapper
for each line in input_file:
    if line.matches(".*dfs[a-z.]+.*"):
        for each word in line:
            if word.matches("dfs[a-z.]+"):
                emit(word, 1)
```

**Fase Reduce:**
```java
// Pseudoc√≥digo do Reducer
for each unique_word:
    total_count = sum(all_counts_for_word)
    emit(unique_word, total_count)
```

#### **Visualiza√ß√£o dos Resultados**

**Comando para ver resultados:**
```bash
/usr/local/hadoop/bin/hdfs dfs -cat /user/root/output/*
```

**Exemplo de sa√≠da esperada:**
```
dfs.blocksize	1
dfs.namenode.http-address	1
dfs.namenode.name.dir	1
dfs.replication	1
dfs.datanode.data.dir	1
```

**Interpreta√ß√£o dos resultados:**
- Cada linha mostra uma configura√ß√£o DFS encontrada e quantas vezes apareceu
- `dfs.replication 1`: A configura√ß√£o de replica√ß√£o apareceu uma vez
- `dfs.namenode.http-address 1`: O endere√ßo HTTP do NameNode apareceu uma vez

#### **C√≥pia para Sistema Local (Opcional)**
```bash
# Copiar resultados para o sistema de arquivos local
/usr/local/hadoop/bin/hdfs dfs -get /user/root/output output_grep

# Visualizar resultados localmente
cat output_grep/*
```

### 5.3 Exerc√≠cio 2: WordCount (Contagem de Palavras)

#### **Objetivo do Exerc√≠cio**
O WordCount √© o exemplo mais cl√°ssico de MapReduce, demonstrando como contar a frequ√™ncia de palavras em documentos. Este exerc√≠cio √© fundamental para entender os conceitos de Map e Reduce.

#### **Conceitos Aprendidos**
- **Tokeniza√ß√£o**: Como texto √© dividido em palavras individuais
- **Agrega√ß√£o distribu√≠da**: Como contagens s√£o somadas em paralelo
- **Particionamento**: Como dados s√£o distribu√≠dos entre reducers
- **Combiners**: Otimiza√ß√£o que reduz tr√°fego de rede

#### **Cen√°rio Pr√°tico**
Este tipo de processamento √© usado em:
- **An√°lise de sentimentos**: Contagem de palavras positivas/negativas em reviews
- **SEO e marketing**: An√°lise de frequ√™ncia de palavras-chave em conte√∫do
- **Pesquisa acad√™mica**: An√°lise de frequ√™ncia de termos em literatura cient√≠fica
- **Detec√ß√£o de spam**: Identifica√ß√£o de palavras comuns em emails spam

#### **Execu√ß√£o do Exerc√≠cio**

**Comando:**
```bash
/usr/local/hadoop/bin/hadoop jar \
/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar \
wordcount /user/root/input/input.txt /user/root/output_wordcount
```

**Explica√ß√£o detalhada:**
- `wordcount`: Programa que conta frequ√™ncia de palavras
- `/user/root/input/input.txt`: Arquivo espec√≠fico de entrada
- `/user/root/output_wordcount`: Diret√≥rio de sa√≠da para este exerc√≠cio

**Processo detalhado do WordCount:**

**Entrada de exemplo:**
```
O Hadoop facilita o processamento de grandes volumes de dados.
O Hadoop √© uma ferramenta poderosa para Big Data.
```

**Fase Map:**
```
Mapper 1: "O Hadoop facilita o processamento..."
‚Üí [("O", 1), ("Hadoop", 1), ("facilita", 1), ("o", 1), ("processamento", 1), ...]

Mapper 2: "O Hadoop √© uma ferramenta..."
‚Üí [("O", 1), ("Hadoop", 1), ("√©", 1), ("uma", 1), ("ferramenta", 1), ...]
```

**Fase Shuffle e Sort:**
```
"O" ‚Üí [1, 1]
"Hadoop" ‚Üí [1, 1]
"o" ‚Üí [1]
"processamento" ‚Üí [1]
"√©" ‚Üí [1]
"uma" ‚Üí [1]
...
```

**Fase Reduce:**
```
Reducer 1: ("O", [1, 1]) ‚Üí ("O", 2)
Reducer 2: ("Hadoop", [1, 1]) ‚Üí ("Hadoop", 2)
Reducer 3: ("o", [1]) ‚Üí ("o", 1)
...
```

#### **Visualiza√ß√£o dos Resultados**

**Comando:**
```bash
/usr/local/hadoop/bin/hdfs dfs -cat /user/root/output_wordcount/*
```

**Exemplo de sa√≠da:**
```
Big	1
Data	1
Hadoop	2
O	2
de	1
dados	1
√©	1
facilita	1
ferramenta	1
grandes	1
o	1
para	1
poderosa	1
processamento	1
uma	1
volumes	1
```

**An√°lise dos resultados:**
- Palavras s√£o ordenadas alfabeticamente
- "Hadoop" e "O" aparecem 2 vezes cada
- Outras palavras aparecem 1 vez cada
- Case-sensitive: "O" e "o" s√£o contados separadamente

#### **C√≥pia para Sistema Local**
```bash
/usr/local/hadoop/bin/hdfs dfs -get /user/root/output_wordcount output_wordcount_local
cat output_wordcount_local/*
```

### 5.4 Exerc√≠cio 3: An√°lise de Logs (Novo)

#### **Objetivo do Exerc√≠cio**
Este exerc√≠cio avan√ßado demonstra como processar logs de servidor web para extrair estat√≠sticas √∫teis, simulando um cen√°rio real de an√°lise de Big Data.

#### **Prepara√ß√£o dos Dados**

Primeiro, vamos criar um arquivo de log simulado:

```bash
# Criar arquivo de log de exemplo
cat > webserver.log << 'EOF'
192.168.1.100 - - [10/Jan/2025:13:55:36 +0000] "GET /index.html HTTP/1.1" 200 2326
192.168.1.101 - - [10/Jan/2025:13:55:37 +0000] "GET /about.html HTTP/1.1" 200 1024
192.168.1.102 - - [10/Jan/2025:13:55:38 +0000] "POST /login HTTP/1.1" 404 512
192.168.1.100 - - [10/Jan/2025:13:55:39 +0000] "GET /products.html HTTP/1.1" 200 4096
192.168.1.103 - - [10/Jan/2025:13:55:40 +0000] "GET /index.html HTTP/1.1" 200 2326
192.168.1.101 - - [10/Jan/2025:13:55:41 +0000] "GET /contact.html HTTP/1.1" 500 256
EOF

# Copiar para HDFS
/usr/local/hadoop/bin/hdfs dfs -put webserver.log /user/root/input/
```

#### **An√°lise de C√≥digos de Status HTTP**

Vamos criar um programa MapReduce personalizado para contar c√≥digos de status HTTP:

```bash
# Usar grep para extrair c√≥digos de status
/usr/local/hadoop/bin/hadoop jar \
/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar \
grep /user/root/input/webserver.log /user/root/output_status_codes '[0-9]{3}'
```

**Visualizar resultados:**
```bash
/usr/local/hadoop/bin/hdfs dfs -cat /user/root/output_status_codes/*
```

#### **An√°lise de IPs Mais Frequentes**

```bash
# Extrair IPs usando grep
/usr/local/hadoop/bin/hadoop jar \
/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar \
grep /user/root/input/webserver.log /user/root/output_ips '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+'
```

### 5.5 Exerc√≠cio 4: Processamento de Dados CSV (Novo)

#### **Objetivo do Exerc√≠cio**
Demonstrar como processar dados estruturados em formato CSV, um caso comum em an√°lise de dados empresariais.

#### **Prepara√ß√£o dos Dados**

Criar arquivo CSV de vendas:

```bash
cat > sales_data.csv << 'EOF'
produto,categoria,preco,quantidade,data
Notebook,Eletr√¥nicos,2500.00,2,2025-01-10
Mouse,Eletr√¥nicos,50.00,10,2025-01-10
Teclado,Eletr√¥nicos,150.00,5,2025-01-10
Cadeira,M√≥veis,800.00,3,2025-01-10
Mesa,M√≥veis,1200.00,1,2025-01-10
Notebook,Eletr√¥nicos,2500.00,1,2025-01-11
Mouse,Eletr√¥nicos,50.00,15,2025-01-11
EOF

# Copiar para HDFS
/usr/local/hadoop/bin/hdfs dfs -put sales_data.csv /user/root/input/
```

#### **An√°lise de Produtos Mais Vendidos**

```bash
# Contar frequ√™ncia de produtos
/usr/local/hadoop/bin/hadoop jar \
/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar \
wordcount /user/root/input/sales_data.csv /user/root/output_products
```

#### **An√°lise de Categorias**

```bash
# Extrair categorias usando grep
/usr/local/hadoop/bin/hadoop jar \
/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar \
grep /user/root/input/sales_data.csv /user/root/output_categories 'Eletr√¥nicos|M√≥veis'
```

### 5.6 Limpeza e Prepara√ß√£o para Novos Exerc√≠cios

#### **Limpeza de Diret√≥rios de Sa√≠da**

Antes de executar novos exerc√≠cios, √© necess√°rio limpar os diret√≥rios de sa√≠da anteriores:

```bash
# Remover diret√≥rios de sa√≠da anteriores
/usr/local/hadoop/bin/hdfs dfs -rm -r /user/root/output
/usr/local/hadoop/bin/hdfs dfs -rm -r /user/root/output_wordcount
/usr/local/hadoop/bin/hdfs dfs -rm -r /user/root/output_status_codes
/usr/local/hadoop/bin/hdfs dfs -rm -r /user/root/output_ips
/usr/local/hadoop/bin/hdfs dfs -rm -r /user/root/output_products
/usr/local/hadoop/bin/hdfs dfs -rm -r /user/root/output_categories
```

#### **Verifica√ß√£o do Espa√ßo no HDFS**

```bash
# Verificar uso do espa√ßo no HDFS
/usr/local/hadoop/bin/hdfs dfs -du -h /user/root/

# Relat√≥rio detalhado do HDFS
/usr/local/hadoop/bin/hdfs dfsadmin -report
```

### 5.7 Exerc√≠cios Desafio (Opcionais)

#### **Desafio 1: An√°lise de Temperatura**

Crie um arquivo com dados de temperatura e use MapReduce para encontrar a temperatura m√°xima por cidade:

```bash
cat > temperature_data.txt << 'EOF'
S√£o Paulo,25.5,2025-01-10
Rio de Janeiro,32.1,2025-01-10
Bras√≠lia,28.3,2025-01-10
S√£o Paulo,26.2,2025-01-11
Rio de Janeiro,33.5,2025-01-11
Bras√≠lia,29.1,2025-01-11
EOF

/usr/local/hadoop/bin/hdfs dfs -put temperature_data.txt /user/root/input/
```

#### **Desafio 2: An√°lise de Redes Sociais**

Simule dados de intera√ß√µes em redes sociais:

```bash
cat > social_media.txt << 'EOF'
user1 likes post123
user2 shares post123
user1 comments post456
user3 likes post123
user2 likes post789
user1 shares post456
EOF

/usr/local/hadoop/bin/hdfs dfs -put social_media.txt /user/root/input/
```

Use WordCount para analisar as a√ß√µes mais comuns (likes, shares, comments).

---


## 6. MONITORAMENTO E INTERFACE WEB

### 6.1 Interface Web do Hadoop

O Hadoop fornece interfaces web intuitivas para monitoramento e gerenciamento do cluster. Essas interfaces s√£o essenciais para administradores e desenvolvedores acompanharem o desempenho e status do sistema.

#### **NameNode Web UI (Porto 9870)**

Acesse: `http://localhost:9870`

**Funcionalidades principais:**
- **Overview**: Status geral do cluster, capacidade de armazenamento, n√∫mero de arquivos
- **Datanodes**: Lista de DataNodes ativos, capacidade individual, status de sa√∫de
- **Browse File System**: Navegador web para o HDFS, permite visualizar arquivos e diret√≥rios
- **Logs**: Logs detalhados do NameNode para troubleshooting

**M√©tricas importantes:**
- **Configured Capacity**: Capacidade total configurada do cluster
- **DFS Used**: Espa√ßo utilizado no HDFS
- **Non DFS Used**: Espa√ßo usado por arquivos n√£o-HDFS
- **DFS Remaining**: Espa√ßo dispon√≠vel no HDFS
- **Block Pool Used**: Espa√ßo usado pelos metadados dos blocos

#### **ResourceManager Web UI (Porto 8088)**

Acesse: `http://localhost:8088`

**Funcionalidades principais:**
- **Applications**: Lista de aplica√ß√µes em execu√ß√£o, conclu√≠das e falhas
- **Cluster**: M√©tricas do cluster, n√≥s ativos, recursos dispon√≠veis
- **Scheduler**: Informa√ß√µes sobre agendamento de recursos
- **Tools**: Ferramentas de configura√ß√£o e logs

**M√©tricas importantes:**
- **Memory Total**: Mem√≥ria total dispon√≠vel no cluster
- **Memory Used**: Mem√≥ria atualmente em uso
- **VCores Total**: N√∫mero total de cores virtuais
- **Active Nodes**: N√∫mero de n√≥s ativos no cluster

### 6.2 Monitoramento de Jobs

#### **Acompanhamento de Jobs MapReduce**

Quando voc√™ executa um job MapReduce, pode acompanhar seu progresso atrav√©s da interface web:

1. **Acesse o ResourceManager** (`http://localhost:8088`)
2. **Clique em "Applications"** para ver jobs em execu√ß√£o
3. **Clique no Application ID** para detalhes espec√≠ficos do job
4. **Monitore as fases**: Map, Shuffle, Reduce

**Informa√ß√µes detalhadas do job:**
- **Progress**: Porcentagem de conclus√£o de cada fase
- **Elapsed Time**: Tempo decorrido desde o in√≠cio
- **Maps/Reduces**: N√∫mero de tasks map e reduce
- **Pending/Running/Complete**: Status das tasks

#### **Logs de Aplica√ß√µes**

Para acessar logs detalhados de um job:

```bash
# Listar aplica√ß√µes recentes
/usr/local/hadoop/bin/yarn application -list

# Ver logs de uma aplica√ß√£o espec√≠fica
/usr/local/hadoop/bin/yarn logs -applicationId application_1234567890123_0001
```

### 6.3 An√°lise de Performance

#### **M√©tricas de Performance do HDFS**

```bash
# Relat√≥rio detalhado do HDFS
/usr/local/hadoop/bin/hdfs dfsadmin -report

# Verificar sa√∫de do sistema de arquivos
/usr/local/hadoop/bin/hdfs fsck /

# Estat√≠sticas de uso por diret√≥rio
/usr/local/hadoop/bin/hdfs dfs -du -h /user/root/
```

#### **M√©tricas de Performance do YARN**

```bash
# Status dos n√≥s do cluster
/usr/local/hadoop/bin/yarn node -list

# Informa√ß√µes sobre filas de recursos
/usr/local/hadoop/bin/yarn queue -status default
```

#### **Comandos de Monitoramento do Sistema**

```bash
# Monitorar processos Java do Hadoop
jps -v

# Monitorar uso de recursos
htop

# Monitorar I/O de disco
iotop

# Monitorar conex√µes de rede
netstat -tulpn | grep java
```

## 7. INTEGRA√á√ÉO COM PYTHON

### 7.1 Bibliotecas Python para Hadoop

O Python oferece v√°rias bibliotecas para interagir com o ecossistema Hadoop, facilitando o desenvolvimento de aplica√ß√µes de Big Data.

#### **PyHDFS - Intera√ß√£o com HDFS**

Instala√ß√£o e uso b√°sico:

```bash
# Instalar PyHDFS
pip3 install hdfs

# Exemplo de uso
python3 << 'EOF'
from hdfs import InsecureClient

# Conectar ao HDFS
client = InsecureClient('http://localhost:9870', user='root')

# Listar arquivos
print("Arquivos no HDFS:")
print(client.list('/user/root/'))

# Ler arquivo
try:
    with client.read('/user/root/input/input.txt') as reader:
        content = reader.read()
        print("Conte√∫do do arquivo:")
        print(content.decode('utf-8'))
except Exception as e:
    print(f"Erro ao ler arquivo: {e}")
EOF
```

#### **MRJob - MapReduce em Python**

Instala√ß√£o:
```bash
pip3 install mrjob
```

Exemplo de WordCount em Python:

```python
# Criar arquivo word_count.py
cat > word_count.py << 'EOF'
from mrjob.job import MRJob
import re

class WordCount(MRJob):
    
    def mapper(self, _, line):
        # Dividir linha em palavras e emitir cada palavra com contagem 1
        words = re.findall(r'\w+', line.lower())
        for word in words:
            yield word, 1
    
    def reducer(self, word, counts):
        # Somar todas as contagens para cada palavra
        yield word, sum(counts)

if __name__ == '__main__':
    WordCount.run()
EOF

# Executar localmente
python3 word_count.py input.txt

# Executar no Hadoop
python3 word_count.py -r hadoop hdfs:///user/root/input/input.txt
```

### 7.2 Exemplos Pr√°ticos com Python

#### **Exemplo 1: An√°lise de Logs com Python**

```python
cat > log_analyzer.py << 'EOF'
from mrjob.job import MRJob
import re

class LogAnalyzer(MRJob):
    
    def mapper(self, _, line):
        # Extrair c√≥digo de status HTTP usando regex
        status_match = re.search(r'" (\d{3}) ', line)
        if status_match:
            status_code = status_match.group(1)
            yield status_code, 1
    
    def reducer(self, status_code, counts):
        yield status_code, sum(counts)

if __name__ == '__main__':
    LogAnalyzer.run()
EOF
```

#### **Exemplo 2: Processamento de CSV com Python**

```python
cat > csv_processor.py << 'EOF'
from mrjob.job import MRJob
import csv
from io import StringIO

class CSVProcessor(MRJob):
    
    def mapper(self, _, line):
        # Pular cabe√ßalho
        if line.startswith('produto,'):
            return
        
        try:
            # Processar linha CSV
            reader = csv.reader(StringIO(line))
            row = next(reader)
            produto, categoria, preco, quantidade, data = row
            
            # Emitir categoria com quantidade vendida
            yield categoria, int(quantidade)
            
        except Exception as e:
            # Ignorar linhas malformadas
            pass
    
    def reducer(self, categoria, quantidades):
        total = sum(quantidades)
        yield categoria, total

if __name__ == '__main__':
    CSVProcessor.run()
EOF
```

### 7.3 Desenvolvimento de Jobs MapReduce em Python

#### **Estrutura B√°sica de um Job MRJob**

```python
from mrjob.job import MRJob
from mrjob.step import MRStep

class MultiStepJob(MRJob):
    
    def steps(self):
        return [
            MRStep(mapper=self.mapper_step1,
                   reducer=self.reducer_step1),
            MRStep(mapper=self.mapper_step2,
                   reducer=self.reducer_step2)
        ]
    
    def mapper_step1(self, _, line):
        # Primeiro passo de mapeamento
        pass
    
    def reducer_step1(self, key, values):
        # Primeiro passo de redu√ß√£o
        pass
    
    def mapper_step2(self, key, value):
        # Segundo passo de mapeamento
        pass
    
    def reducer_step2(self, key, values):
        # Segundo passo de redu√ß√£o
        pass
```

#### **Configura√ß√£o para Hadoop**

Criar arquivo de configura√ß√£o `.mrjob.conf`:

```yaml
runners:
  hadoop:
    hadoop_home: /usr/local/hadoop
    hadoop_streaming_jar: /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.1.jar
    jobconf:
      mapreduce.job.maps: 2
      mapreduce.job.reduces: 1
```

## 8. TROUBLESHOOTING E FAQ

### 8.1 Problemas Comuns

#### **Problema: "Safe mode is ON"**

**Sintomas:**
- Erro ao tentar escrever no HDFS
- Mensagem "Name node is in safe mode"

**Causa:**
O NameNode entra em safe mode durante a inicializa√ß√£o ou quando detecta problemas no sistema de arquivos.

**Solu√ß√£o:**
```bash
# Verificar status do safe mode
/usr/local/hadoop/bin/hdfs dfsadmin -safemode get

# Sair do safe mode (use com cuidado)
/usr/local/hadoop/bin/hdfs dfsadmin -safemode leave

# Aguardar sa√≠da autom√°tica do safe mode
/usr/local/hadoop/bin/hdfs dfsadmin -safemode wait
```

#### **Problema: DataNode n√£o inicia**

**Sintomas:**
- DataNode n√£o aparece no `jps`
- Interface web mostra 0 DataNodes

**Poss√≠veis causas e solu√ß√µes:**

1. **Conflito de cluster ID:**
```bash
# Verificar logs do DataNode
tail -f /usr/local/hadoop/logs/hadoop-*-datanode-*.log

# Se houver conflito de cluster ID, reformatar
/usr/local/hadoop/bin/hdfs namenode -format -force
```

2. **Permiss√µes incorretas:**
```bash
# Corrigir permiss√µes
sudo chown -R $USER:$USER /usr/local/hadoop
sudo chmod -R 755 /usr/local/hadoop
```

3. **Portas em uso:**
```bash
# Verificar portas
netstat -tulpn | grep :9000
netstat -tulpn | grep :9864

# Reiniciar servi√ßos
/usr/local/hadoop/sbin/stop-dfs.sh
/usr/local/hadoop/sbin/start-dfs.sh
```

#### **Problema: Job falha com "Container killed"**

**Sintomas:**
- Jobs MapReduce falham durante execu√ß√£o
- Logs mostram "Container killed by ResourceManager"

**Causa:**
Geralmente relacionado a limita√ß√µes de mem√≥ria.

**Solu√ß√£o:**
```bash
# Verificar configura√ß√µes de mem√≥ria
grep -r "yarn.nodemanager.resource.memory-mb" /usr/local/hadoop/etc/hadoop/
grep -r "yarn.scheduler.maximum-allocation-mb" /usr/local/hadoop/etc/hadoop/

# Ajustar configura√ß√µes se necess√°rio
# Editar yarn-site.xml para aumentar limites de mem√≥ria
```

### 8.2 Solu√ß√µes e Workarounds

#### **Otimiza√ß√£o de Performance**

**Para jobs pequenos:**
```bash
# Reduzir overhead de inicializa√ß√£o
export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"
export HADOOP_CLIENT_OPTS="$HADOOP_CLIENT_OPTS -Xmx512m"
```

**Para processamento de arquivos pequenos:**
```bash
# Usar CombineFileInputFormat para arquivos pequenos
-D mapreduce.input.fileinputformat.split.maxsize=134217728
```

#### **Monitoramento Avan√ßado**

```bash
# Script de monitoramento cont√≠nuo
cat > monitor_hadoop.sh << 'EOF'
#!/bin/bash
while true; do
    echo "=== $(date) ==="
    echo "HDFS Status:"
    /usr/local/hadoop/bin/hdfs dfsadmin -report | grep -E "(Live datanodes|DFS Used|DFS Remaining)"
    echo ""
    echo "YARN Status:"
    /usr/local/hadoop/bin/yarn node -list | grep RUNNING
    echo ""
    echo "Running Applications:"
    /usr/local/hadoop/bin/yarn application -list | grep RUNNING
    echo "========================"
    sleep 30
done
EOF

chmod +x monitor_hadoop.sh
```

### 8.3 Perguntas Frequentes

**Q: Posso executar m√∫ltiplos jobs simultaneamente?**
A: Sim, o YARN gerencia recursos automaticamente e pode executar m√∫ltiplos jobs em paralelo, dependendo dos recursos dispon√≠veis.

**Q: Como aumentar o n√∫mero de mappers/reducers?**
A: O n√∫mero de mappers √© determinado pelo tamanho dos dados de entrada e configura√ß√£o de split. O n√∫mero de reducers pode ser configurado com `-D mapreduce.job.reduces=N`.

**Q: √â poss√≠vel usar Hadoop com dados em tempo real?**
A: O Hadoop tradicional √© otimizado para processamento batch. Para dados em tempo real, considere Apache Storm, Apache Spark Streaming ou Apache Flink.

**Q: Como fazer backup dos dados do HDFS?**
A: Use `hdfs dfs -cp` para c√≥pia dentro do HDFS ou `hdfs dfs -get` para exportar para sistema local. Para clusters de produ√ß√£o, considere DistCp.

**Q: Hadoop funciona bem em m√°quinas virtuais?**
A: Sim, mas com limita√ß√µes de performance. Para produ√ß√£o, hardware dedicado √© recomendado.

## 9. RECURSOS ADICIONAIS

### 9.1 Documenta√ß√£o Oficial

- **Apache Hadoop**: https://hadoop.apache.org/docs/stable/
- **HDFS Architecture**: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html
- **MapReduce Tutorial**: https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html
- **YARN Architecture**: https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html

### 9.2 Tutoriais Complementares

- **Cloudera Tutorials**: https://www.cloudera.com/tutorials.html
- **Hortonworks Sandbox**: https://www.cloudera.com/downloads/hortonworks-sandbox.html
- **Hadoop Ecosystem**: https://data-flair.training/blogs/hadoop-ecosystem-components/

### 9.3 Comunidade e Suporte

- **Stack Overflow**: Tag "hadoop" para perguntas t√©cnicas
- **Apache Hadoop Mailing Lists**: https://hadoop.apache.org/mailing_lists.html
- **Hadoop User Groups**: Grupos locais de usu√°rios Hadoop
- **Conferences**: Strata Data Conference, Hadoop Summit

### 9.4 Ferramentas de Desenvolvimento

- **IDEs Recomendadas**:
  - IntelliJ IDEA com plugin Hadoop
  - Eclipse com plugin Hadoop Development Tools
  - Visual Studio Code com extens√µes Big Data

- **Ferramentas de Linha de Comando**:
  - `hdfs dfs`: Comandos do sistema de arquivos HDFS
  - `yarn`: Comandos de gerenciamento YARN
  - `mapred`: Comandos espec√≠ficos do MapReduce

### 9.5 Pr√≥ximos Passos

Ap√≥s dominar os conceitos b√°sicos deste laborat√≥rio, considere explorar:

1. **Apache Spark**: Framework mais moderno para processamento de Big Data
2. **Apache Hive**: Data warehouse para consultas SQL em dados Hadoop
3. **Apache HBase**: Banco de dados NoSQL para acesso aleat√≥rio
4. **Apache Kafka**: Plataforma de streaming para dados em tempo real
5. **Machine Learning**: Integra√ß√£o com TensorFlow, Scikit-learn, MLlib

## 10. REFER√äNCIAS

[1] Teradata. "What are the 5 V's of Big Data?" Acessado em janeiro 2025. https://www.teradata.com/glossary/what-are-the-5-v-s-of-big-data

[2] Twilio. "Understanding the 5 V's of Big Data." Julho 2025. https://www.twilio.com/en-us/resource-center/big-data-characteristics

[3] GeeksforGeeks. "6V's of Big Data." Junho 2025. https://www.geeksforgeeks.org/data-science/5-vs-of-big-data/

[4] Apache Software Foundation. "Apache Hadoop Documentation." https://hadoop.apache.org/docs/stable/

[5] DataCamp. "Big Data Technologies: Tools, Solutions, and Trends for 2025." Agosto 2025. https://www.datacamp.com/blog/big-data-technologies

[6] TechVidvan. "Apache Hadoop Architecture - HDFS, YARN & MapReduce." Abril 2020. https://techvidvan.com/tutorials/hadoop-architecture/

[7] Apache Software Foundation. "HDFS Architecture Guide." https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html

[8] Apache Software Foundation. "MapReduce Tutorial." https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html

[9] Apache Software Foundation. "Apache Hadoop YARN." https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html

[10] Data Flair. "Hadoop Ecosystem Components - A Complete Tutorial." https://data-flair.training/blogs/hadoop-ecosystem-components/



