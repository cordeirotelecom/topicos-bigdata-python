# üõ†Ô∏è Ferramentas de Big Data - Guia Completo

## **Categorias de Ferramentas**

### **üóÑÔ∏è Armazenamento e Sistemas de Arquivos**

#### **Apache Hadoop HDFS**
**Descri√ß√£o:** Sistema de arquivos distribu√≠do para armazenamento de grandes volumes  
**Quando usar:** Dados estruturados e n√£o estruturados em larga escala  
**Pr√≥s:** Toler√¢ncia a falhas, escalabilidade horizontal  
**Contras:** Lat√™ncia alta para consultas em tempo real  

**Instala√ß√£o Passo a Passo:**
```bash
# 1. Download do Hadoop
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz

# 2. Extra√ß√£o
tar -xzf hadoop-3.3.4.tar.gz
sudo mv hadoop-3.3.4 /opt/hadoop

# 3. Configura√ß√£o de vari√°veis
export HADOOP_HOME=/opt/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# 4. Configura√ß√£o b√°sica
# Editar core-site.xml, hdfs-site.xml, yarn-site.xml
```

**Caso Pr√°tico:** Armazenamento de logs de aplica√ß√µes web de uma empresa de e-commerce

---

#### **Amazon S3**
**Descri√ß√£o:** Servi√ßo de armazenamento em nuvem escal√°vel  
**Quando usar:** Data Lakes, backup, arquivamento  
**Pr√≥s:** Durabilidade 99.999999999%, integra√ß√£o com AWS  
**Contras:** Custos podem escalar rapidamente  

**Configura√ß√£o:**
```python
import boto3

# Configura√ß√£o do cliente S3
s3_client = boto3.client('s3',
    aws_access_key_id='YOUR_ACCESS_KEY',
    aws_secret_access_key='YOUR_SECRET_KEY',
    region_name='us-east-1'
)

# Upload de arquivo
s3_client.upload_file('local_file.csv', 'bucket-name', 'path/file.csv')
```

---

### **‚ö° Processamento de Dados**

#### **Apache Spark**
**Descri√ß√£o:** Engine de processamento distribu√≠do para Big Data  
**Quando usar:** Processamento batch e streaming, ML  
**Pr√≥s:** Velocidade, versatilidade, APIs m√∫ltiplas  
**Contras:** Uso intensivo de mem√≥ria  

**Instala√ß√£o e Configura√ß√£o:**
```bash
# Instala√ß√£o via pip
pip install pyspark

# Ou via conda
conda install pyspark

# Download standalone
wget https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
tar -xzf spark-3.4.1-bin-hadoop3.tgz
```

**Exemplo Pr√°tico:**
```python
from pyspark.sql import SparkSession

# Cria√ß√£o da sess√£o Spark
spark = SparkSession.builder \
    .appName("BigDataAnalysis") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# Leitura de dados
df = spark.read.csv("hdfs://path/to/large_dataset.csv", header=True)

# Processamento
result = df.groupBy("category").agg({"sales": "sum"}).orderBy("category")
result.show()
```

**Caso Real:** An√°lise de transa√ß√µes banc√°rias para detec√ß√£o de fraude

---

#### **Apache Kafka**
**Descri√ß√£o:** Plataforma de streaming distribu√≠da  
**Quando usar:** Streaming de dados em tempo real  
**Pr√≥s:** Baixa lat√™ncia, alta throughput  
**Contras:** Complexidade de configura√ß√£o  

**Instala√ß√£o:**
```bash
# Download
wget https://downloads.apache.org/kafka/2.8.2/kafka_2.13-2.8.2.tgz
tar -xzf kafka_2.13-2.8.2.tgz

# Iniciar Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# Iniciar Kafka
bin/kafka-server-start.sh config/server.properties
```

**Exemplo Python:**
```python
from kafka import KafkaProducer, KafkaConsumer
import json

# Producer
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda x: json.dumps(x).encode('utf-8')
)

# Envio de dados
producer.send('bigdata-topic', {'sensor_id': 1, 'temperature': 25.6})

# Consumer
consumer = KafkaConsumer(
    'bigdata-topic',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

for message in consumer:
    print(f"Received: {message.value}")
```

---

### **üîç Processamento de Consultas**

#### **Apache Hive**
**Descri√ß√£o:** Data warehouse software para consultas SQL em Hadoop  
**Quando usar:** An√°lises batch em dados estruturados  
**Pr√≥s:** Interface SQL familiar, integra√ß√£o com Hadoop  
**Contras:** Lat√™ncia alta  

**Instala√ß√£o:**
```bash
# Download Hive
wget https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz
tar -xzf apache-hive-3.1.3-bin.tar.gz

# Configura√ß√£o
export HIVE_HOME=/opt/hive
export PATH=$PATH:$HIVE_HOME/bin
```

**Exemplo de Uso:**
```sql
-- Cria√ß√£o de tabela externa
CREATE EXTERNAL TABLE sales_data (
    id INT,
    product_name STRING,
    price DOUBLE,
    quantity INT,
    sale_date STRING
)
STORED AS TEXTFILE
LOCATION '/user/data/sales/';

-- Query de an√°lise
SELECT product_name, SUM(price * quantity) as revenue
FROM sales_data
WHERE sale_date >= '2023-01-01'
GROUP BY product_name
ORDER BY revenue DESC;
```

---

#### **Apache Drill**
**Descri√ß√£o:** Engine SQL para explora√ß√£o de dados  
**Quando usar:** Consultas ad-hoc em dados semi-estruturados  
**Pr√≥s:** Schema-free, m√∫ltiplas fontes de dados  
**Contras:** Limita√ß√µes em joins complexos  

---

### **üèóÔ∏è Orquestra√ß√£o e Workflow**

#### **Apache Airflow**
**Descri√ß√£o:** Plataforma para orquestra√ß√£o de workflows  
**Quando usar:** ETL complexos, pipelines de dados  
**Pr√≥s:** Interface visual, extensibilidade  
**Contras:** Curva de aprendizado √≠ngreme  

**Instala√ß√£o:**
```bash
# Instala√ß√£o via pip
pip install apache-airflow

# Inicializa√ß√£o
airflow db init
airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com

# Iniciar webserver
airflow webserver --port 8080

# Iniciar scheduler
airflow scheduler
```

**Exemplo de DAG:**
```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

def extract_data():
    # L√≥gica de extra√ß√£o
    pass

def transform_data():
    # L√≥gica de transforma√ß√£o
    pass

def load_data():
    # L√≥gica de carga
    pass

dag = DAG(
    'bigdata_etl_pipeline',
    default_args={
        'owner': 'data-team',
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
    },
    description='Pipeline ETL para Big Data',
    schedule_interval='@daily',
    start_date=datetime(2023, 1, 1),
    catchup=False
)

extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag
)

load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=dag
)

extract_task >> transform_task >> load_task
```

---

### **üìä Bancos NoSQL**

#### **MongoDB**
**Descri√ß√£o:** Banco de dados orientado a documentos  
**Quando usar:** Dados semi-estruturados, desenvolvimento √°gil  
**Pr√≥s:** Flexibilidade de schema, escalabilidade horizontal  
**Contras:** Consist√™ncia eventual  

**Instala√ß√£o:**
```bash
# Ubuntu/Debian
sudo apt-get install mongodb

# Usando Docker
docker run --name mongodb -p 27017:27017 -d mongo:latest
```

**Exemplo Python:**
```python
from pymongo import MongoClient
import pandas as pd

# Conex√£o
client = MongoClient('mongodb://localhost:27017/')
db = client['bigdata_db']
collection = db['sensor_data']

# Inser√ß√£o de dados
sensor_data = {
    'sensor_id': 'TEMP001',
    'timestamp': datetime.now(),
    'temperature': 25.6,
    'humidity': 60.2,
    'location': {'lat': -23.5505, 'lng': -46.6333}
}
collection.insert_one(sensor_data)

# Consulta e convers√£o para DataFrame
cursor = collection.find({'sensor_id': 'TEMP001'})
df = pd.DataFrame(list(cursor))
```

---

#### **Apache Cassandra**
**Descri√ß√£o:** Banco NoSQL distribu√≠do  
**Quando usar:** Alta disponibilidade, write-heavy workloads  
**Pr√≥s:** Performance linear com escala  
**Contras:** Modelo de dados limitado  

---

### **‚òÅÔ∏è Ferramentas Cloud**

#### **Amazon EMR**
**Descri√ß√£o:** Plataforma Big Data gerenciada na AWS  
**Quando usar:** Processamento Spark/Hadoop sem gerenciar infraestrutura  
**Pr√≥s:** Configura√ß√£o autom√°tica, integra√ß√£o AWS  
**Contras:** Vendor lock-in, custos  

**Configura√ß√£o via CLI:**
```bash
# Criar cluster EMR
aws emr create-cluster \
    --name "BigData-Cluster" \
    --release-label emr-6.4.0 \
    --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m5.xlarge \
                      InstanceGroupType=CORE,InstanceCount=2,InstanceType=m5.xlarge \
    --applications Name=Spark Name=Hadoop Name=Hive \
    --ec2-attributes KeyName=my-key-pair \
    --use-default-roles
```

---

#### **Google BigQuery**
**Descri√ß√£o:** Data warehouse serverless  
**Quando usar:** Analytics em petabytes de dados  
**Pr√≥s:** Sem infraestrutura, SQL padr√£o  
**Contras:** Custos por consulta  

**Exemplo Python:**
```python
from google.cloud import bigquery
import pandas as pd

# Cliente BigQuery
client = bigquery.Client()

# Query
query = """
    SELECT product_category, SUM(sales_amount) as total_sales
    FROM `project.dataset.sales_table`
    WHERE sale_date >= '2023-01-01'
    GROUP BY product_category
    ORDER BY total_sales DESC
"""

# Executar e converter para DataFrame
df = client.query(query).to_dataframe()
```

---

### **üîß Ferramentas de Desenvolvimento**

#### **Jupyter Notebooks**
**Descri√ß√£o:** Ambiente interativo para an√°lise de dados  
**Quando usar:** Explora√ß√£o de dados, prototipagem  
**Pr√≥s:** Interatividade, visualiza√ß√µes inline  
**Contras:** N√£o adequado para produ√ß√£o  

**Configura√ß√£o para Big Data:**
```bash
# Instala√ß√£o com extens√µes
pip install jupyter jupyterlab
pip install pyspark findspark

# Configura√ß√£o para Spark
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS='notebook'
```

---

#### **Apache Zeppelin**
**Descri√ß√£o:** Notebook para analytics interativo  
**Quando usar:** Colabora√ß√£o em an√°lises Big Data  
**Pr√≥s:** M√∫ltiplas linguagens, integra√ß√£o Spark  
**Contras:** Interface menos polida que Jupyter  

---

### **üìà Monitoramento e Visualiza√ß√£o**

#### **Grafana**
**Descri√ß√£o:** Plataforma de monitoramento e observabilidade  
**Quando usar:** Dashboards em tempo real  
**Pr√≥s:** M√∫ltiplas fontes de dados, alertas  
**Contras:** Requer configura√ß√£o de datasources  

#### **Apache Superset**
**Descri√ß√£o:** Plataforma de visualiza√ß√£o de dados  
**Quando usar:** Dashboards empresariais  
**Pr√≥s:** Interface web moderna, SQL Lab  
**Contras:** Curva de aprendizado  

---

## **üèóÔ∏è Arquiteturas de Refer√™ncia**

### **Lambda Architecture**
```
Data Sources ‚Üí [Batch Layer] ‚Üí [Master Dataset] ‚Üí [Batch Views]
             ‚Üí [Speed Layer] ‚Üí [Real-time Views]  ‚Üí [Serving Layer]
```

### **Kappa Architecture**
```
Data Sources ‚Üí [Stream Processing] ‚Üí [Serving Layer]
```

### **Modern Data Stack**
```
Sources ‚Üí [Ingestion] ‚Üí [Storage] ‚Üí [Transformation] ‚Üí [Analytics] ‚Üí [Activation]
```

---

## **üìã Guia de Sele√ß√£o de Ferramentas**

### **Por Volume de Dados**
- **< 1GB:** Pandas, SQLite
- **1GB - 100GB:** PostgreSQL, MySQL
- **100GB - 10TB:** Spark, Clickhouse
- **> 10TB:** Hadoop, Cloud Data Warehouses

### **Por Velocidade de Processamento**
- **Batch (horas/dias):** Hadoop MapReduce
- **Near Real-time (minutos):** Spark Structured Streaming
- **Real-time (segundos):** Kafka Streams, Apache Flink

### **Por Tipo de Dados**
- **Estruturados:** Hive, BigQuery, Redshift
- **Semi-estruturados:** MongoDB, Elasticsearch
- **N√£o estruturados:** HDFS, S3, Azure Blob

---

## **üéØ Cases de Uso por Ind√∫stria**

### **E-commerce**
- **Recomenda√ß√µes:** Spark MLlib + Redis
- **An√°lise de clickstream:** Kafka + Elasticsearch
- **Fraud detection:** Spark Streaming + ML models

### **Fintech**
- **Risk assessment:** Hadoop + Hive + R/Python
- **Real-time trading:** Kafka + Apache Flink
- **Compliance reporting:** Airflow + Spark + BigQuery

### **IoT/Manufacturing**
- **Sensor data:** InfluxDB + Grafana
- **Predictive maintenance:** Spark + TensorFlow
- **Edge processing:** Apache NiFi + Edge computing

---

*Este guia √© atualizado regularmente com as √∫ltimas vers√µes e melhores pr√°ticas.*
