# ğŸ› ï¸ Ferramentas de Big Data - Guia Completo

## **Categorias de Ferramentas**

### **ğŸ—„ï¸ Armazenamento e Sistemas de Arquivos**

#### **Apache Hadoop HDFS**
**DescriÃ§Ã£o:** Sistema de arquivos distribuÃ­do para armazenamento de grandes volumes  
**Quando usar:** Dados estruturados e nÃ£o estruturados em larga escala  
**PrÃ³s:** TolerÃ¢ncia a falhas, escalabilidade horizontal  
**Contras:** LatÃªncia alta para consultas em tempo real  

**InstalaÃ§Ã£o Passo a Passo:**
```bash
# 1. Download do Hadoop
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz

# 2. ExtraÃ§Ã£o
tar -xzf hadoop-3.3.4.tar.gz
sudo mv hadoop-3.3.4 /opt/hadoop

# 3. ConfiguraÃ§Ã£o de variÃ¡veis
export HADOOP_HOME=/opt/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# 4. ConfiguraÃ§Ã£o bÃ¡sica
# Editar core-site.xml, hdfs-site.xml, yarn-site.xml
```

**Caso PrÃ¡tico:** Armazenamento de logs de aplicaÃ§Ãµes web de uma empresa de e-commerce

---

#### **Amazon S3**
**DescriÃ§Ã£o:** ServiÃ§o de armazenamento em nuvem escalÃ¡vel  
**Quando usar:** Data Lakes, backup, arquivamento  
**PrÃ³s:** Durabilidade 99.999999999%, integraÃ§Ã£o com AWS  
**Contras:** Custos podem escalar rapidamente  

**ConfiguraÃ§Ã£o:**
```python
import boto3

# ConfiguraÃ§Ã£o do cliente S3
s3_client = boto3.client('s3',
    aws_access_key_id='YOUR_ACCESS_KEY',
    aws_secret_access_key='YOUR_SECRET_KEY',
    region_name='us-east-1'
)

# Upload de arquivo
s3_client.upload_file('local_file.csv', 'bucket-name', 'path/file.csv')
```

---

### **âš¡ Processamento de Dados**

#### **Apache Spark**
**DescriÃ§Ã£o:** Engine de processamento distribuÃ­do para Big Data  
**Quando usar:** Processamento batch e streaming, ML  
**PrÃ³s:** Velocidade, versatilidade, APIs mÃºltiplas  
**Contras:** Uso intensivo de memÃ³ria  

**InstalaÃ§Ã£o e ConfiguraÃ§Ã£o:**
```bash
# InstalaÃ§Ã£o via pip
pip install pyspark

# Ou via conda
conda install pyspark

# Download standalone
wget https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
tar -xzf spark-3.4.1-bin-hadoop3.tgz
```

**Exemplo PrÃ¡tico:**
```python
from pyspark.sql import SparkSession

# CriaÃ§Ã£o da sessÃ£o Spark
spark = SparkSession.builder \
    .appName("BigDataAnalysis") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# Leitura de dados
df = spark.read.csv("hdfs://path/to/large_dataset.csv", header=True)

# Processamento
result = df.groupBy("category").agg({"sales": "sum"}).orderBy("category")
result.show()
```

**Caso Real:** AnÃ¡lise de transaÃ§Ãµes bancÃ¡rias para detecÃ§Ã£o de fraude

---

#### **Apache Kafka**
**DescriÃ§Ã£o:** Plataforma de streaming distribuÃ­da  
**Quando usar:** Streaming de dados em tempo real  
**PrÃ³s:** Baixa latÃªncia, alta throughput  
**Contras:** Complexidade de configuraÃ§Ã£o  

**InstalaÃ§Ã£o:**
```bash
# Download
wget https://downloads.apache.org/kafka/2.8.2/kafka_2.13-2.8.2.tgz
tar -xzf kafka_2.13-2.8.2.tgz

# Iniciar Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# Iniciar Kafka
bin/kafka-server-start.sh config/server.properties
```

**Exemplo Python:**
```python
from kafka import KafkaProducer, KafkaConsumer
import json

# Producer
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda x: json.dumps(x).encode('utf-8')
)

# Envio de dados
producer.send('bigdata-topic', {'sensor_id': 1, 'temperature': 25.6})

# Consumer
consumer = KafkaConsumer(
    'bigdata-topic',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

for message in consumer:
    print(f"Received: {message.value}")
```

---

### **ğŸ” Processamento de Consultas**

#### **Apache Hive**
**DescriÃ§Ã£o:** Data warehouse software para consultas SQL em Hadoop  
**Quando usar:** AnÃ¡lises batch em dados estruturados  
**PrÃ³s:** Interface SQL familiar, integraÃ§Ã£o com Hadoop  
**Contras:** LatÃªncia alta  

**InstalaÃ§Ã£o:**
```bash
# Download Hive
wget https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz
tar -xzf apache-hive-3.1.3-bin.tar.gz

# ConfiguraÃ§Ã£o
export HIVE_HOME=/opt/hive
export PATH=$PATH:$HIVE_HOME/bin
```

**Exemplo de Uso:**
```sql
-- CriaÃ§Ã£o de tabela externa
CREATE EXTERNAL TABLE sales_data (
    id INT,
    product_name STRING,
    price DOUBLE,
    quantity INT,
    sale_date STRING
)
STORED AS TEXTFILE
LOCATION '/user/data/sales/';

-- Query de anÃ¡lise
SELECT product_name, SUM(price * quantity) as revenue
FROM sales_data
WHERE sale_date >= '2023-01-01'
GROUP BY product_name
ORDER BY revenue DESC;
```

---

#### **Apache Drill**
**DescriÃ§Ã£o:** Engine SQL para exploraÃ§Ã£o de dados  
**Quando usar:** Consultas ad-hoc em dados semi-estruturados  
**PrÃ³s:** Schema-free, mÃºltiplas fontes de dados  
**Contras:** LimitaÃ§Ãµes em joins complexos  

---

### **ğŸ—ï¸ OrquestraÃ§Ã£o e Workflow**

#### **Apache Airflow**
**DescriÃ§Ã£o:** Plataforma para orquestraÃ§Ã£o de workflows  
**Quando usar:** ETL complexos, pipelines de dados  
**PrÃ³s:** Interface visual, extensibilidade  
**Contras:** Curva de aprendizado Ã­ngreme  

**InstalaÃ§Ã£o:**
```bash
# InstalaÃ§Ã£o via pip
pip install apache-airflow

# InicializaÃ§Ã£o
airflow db init
airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com

# Iniciar webserver
airflow webserver --port 8080

# Iniciar scheduler
airflow scheduler
```

**Exemplo de DAG:**
```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

def extract_data():
    # LÃ³gica de extraÃ§Ã£o
    pass

def transform_data():
    # LÃ³gica de transformaÃ§Ã£o
    pass

def load_data():
    # LÃ³gica de carga
    pass

dag = DAG(
    'bigdata_etl_pipeline',
    default_args={
        'owner': 'data-team',
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
    },
    description='Pipeline ETL para Big Data',
    schedule_interval='@daily',
    start_date=datetime(2023, 1, 1),
    catchup=False
)

extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag
)

load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=dag
)

extract_task >> transform_task >> load_task
```

---

### **ğŸ“Š Bancos NoSQL**

#### **MongoDB**
**DescriÃ§Ã£o:** Banco de dados orientado a documentos  
**Quando usar:** Dados semi-estruturados, desenvolvimento Ã¡gil  
**PrÃ³s:** Flexibilidade de schema, escalabilidade horizontal  
**Contras:** ConsistÃªncia eventual  

**InstalaÃ§Ã£o:**
```bash
# Ubuntu/Debian
sudo apt-get install mongodb

# Usando Docker
docker run --name mongodb -p 27017:27017 -d mongo:latest
```

**Exemplo Python:**
```python
from pymongo import MongoClient
import pandas as pd

# ConexÃ£o
client = MongoClient('mongodb://localhost:27017/')
db = client['bigdata_db']
collection = db['sensor_data']

# InserÃ§Ã£o de dados
sensor_data = {
    'sensor_id': 'TEMP001',
    'timestamp': datetime.now(),
    'temperature': 25.6,
    'humidity': 60.2,
    'location': {'lat': -23.5505, 'lng': -46.6333}
}
collection.insert_one(sensor_data)

# Consulta e conversÃ£o para DataFrame
cursor = collection.find({'sensor_id': 'TEMP001'})
df = pd.DataFrame(list(cursor))
```

---

#### **Apache Cassandra**
**DescriÃ§Ã£o:** Banco NoSQL distribuÃ­do  
**Quando usar:** Alta disponibilidade, write-heavy workloads  
**PrÃ³s:** Performance linear com escala  
**Contras:** Modelo de dados limitado  

---

### **â˜ï¸ Ferramentas Cloud**

#### **Amazon EMR**
**DescriÃ§Ã£o:** Plataforma Big Data gerenciada na AWS  
**Quando usar:** Processamento Spark/Hadoop sem gerenciar infraestrutura  
**PrÃ³s:** ConfiguraÃ§Ã£o automÃ¡tica, integraÃ§Ã£o AWS  
**Contras:** Vendor lock-in, custos  

**ConfiguraÃ§Ã£o via CLI:**
```bash
# Criar cluster EMR
aws emr create-cluster \
    --name "BigData-Cluster" \
    --release-label emr-6.4.0 \
    --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m5.xlarge \
                      InstanceGroupType=CORE,InstanceCount=2,InstanceType=m5.xlarge \
    --applications Name=Spark Name=Hadoop Name=Hive \
    --ec2-attributes KeyName=my-key-pair \
    --use-default-roles
```

---

#### **Google BigQuery**
**DescriÃ§Ã£o:** Data warehouse serverless  
**Quando usar:** Analytics em petabytes de dados  
**PrÃ³s:** Sem infraestrutura, SQL padrÃ£o  
**Contras:** Custos por consulta  

**Exemplo Python:**
```python
from google.cloud import bigquery
import pandas as pd

# Cliente BigQuery
client = bigquery.Client()

# Query
query = """
    SELECT product_category, SUM(sales_amount) as total_sales
    FROM `project.dataset.sales_table`
    WHERE sale_date >= '2023-01-01'
    GROUP BY product_category
    ORDER BY total_sales DESC
"""

# Executar e converter para DataFrame
df = client.query(query).to_dataframe()
```

---

### **ğŸ”§ Ferramentas de Desenvolvimento**

#### **Jupyter Notebooks**
**DescriÃ§Ã£o:** Ambiente interativo para anÃ¡lise de dados  
**Quando usar:** ExploraÃ§Ã£o de dados, prototipagem  
**PrÃ³s:** Interatividade, visualizaÃ§Ãµes inline  
**Contras:** NÃ£o adequado para produÃ§Ã£o  

**ConfiguraÃ§Ã£o para Big Data:**
```bash
# InstalaÃ§Ã£o com extensÃµes
pip install jupyter jupyterlab
pip install pyspark findspark

# ConfiguraÃ§Ã£o para Spark
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS='notebook'
```

---

#### **Apache Zeppelin**
**DescriÃ§Ã£o:** Notebook para analytics interativo  
**Quando usar:** ColaboraÃ§Ã£o em anÃ¡lises Big Data  
**PrÃ³s:** MÃºltiplas linguagens, integraÃ§Ã£o Spark  
**Contras:** Interface menos polida que Jupyter  

---

### **ğŸ“ˆ Monitoramento e VisualizaÃ§Ã£o**

#### **Grafana**
**DescriÃ§Ã£o:** Plataforma de monitoramento e observabilidade  
**Quando usar:** Dashboards em tempo real  
**PrÃ³s:** MÃºltiplas fontes de dados, alertas  
**Contras:** Requer configuraÃ§Ã£o de datasources  

#### **Apache Superset**
**DescriÃ§Ã£o:** Plataforma de visualizaÃ§Ã£o de dados  
**Quando usar:** Dashboards empresariais  
**PrÃ³s:** Interface web moderna, SQL Lab  
**Contras:** Curva de aprendizado  

---

## **ğŸ—ï¸ Arquiteturas de ReferÃªncia**

### **Lambda Architecture**
```
Data Sources â†’ [Batch Layer] â†’ [Master Dataset] â†’ [Batch Views]
             â†’ [Speed Layer] â†’ [Real-time Views]  â†’ [Serving Layer]
```

### **Kappa Architecture**
```
Data Sources â†’ [Stream Processing] â†’ [Serving Layer]
```

### **Modern Data Stack**
```
Sources â†’ [Ingestion] â†’ [Storage] â†’ [Transformation] â†’ [Analytics] â†’ [Activation]
```

---

## **ğŸ“‹ Guia de SeleÃ§Ã£o de Ferramentas**

### **Por Volume de Dados**
- **< 1GB:** Pandas, SQLite
- **1GB - 100GB:** PostgreSQL, MySQL
- **100GB - 10TB:** Spark, Clickhouse
- **> 10TB:** Hadoop, Cloud Data Warehouses

### **Por Velocidade de Processamento**
- **Batch (horas/dias):** Hadoop MapReduce
- **Near Real-time (minutos):** Spark Structured Streaming
- **Real-time (segundos):** Kafka Streams, Apache Flink

### **Por Tipo de Dados**
- **Estruturados:** Hive, BigQuery, Redshift
- **Semi-estruturados:** MongoDB, Elasticsearch
- **NÃ£o estruturados:** HDFS, S3, Azure Blob

---

## **ğŸ¯ Cases de Uso por IndÃºstria**

### **E-commerce**
- **RecomendaÃ§Ãµes:** Spark MLlib + Redis
- **AnÃ¡lise de clickstream:** Kafka + Elasticsearch
- **Fraud detection:** Spark Streaming + ML models

### **Fintech**
- **Risk assessment:** Hadoop + Hive + R/Python
- **Real-time trading:** Kafka + Apache Flink
- **Compliance reporting:** Airflow + Spark + BigQuery

### **IoT/Manufacturing**
- **Sensor data:** InfluxDB + Grafana
- **Predictive maintenance:** Spark + TensorFlow
- **Edge processing:** Apache NiFi + Edge computing

---

*Este guia Ã© atualizado regularmente com as Ãºltimas versÃµes e melhores prÃ¡ticas.*
