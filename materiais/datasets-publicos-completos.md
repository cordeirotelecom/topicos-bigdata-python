# üìä Datasets P√∫blicos para Big Data - Lista Completa

## **üè¢ Datasets Corporativos e E-commerce**

### **Amazon Product Reviews**
**Descri√ß√£o:** Milh√µes de reviews de produtos da Amazon  
**Volume:** 233+ milh√µes de reviews  
**Formato:** JSON, CSV  
**URL:** https://nijianmo.github.io/amazon/index.html  
**Casos de Uso:** NLP, sistemas de recomenda√ß√£o, an√°lise de sentimento  

**Exemplo de Estrutura:**
```json
{
  "reviewerID": "A2SUAM1J3GNN3B",
  "asin": "0000013714",
  "reviewerName": "J. McDonald",
  "helpful": [2, 3],
  "reviewText": "I bought this for my husband...",
  "overall": 5.0,
  "summary": "Great product!",
  "unixReviewTime": 1362096000,
  "reviewTime": "03 1, 2013"
}
```

---

### **Instacart Market Basket Analysis**
**Descri√ß√£o:** 3+ milh√µes de pedidos de grocery do Instacart  
**Volume:** 32+ milh√µes de produtos em carrinho  
**Formato:** CSV  
**URL:** https://www.kaggle.com/c/instacart-market-basket-analysis  
**Casos de Uso:** An√°lise de cesta de compras, recomenda√ß√µes  

**Tabelas:**
- orders.csv (3.4M orders)
- order_products.csv (32M order-product relationships)
- products.csv (49K products)
- departments.csv, aisles.csv

---

### **Walmart Sales Forecasting**
**Descri√ß√£o:** Dados de vendas hist√≥ricas de 45 lojas Walmart  
**Volume:** 421,570 registros  
**Formato:** CSV  
**URL:** https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting  
**Casos de Uso:** Previs√£o de demanda, time series analysis  

---

## **üöó Transporte e Mobilidade**

### **NYC Taxi Trip Data**
**Descri√ß√£o:** Dados de viagens de t√°xi em Nova York  
**Volume:** 1+ bilh√£o de viagens (2009-presente)  
**Formato:** CSV, Parquet  
**URL:** https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page  
**Casos de Uso:** An√°lise de mobilidade urbana, otimiza√ß√£o de rotas  

**Exemplo de Schema:**
```
VendorID, tpep_pickup_datetime, tpep_dropoff_datetime,
passenger_count, trip_distance, pickup_longitude,
pickup_latitude, dropoff_longitude, dropoff_latitude,
payment_type, fare_amount, tip_amount, total_amount
```

---

### **Uber Movement Data**
**Descri√ß√£o:** Dados de movimento agregados do Uber  
**Volume:** Milh√µes de viagens anonimizadas  
**Formato:** CSV, JSON  
**URL:** https://movement.uber.com/  
**Casos de Uso:** Planejamento urbano, an√°lise de tr√°fego  

---

### **Citibike Trip Data**
**Descri√ß√£o:** Sistema de bike sharing de NYC  
**Volume:** 200+ milh√µes de viagens  
**Formato:** CSV  
**URL:** https://www.citibikenyc.com/system-data  
**Casos de Uso:** Mobilidade sustent√°vel, otimiza√ß√£o de esta√ß√µes  

---

## **üí∞ Dados Financeiros**

### **Stock Market Data**
**Descri√ß√£o:** Pre√ßos hist√≥ricos de a√ß√µes  
**Volume:** D√©cadas de dados de milhares de a√ß√µes  
**Formato:** CSV, API  
**URL:** https://finance.yahoo.com/, https://www.quandl.com/  
**Casos de Uso:** Trading algor√≠tmico, an√°lise de risco  

**APIs Gratuitas:**
- Alpha Vantage: https://www.alphavantage.co/
- Yahoo Finance API: https://pypi.org/project/yfinance/
- Quandl: https://www.quandl.com/

**Exemplo Python:**
```python
import yfinance as yf
import pandas as pd

# Download dados hist√≥ricos
ticker = yf.Ticker("AAPL")
data = ticker.history(period="10y")
print(data.head())
```

---

### **Bitcoin Blockchain Data**
**Descri√ß√£o:** Todas as transa√ß√µes Bitcoin  
**Volume:** 700+ GB de dados  
**Formato:** JSON, SQL dumps  
**URL:** https://www.blockchain.com/api, https://gz.blockchair.com/  
**Casos de Uso:** An√°lise de redes, detec√ß√£o de fraudes  

---

### **Credit Card Fraud Detection**
**Descri√ß√£o:** Transa√ß√µes de cart√£o de cr√©dito com labels de fraude  
**Volume:** 284,807 transa√ß√µes  
**Formato:** CSV  
**URL:** https://www.kaggle.com/mlg-ulb/creditcardfraud  
**Casos de Uso:** Machine learning para detec√ß√£o de fraudes  

---

## **üè• Sa√∫de e Medicina**

### **COVID-19 Data**
**Descri√ß√£o:** Dados globais da pandemia COVID-19  
**Volume:** Dados di√°rios desde janeiro 2020  
**Formato:** CSV, JSON  
**URL:** https://github.com/CSSEGISandData/COVID-19  
**Casos de Uso:** Epidemiologia, modelagem de surtos  

**Estrutura:**
- Casos confirmados por pa√≠s/regi√£o
- Mortes por pa√≠s/regi√£o
- Recuperados por pa√≠s/regi√£o
- Dados temporais e geogr√°ficos

---

### **MIMIC-III Critical Care Database**
**Descri√ß√£o:** Dados de pacientes em UTI  
**Volume:** 40,000+ pacientes cr√≠ticos  
**Formato:** SQL, CSV  
**URL:** https://mimic.mit.edu/  
**Casos de Uso:** An√°lise cl√≠nica, ML em sa√∫de  
**Nota:** Requer certifica√ß√£o para acesso

---

### **FDA Drug Recalls**
**Descri√ß√£o:** Recalls de medicamentos pela FDA  
**Volume:** 10,000+ recalls  
**Formato:** JSON, XML  
**URL:** https://open.fda.gov/  
**Casos de Uso:** Farmacovigil√¢ncia, an√°lise regulat√≥ria  

---

## **üåç Dados Ambientais e Clim√°ticos**

### **NOAA Climate Data**
**Descri√ß√£o:** Dados clim√°ticos hist√≥ricos globais  
**Volume:** S√©culos de dados meteorol√≥gicos  
**Formato:** NetCDF, CSV, XML  
**URL:** https://www.ncdc.noaa.gov/data-access  
**Casos de Uso:** Modelagem clim√°tica, agricultura  

**Tipos de Dados:**
- Temperatura global
- Precipita√ß√£o
- Velocidade do vento
- Press√£o atmosf√©rica
- Dados de sat√©lite

---

### **NASA Earth Data**
**Descri√ß√£o:** Dados de observa√ß√£o da Terra  
**Volume:** Petabytes de dados satelitais  
**Formato:** HDF, NetCDF, GeoTIFF  
**URL:** https://earthdata.nasa.gov/  
**Casos de Uso:** Mudan√ßas clim√°ticas, monitoramento ambiental  

---

### **Global Power Plant Database**
**Descri√ß√£o:** Usinas de energia globais  
**Volume:** 35,000+ usinas  
**Formato:** CSV  
**URL:** https://datasets.wri.org/dataset/globalpowerplantdatabase  
**Casos de Uso:** An√°lise energ√©tica, sustentabilidade  

---

## **üì± Redes Sociais e Web**

### **Twitter API**
**Descri√ß√£o:** Tweets em tempo real e hist√≥ricos  
**Volume:** 500+ milh√µes de tweets/dia  
**Formato:** JSON  
**URL:** https://developer.twitter.com/en/docs/twitter-api  
**Casos de Uso:** An√°lise de sentimento, trending topics  

**Exemplo de Coleta:**
```python
import tweepy
import pandas as pd

# Configura√ß√£o da API
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth)

# Coleta de tweets
tweets = tweepy.Cursor(api.search_tweets, q="#bigdata", lang="en").items(1000)
tweet_data = [[tweet.created_at, tweet.text, tweet.user.screen_name] for tweet in tweets]
df = pd.DataFrame(tweet_data, columns=['Date', 'Text', 'User'])
```

---

### **Reddit Comments Dataset**
**Descri√ß√£o:** Coment√°rios hist√≥ricos do Reddit  
**Volume:** 1.7+ bilh√µes de coment√°rios  
**Formato:** JSON, BigQuery  
**URL:** https://www.reddit.com/r/datasets/  
**Casos de Uso:** NLP, an√°lise de comunidades  

---

### **Wikipedia Page Views**
**Descri√ß√£o:** Visualiza√ß√µes de p√°ginas da Wikipedia  
**Volume:** Trilh√µes de page views  
**Formato:** Gzip, Parquet  
**URL:** https://dumps.wikimedia.org/other/pageviews/  
**Casos de Uso:** An√°lise de interesse p√∫blico, trending  

---

## **üéµ M√≠dia e Entretenimento**

### **Spotify Music Data**
**Descri√ß√£o:** Caracter√≠sticas de milh√µes de m√∫sicas  
**Volume:** 1.2+ milh√£o de faixas  
**Formato:** CSV  
**URL:** https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-600k-tracks  
**Casos de Uso:** Sistemas de recomenda√ß√£o musical  

**Features:**
- acousticness, danceability, energy
- instrumentalness, liveness, loudness
- speechiness, tempo, valence

---

### **MovieLens Dataset**
**Descri√ß√£o:** Ratings de filmes por usu√°rios  
**Volume:** 27+ milh√µes de ratings  
**Formato:** CSV  
**URL:** https://grouplens.org/datasets/movielens/  
**Casos de Uso:** Sistemas de recomenda√ß√£o, filtros colaborativos  

**Tamanhos Dispon√≠veis:**
- MovieLens 100K (100,000 ratings)
- MovieLens 1M (1 million ratings)
- MovieLens 10M (10 million ratings)
- MovieLens 25M (25 million ratings)

---

### **YouTube Trending Videos**
**Descri√ß√£o:** V√≠deos em trending do YouTube  
**Volume:** 200,000+ v√≠deos  
**Formato:** CSV  
**URL:** https://www.kaggle.com/datasnaek/youtube-new  
**Casos de Uso:** An√°lise de viral content, predi√ß√£o de tend√™ncias  

---

## **üèõÔ∏è Dados Governamentais**

### **US Government Open Data**
**Descri√ß√£o:** Dados abertos do governo americano  
**Volume:** 300,000+ datasets  
**Formato:** CSV, JSON, XML, API  
**URL:** https://www.data.gov/  
**Casos de Uso:** An√°lise de pol√≠ticas p√∫blicas, transpar√™ncia  

**Categorias:**
- Agriculture, Climate, Consumer, Ecosystems
- Education, Energy, Finance, Health
- Manufacturing, Science & Research

---

### **UK Government Data**
**Descri√ß√£o:** Dados abertos do governo brit√¢nico  
**Volume:** 50,000+ datasets  
**Formato:** CSV, JSON, XML  
**URL:** https://data.gov.uk/  
**Casos de Uso:** An√°lise comparativa internacional  

---

### **World Bank Open Data**
**Descri√ß√£o:** Indicadores de desenvolvimento mundial  
**Volume:** 1,400+ indicadores para 217 economias  
**Formato:** CSV, XML, JSON, API  
**URL:** https://data.worldbank.org/  
**Casos de Uso:** An√°lise macroecon√¥mica, desenvolvimento  

**Exemplo de Acesso:**
```python
import wbdata
import pandas as pd

# Indicadores dispon√≠veis
indicators = wbdata.get_indicators()

# Dados espec√≠ficos
data = wbdata.get_dataframe({
    "SP.POP.TOTL": "Population",
    "NY.GDP.MKTP.CD": "GDP"
}, country="all", date="2020")
```

---

## **üèÉ‚Äç‚ôÇÔ∏è Esportes e Fitness**

### **Sports Statistics**
**Descri√ß√£o:** Estat√≠sticas esportivas hist√≥ricas  
**Volume:** D√©cadas de dados esportivos  
**Formato:** CSV, JSON, API  
**URL:** https://www.sports-reference.com/  
**Casos de Uso:** Analytics esportivos, predi√ß√£o de resultados  

**APIs Populares:**
- ESPN API
- The Sports DB
- Football-Data.org
- NBA Stats API

---

### **Strava Activity Data**
**Descri√ß√£o:** Dados de atividades f√≠sicas  
**Volume:** Bilh√µes de atividades  
**Formato:** JSON via API  
**URL:** https://developers.strava.com/  
**Casos de Uso:** An√°lise de fitness, padr√µes de exerc√≠cio  

---

## **üöÄ Como Usar os Datasets**

### **1. Downloading Large Datasets**
```python
import requests
import pandas as pd
from tqdm import tqdm

def download_large_file(url, filename):
    response = requests.get(url, stream=True)
    total_size = int(response.headers.get('content-length', 0))
    
    with open(filename, 'wb') as file, tqdm(
        desc=filename,
        total=total_size,
        unit='B',
        unit_scale=True,
        unit_divisor=1024,
    ) as pbar:
        for chunk in response.iter_content(chunk_size=8192):
            size = file.write(chunk)
            pbar.update(size)

# Uso
url = "https://example.com/large_dataset.csv"
download_large_file(url, "dataset.csv")
```

### **2. Efficient Data Loading**
```python
# Para CSVs grandes
df = pd.read_csv('large_dataset.csv', 
                 chunksize=10000,  # Processa em chunks
                 dtype={'column': 'category'},  # Otimiza tipos
                 parse_dates=['date_column'])

# Para Parquet (mais eficiente)
import pyarrow.parquet as pq
df = pq.read_table('dataset.parquet').to_pandas()
```

### **3. Sampling for Development**
```python
# Sample aleat√≥rio para desenvolvimento
df_sample = df.sample(n=10000, random_state=42)

# Sample estratificado
from sklearn.model_selection import train_test_split
df_sample = df.groupby('category').apply(
    lambda x: x.sample(min(len(x), 1000))
).reset_index(drop=True)
```

---

## **üìÅ Datasets por N√≠vel de Dificuldade**

### **üü¢ Iniciante (< 1GB)**
- Titanic Dataset (Kaggle)
- Iris Dataset
- Boston Housing Dataset
- Wine Quality Dataset

### **üü° Intermedi√°rio (1-10GB)**
- NYC Taxi Data (1 ano)
- Amazon Reviews (categoria espec√≠fica)
- MovieLens 25M
- Spotify Dataset

### **üî¥ Avan√ßado (>10GB)**
- Full NYC Taxi Data
- Complete Amazon Reviews
- Google Books N-grams
- Common Crawl Web Data

---

## **üîß Ferramentas para Gerenciar Datasets**

### **DVC (Data Version Control)**
```bash
# Instala√ß√£o
pip install dvc

# Adicionar dataset
dvc add large_dataset.csv
git add large_dataset.csv.dv .gitignore
git commit -m "Add dataset"

# Push para storage remoto
dvc push
```

### **Kaggle API**
```bash
# Instala√ß√£o
pip install kaggle

# Download de dataset
kaggle datasets download -d owner/dataset-name

# Upload de dataset
kaggle datasets create -p /path/to/dataset
```

### **AWS S3 para Datasets**
```python
import boto3

s3 = boto3.client('s3')

# Upload
s3.upload_file('local_dataset.csv', 'my-bucket', 'datasets/dataset.csv')

# Download
s3.download_file('my-bucket', 'datasets/dataset.csv', 'local_dataset.csv')
```

---

## **üìä Templates de An√°lise**

### **Template EDA**
```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def basic_eda(df):
    print("=== DATASET OVERVIEW ===")
    print(f"Shape: {df.shape}")
    print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    print("\n=== DATA TYPES ===")
    print(df.dtypes.value_counts())
    
    print("\n=== MISSING VALUES ===")
    missing = df.isnull().sum()
    print(missing[missing > 0])
    
    print("\n=== NUMERICAL SUMMARY ===")
    print(df.describe())
    
    # Visualizations
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Missing values heatmap
    sns.heatmap(df.isnull(), ax=axes[0,0])
    axes[0,0].set_title('Missing Values Pattern')
    
    # Correlation matrix
    numeric_cols = df.select_dtypes(include='number').columns
    if len(numeric_cols) > 1:
        sns.heatmap(df[numeric_cols].corr(), annot=True, ax=axes[0,1])
        axes[0,1].set_title('Correlation Matrix')
    
    plt.tight_layout()
    plt.show()

# Uso
df = pd.read_csv('your_dataset.csv')
basic_eda(df)
```

---

*Esta lista √© atualizada regularmente. Sempre verifique os termos de uso dos datasets antes de utiliz√°-los em projetos comerciais.*
