# ğŸ› ï¸ Guia PrÃ¡tico: InstalaÃ§Ã£o e ConfiguraÃ§Ã£o Completa

*Tutorial passo-a-passo para configurar seu ambiente de Big Data e IA*

---

## ğŸ“‹ **PrÃ©-requisitos do Sistema**

### **Windows 10/11**
```powershell
# Verificar versÃ£o do Windows
winver

# Verificar se tem PowerShell 5.1+
$PSVersionTable.PSVersion
```

### **macOS**
```bash
# Verificar versÃ£o do macOS
sw_vers

# Instalar Homebrew (se nÃ£o tiver)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

### **Ubuntu/Linux**
```bash
# Verificar versÃ£o do Ubuntu
lsb_release -a

# Atualizar sistema
sudo apt update && sudo apt upgrade -y
```

---

## ğŸ **InstalaÃ§Ã£o do Python**

### **OpÃ§Ã£o 1: Python.org (Recomendado para iniciantes)**

**Windows:**
1. Acesse [python.org/downloads](https://python.org/downloads)
2. Baixe Python 3.11.x (versÃ£o mais estÃ¡vel)
3. **IMPORTANTE**: Marque "Add Python to PATH"
4. Execute a instalaÃ§Ã£o
5. Teste no PowerShell:
```powershell
python --version
pip --version
```

**macOS:**
```bash
# Via Homebrew (recomendado)
brew install python@3.11

# Verificar instalaÃ§Ã£o
python3 --version
pip3 --version
```

**Ubuntu:**
```bash
# Instalar Python 3.11
sudo apt install python3.11 python3.11-pip python3.11-venv

# Criar link simbÃ³lico (opcional)
sudo ln -sf /usr/bin/python3.11 /usr/bin/python

# Verificar
python --version
```

### **OpÃ§Ã£o 2: Anaconda (Recomendado para cientistas de dados)**

1. Baixe [Anaconda](https://www.anaconda.com/download)
2. Instale seguindo o assistente
3. Abra Anaconda Navigator ou Anaconda Prompt
4. Teste:
```bash
conda --version
python --version
```

---

## â˜• **InstalaÃ§Ã£o do Java (Essencial para PySpark)**

### **Por que preciso do Java?**
Apache Spark Ã© escrito em Scala (que roda na JVM). PySpark Ã© uma interface Python para Spark, mas ainda precisa do Java.

### **Windows:**
```powershell
# OpÃ§Ã£o 1: Chocolatey (package manager)
choco install openjdk11

# OpÃ§Ã£o 2: Download manual
# 1. Acesse https://adoptium.net/
# 2. Baixe OpenJDK 11 LTS
# 3. Instale e configure JAVA_HOME

# Verificar instalaÃ§Ã£o
java -version
echo $env:JAVA_HOME
```

### **macOS:**
```bash
# Via Homebrew
brew install openjdk@11

# Configurar JAVA_HOME no .zshrc ou .bash_profile
echo 'export JAVA_HOME=/opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home' >> ~/.zshrc
source ~/.zshrc

# Verificar
java -version
echo $JAVA_HOME
```

### **Ubuntu:**
```bash
# Instalar OpenJDK 11
sudo apt install openjdk-11-jdk

# Configurar JAVA_HOME
echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc
source ~/.bashrc

# Verificar
java -version
echo $JAVA_HOME
```

---

## ğŸ“¦ **CriaÃ§Ã£o do Ambiente Virtual**

### **Por que usar ambiente virtual?**
- Isola dependÃªncias do projeto
- Evita conflitos entre bibliotecas
- Facilita reproduÃ§Ã£o em outros sistemas

### **Criando o ambiente:**

```bash
# 1. Navegar para pasta do projeto
cd "caminho/para/BigData em Python"

# 2. Criar ambiente virtual
python -m venv venv

# 3. Ativar ambiente virtual
# Windows:
venv\Scripts\activate

# macOS/Linux:
source venv/bin/activate

# 4. Verificar se estÃ¡ ativo (deve aparecer (venv) no prompt)
which python  # macOS/Linux
where python   # Windows
```

---

## ğŸ“š **InstalaÃ§Ã£o das Bibliotecas**

### **Passo 1: Instalar dependÃªncias bÃ¡sicas**
```bash
# Atualizar pip
python -m pip install --upgrade pip

# Instalar bibliotecas essenciais
pip install pandas numpy matplotlib seaborn

# Verificar instalaÃ§Ã£o
python -c "import pandas as pd; print('âœ… Pandas OK!')"
python -c "import numpy as np; print('âœ… NumPy OK!')"
python -c "import matplotlib.pyplot as plt; print('âœ… Matplotlib OK!')"
```

### **Passo 2: Instalar PySpark**
```bash
# Instalar PySpark
pip install pyspark findspark

# Testar instalaÃ§Ã£o
python -c "
import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('Teste').getOrCreate()
print('âœ… PySpark funcionando!')
spark.stop()
"
```

### **Passo 3: Machine Learning e IA**
```bash
# Bibliotecas de ML tradicionais
pip install scikit-learn xgboost

# Deep Learning (opcional)
pip install tensorflow torch torchvision

# Processamento de texto (NLP)
pip install nltk spacy transformers

# Verificar ML
python -c "from sklearn.datasets import make_classification; print('âœ… Scikit-learn OK!')"
```

### **Passo 4: Jupyter e visualizaÃ§Ã£o**
```bash
# Jupyter ecosystem
pip install jupyter jupyterlab ipywidgets

# VisualizaÃ§Ã£o avanÃ§ada
pip install plotly bokeh streamlit

# Iniciar Jupyter Lab
jupyter lab
```

---

## ğŸ”§ **ConfiguraÃ§Ã£o AvanÃ§ada**

### **Configurar PySpark no Jupyter**

Criar arquivo `spark_config.py`:
```python
import os
import findspark

# Configurar caminhos (adapte para seu sistema)
os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'  # Linux
# os.environ['JAVA_HOME'] = 'C:/Program Files/Java/jdk-11.0.x'  # Windows
# os.environ['JAVA_HOME'] = '/opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home'  # macOS

findspark.init()

from pyspark.sql import SparkSession
from pyspark.conf import SparkConf

def criar_spark_session(app_name="BigData_SC"):
    """Cria sessÃ£o Spark configurada para aprendizado"""
    conf = SparkConf()
    conf.set("spark.sql.adaptive.enabled", "true")
    conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
    
    spark = SparkSession.builder \
        .appName(app_name) \
        .config(conf=conf) \
        .getOrCreate()
    
    return spark

# Usar assim:
# spark = criar_spark_session()
```

### **Configurar GPU para Deep Learning (Opcional)**

**Para NVIDIA GPU:**
```bash
# Verificar se tem GPU NVIDIA
nvidia-smi

# Instalar CUDA toolkit (versÃ£o compatÃ­vel com TensorFlow)
# Windows/Linux: https://developer.nvidia.com/cuda-toolkit
# Verificar compatibilidade: https://www.tensorflow.org/install/source#gpu

# Instalar TensorFlow com GPU
pip install tensorflow[and-cuda]

# Testar GPU
python -c "
import tensorflow as tf
print('GPUs disponÃ­veis:', tf.config.list_physical_devices('GPU'))
"
```

---

## ğŸ§ª **Testes de ValidaÃ§Ã£o Completa**

### **Script de teste automÃ¡tico:**

Criar `teste_instalacao.py`:
```python
#!/usr/bin/env python3
"""
Script de validaÃ§Ã£o completa do ambiente Big Data + IA
"""

def teste_bibliotecas_basicas():
    print("ğŸ” Testando bibliotecas bÃ¡sicas...")
    try:
        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import seaborn as sns
        print("âœ… Pandas, NumPy, Matplotlib, Seaborn: OK")
        
        # Teste rÃ¡pido
        df = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})
        assert len(df) == 3
        print("âœ… Teste funcional Pandas: OK")
        
    except Exception as e:
        print(f"âŒ Erro nas bibliotecas bÃ¡sicas: {e}")
        return False
    return True

def teste_pyspark():
    print("\nğŸ” Testando PySpark...")
    try:
        import findspark
        findspark.init()
        
        from pyspark.sql import SparkSession
        spark = SparkSession.builder \
            .appName("TesteValidacao") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
            .getOrCreate()
        
        # Teste com dados
        dados = [("Patrick", 25, "FlorianÃ³polis"), ("Ana", 30, "SÃ£o JosÃ©")]
        df = spark.createDataFrame(dados, ["nome", "idade", "cidade"])
        
        assert df.count() == 2
        print("âœ… PySpark: OK")
        
        spark.stop()
        
    except Exception as e:
        print(f"âŒ Erro no PySpark: {e}")
        print("ğŸ’¡ Dica: Verifique se Java estÃ¡ instalado (java -version)")
        return False
    return True

def teste_machine_learning():
    print("\nğŸ” Testando Machine Learning...")
    try:
        from sklearn.datasets import make_classification
        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import accuracy_score
        
        # Criar dataset sintÃ©tico
        X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
        
        # Treinar modelo
        model = RandomForestClassifier(n_estimators=10, random_state=42)
        model.fit(X_train, y_train)
        
        # Predizer
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        
        assert acc > 0.7  # Pelo menos 70% de acurÃ¡cia
        print(f"âœ… Scikit-learn: OK (AcurÃ¡cia: {acc:.3f})")
        
    except Exception as e:
        print(f"âŒ Erro no Machine Learning: {e}")
        return False
    return True

def teste_deep_learning():
    print("\nğŸ” Testando Deep Learning (opcional)...")
    try:
        import tensorflow as tf
        
        # Verificar GPU
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            print(f"âœ… TensorFlow com {len(gpus)} GPU(s): OK")
        else:
            print("âœ… TensorFlow (CPU apenas): OK")
        
        # Teste simples
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(10, activation='relu', input_shape=(5,)),
            tf.keras.layers.Dense(1, activation='sigmoid')
        ])
        
        # Dados sintÃ©ticos
        import numpy as np
        X = np.random.random((100, 5))
        y = np.random.randint(0, 2, (100, 1))
        
        model.compile(optimizer='adam', loss='binary_crossentropy')
        model.fit(X, y, epochs=1, verbose=0)
        
        print("âœ… Teste TensorFlow: OK")
        
    except ImportError:
        print("âš ï¸  TensorFlow nÃ£o instalado (opcional)")
        return True
    except Exception as e:
        print(f"âŒ Erro no TensorFlow: {e}")
        return False
    return True

def teste_jupyter():
    print("\nğŸ” Testando Jupyter...")
    try:
        import jupyter
        import jupyterlab
        print("âœ… Jupyter Lab: OK")
        print("ğŸ’¡ Para iniciar: jupyter lab")
        
    except Exception as e:
        print(f"âŒ Erro no Jupyter: {e}")
        return False
    return True

def relatorio_sistema():
    print("\nğŸ“Š RelatÃ³rio do Sistema:")
    import sys
    import platform
    
    print(f"ğŸ Python: {sys.version.split()[0]}")
    print(f"ğŸ’» Sistema: {platform.system()} {platform.release()}")
    print(f"ğŸ—ï¸  Arquitetura: {platform.architecture()[0]}")
    
    # Verificar Java
    import subprocess
    try:
        result = subprocess.run(['java', '-version'], 
                              capture_output=True, text=True, stderr=subprocess.STDOUT)
        java_version = result.stdout.split('\n')[0]
        print(f"â˜• Java: {java_version}")
    except:
        print("âŒ Java: NÃ£o encontrado")

if __name__ == "__main__":
    print("ğŸš€ Iniciando validaÃ§Ã£o completa do ambiente Big Data + IA\n")
    
    relatorio_sistema()
    
    testes = [
        teste_bibliotecas_basicas,
        teste_pyspark,
        teste_machine_learning,
        teste_deep_learning,
        teste_jupyter
    ]
    
    resultados = []
    for teste in testes:
        resultado = teste()
        resultados.append(resultado)
    
    print(f"\nğŸ“‹ Resumo: {sum(resultados)}/{len(resultados)} testes passaram")
    
    if all(resultados):
        print("ğŸ‰ Ambiente configurado com sucesso!")
        print("\nğŸ“š PrÃ³ximos passos:")
        print("1. jupyter lab")
        print("2. Abrir livro/capitulo01-despertar-dos-dados.md")
        print("3. Seguir a jornada de Patrick em Big Data!")
    else:
        print("âš ï¸  Alguns testes falharam. Verifique a instalaÃ§Ã£o.")
```

### **Executar validaÃ§Ã£o:**
```bash
python teste_instalacao.py
```

---

## ğŸš¨ **SoluÃ§Ã£o de Problemas Comuns**

### **1. "Java not found" no PySpark**
```bash
# Verificar se Java estÃ¡ no PATH
java -version

# Se nÃ£o encontrar, definir JAVA_HOME
# Windows (PowerShell):
$env:JAVA_HOME = "C:\Program Files\Java\jdk-11.0.x"

# macOS/Linux:
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

### **2. "Permission denied" no macOS/Linux**
```bash
# Dar permissÃµes de execuÃ§Ã£o
chmod +x teste_instalacao.py

# Usar sudo apenas se necessÃ¡rio
sudo pip install nome_biblioteca
```

### **3. Conflitos de versÃ£o**
```bash
# Limpar cache do pip
pip cache purge

# Reinstalar tudo
pip uninstall -y -r requirements.txt
pip install -r requirements.txt
```

### **4. Problemas com Jupyter**
```bash
# Recriar kernels
python -m ipykernel install --user --name=venv

# Iniciar Jupyter com configuraÃ§Ãµes padrÃ£o
jupyter lab --generate-config
```

---

## ğŸ¯ **PrÃ³ximos Passos**

1. **âœ… Executar teste de validaÃ§Ã£o**
2. **ğŸ“š Ler CapÃ­tulo 1** - Patrick e os fundamentos de Big Data
3. **ğŸ’» Abrir Jupyter Lab** - ambiente interativo
4. **ğŸ”¬ Experimentar cÃ³digos** - cada capÃ­tulo tem exemplos
5. **ğŸš€ Criar seu primeiro projeto** - adaptar casos para sua regiÃ£o

**Ambiente configurado? Hora de comeÃ§ar a jornada com Patrick! ğŸŒŸ**
